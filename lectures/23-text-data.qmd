---
title: "Visualizing Text Data"
author: "Prof Ron Yurko"
footer:  "[statds-36315-spring25](https://ryurko.github.io/statds-36315-spring25/)"
date: 2025-04-14
engine: knitr
format:
  revealjs:
    theme: theme.scss
    chalkboard: true
    pdf-separate-fragments: true
    slide-number: c/t
    smaller: true
    code-line-numbers: true
    linestretch: 1.25
    html-math-method:
      method: mathjax
      url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

```{r}
#| include: false
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center"
)

library(tidyverse)
```

## Announcements, previously, and today...

::: {style="font-size: 95%;"}

**You should be working on your final projects!**

**You must email me the slides for your presentation by 10 AM ET the day of your presentation - either a Google slides link or PDF file**

**You do have lab this week!**

:::

. . .

::: {style="font-size: 95%;"}

**Last time:** visualizing areal data


**TODAY:** Text data, starting with:

- Bag of Words representation of text.

- Word clouds

Will be able to answer the following questions:

- Which words are most frequent in a set of documents?

- How do two (or more) sets of documents compare in their word usage?

- Which unique words occur most frequently in a set of documents?

:::


---

## Working with raw text data 

- We'll work with script from the best episode of ['The Office': Season 4, Episode 13 - 'Dinner Party'](https://en.wikipedia.org/wiki/Dinner_Party_(The_Office))

- We can access the script using the [`schrute` package (yes this is a real thing)](https://cran.r-project.org/web/packages/schrute/vignettes/theoffice.html):

```{r}
library(schrute)
# Create a table from this package just corresponding to the Dinner Party episode:
dinner_party_table <- theoffice |>
  filter(season == 4, episode == 13) |>
  # Just select columns of interest:
  dplyr::select(index, character, text)
head(dinner_party_table)
```

---

## Bag of Words representation of text

- Most common way to store text data is with a __document-term matrix__ (DTM):

|            | Word 1   | Word 2   | $\dots$  | Word $J$ |
| ---------- | -------- | -------- | -------- | -------- |
| Document 1 | $w_{11}$ | $w_{12}$ | $\dots$  | $w_{1J}$ |
| Document 2 | $w_{21}$ | $w_{22}$ | $\dots$  | $w_{2J}$ |
| $\dots$    | $\dots$  | $\dots$  | $\dots$  | $\dots$  |
| Document N | $w_{N1}$ | $w_{N2}$ | $\dots$  | $w_{NJ}$ |

- $w_{ij}$: count of word $j$ in document $i$, aka _term frequencies_

. . .

Two additional ways to reduce number of columns:

1. __Stop words__: remove extremely common words (e.g., of, the, a)

2. __Stemming__: Reduce all words to their "stem"

  - For example: Reducing = reduc. Reduce = reduc. Reduces = reduc.


---

## Tokenize text into long format

- Convert raw text into long, tidy table with one-token-per-document-per-row

  - A __token__ equals a unit of text - typically a word
  
```{r}
library(tidytext)
tidy_dinner_party_tokens <- dinner_party_table |>
  unnest_tokens(word, text)
head(tidy_dinner_party_tokens)
```

Easy to convert text into DTM format using [`tidytext` package](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html)

---

## Remove stop words

- Load `stop_words` from `tidytext`

```{r}
data(stop_words)

tidy_dinner_party_tokens <- tidy_dinner_party_tokens |>
  filter(!(word %in% stop_words$word))

head(tidy_dinner_party_tokens)
```

---

## Apply stemming

- Can use [`SnowballC` package](https://cran.r-project.org/web/packages/SnowballC/SnowballC.pdf) to perform stemming

```{r}
library(SnowballC)

tidy_dinner_party_tokens <- tidy_dinner_party_tokens |>
  mutate(stem = wordStem(word))

head(tidy_dinner_party_tokens)
```

---

## Create word cloud using term frequencies

__Word Cloud__: Displays all words mentioned across documents, where more common words are larger

- To do this, you must compute the _total_ word counts:

$$w_{\cdot 1} = \sum_{i=1}^N w_{i1} \hspace{0.1in} \dots \hspace{0.1in} w_{\cdot J} = \sum_{i=1}^N w_{iJ}$$

- Then, the size of Word $j$ is proportional to $w_{\cdot j}$

. . .

Create word clouds in `R` using [`wordcloud` package](https://cran.r-project.org/web/packages/wordcloud/wordcloud.pdf)

Takes in two main arguments to create word clouds:

1. `words`: vector of unique words

2. `freq`: vector of frequencies


---

## Create word cloud using term frequencies

```{r}
#| output-location: slide
token_summary <- tidy_dinner_party_tokens |>
  group_by(stem) |>
  count() |>
  ungroup() 

library(wordcloud)
wordcloud(words = token_summary$stem, 
          freq = token_summary$n, 
          random.order = FALSE, 
          max.words = 100, 
          colors = brewer.pal(8, "Dark2"))
```

- Set `random.order = FALSE` to place biggest words in center

- Can customize to display limited # words (`max.words`)

- Other options as well like `colors`


---

## Comparison clouds

Imagine we have two different collections of documents, $\mathcal{A}$ and $\mathcal{B}$, that we wish to visually compare.

. . .

Imagine we create the word clouds for the two collections of documents. Then this means we constructed vectors of total words for each collection:

+ $\mathbf{w}^{\mathcal{A}} = (w_{\cdot 1}^{\mathcal{A}}, \dots, w_{\cdot J}^{\mathcal{A}})$

+ $\mathbf{w}^{\mathcal{B}} = (w_{\cdot 1}^{\mathcal{B}}, \dots, w_{\cdot J}^{\mathcal{B}})$

Consider the $j$th word, let's pretend it's "dinner":

+ If $w_{\cdot j}^{\mathcal{A}}$ is large, then "dinner" is large in the word cloud for $\mathcal{A}$.

+ If $w_{\cdot j}^{\mathcal{B}}$ is large, then "dinner" is large in the word cloud for $\mathcal{B}$.

+ But if both are large, this doesn't tell us whether $w_{\cdot j}^{\mathcal{A}}$ or $w_{\cdot j}^{\mathcal{B}}$ is bigger.


---

## Comparison clouds

This motivates the construction of __comparison word clouds__: 

1. For word $j$, compute $\bar{w}_{\cdot j} = \text{average}(w_{\cdot j}^{\mathcal{A}}, w_{\cdot j}^{\mathcal{B}})$

2. Compute $w_{\cdot j}^{\mathcal{A}} - \bar{w}_{\cdot j}$ and $w_{\cdot j}^{\mathcal{B}} - \bar{w}_{\cdot j}$

3. If $w_{\cdot j}^{\mathcal{A}} - \bar{w}_{\cdot j}$ is very positive, make it large for the $\mathcal{A}$ word cloud. If $w_{\cdot j}^{\mathcal{B}} - \bar{w}_{\cdot j}$ is very positive, make it large for the $\mathcal{B}$ word cloud.


---

## Comparison clouds

```{r}
#| echo: false
character_token_summary <- tidy_dinner_party_tokens |>
  group_by(character, stem) |>
  count() |>
  ungroup() 
michael_jan_summary <- character_token_summary |>
  filter(character %in% c("Michael", "Jan"))
library(reshape2)
michael_jan_summary |>
  acast(stem ~ character, 
        value.var = "n", fill = 0) |>
  comparison.cloud(colors = c("red", "blue"),
                   max.words = 100, scale = c(2, .1),
                   title.size = 2)
```


---

## TF-IDF weighting

- We saw that `michael` was the largest word, but what if I'm interested in comparing text across characters (i.e., documents)?

. . .

- Itâ€™s arguably of more interest to understand which words are frequently used in one set of texts but not the other, i.e., which words are unique?

- Many text analytics methods will __down-weight__ words that occur frequently across all documents

. . .

- __Inverse document frequency (IDF)__: for word $j$ we compute $\text{idf}_j = \log \frac{N}{N_j}$

  - where $N$ is number of documents, $N_j$ is number of documents with word $j$

- Compute __TF-IDF__ $= w_{ij} \times \text{idf}_j$

---

## TF-IDF example with characters

Compute and join TF-IDF using `bind_tf_idf()`:

```{r}
character_token_summary <- tidy_dinner_party_tokens |>
  group_by(character, stem) |> 
  count() |>
  ungroup() 

character_token_summary <- character_token_summary |>
  bind_tf_idf(stem, character, n) 
character_token_summary
```


---

## Top 10 words by TF-IDF for each character

```{r}
#| output-location: slide
character_token_summary |>
  filter(character %in% c("Michael", "Jan", "Jim", "Pam")) |>
  group_by(character) |>
  slice_max(tf_idf, n = 10, with_ties = FALSE) |>
  ungroup() |>
  mutate(stem = reorder_within(stem, tf_idf, character)) |>
  ggplot(aes(y = tf_idf, x = stem),
         fill = "darkblue", alpha = 0.5) +
  geom_col() +
  coord_flip() +
  scale_x_reordered() +
  facet_wrap(~ character, ncol = 2, scales = "free") +
  labs(y = "TF-IDF", x = NULL)
```

---

## Other functions of text

- We've just focused on word counts - __but there are many functions of text__

- For example: __number of unique words__ is often used to measure vocabulary

```{r, out.width="70%", echo = FALSE, fig.align='center'}
knitr::include_graphics("https://pbs.twimg.com/media/DxCgsrxWwAAOWO3.jpg")
```

---

## Recap and next steps

- Most common representation: Bag of words and term frequencies (possibly weighted by TF-IDF)

- Word clouds are the most common way to visualize the most frequent
words in a set of documents

- TF-IDF weighting allows you to detect words that are uniquely used
in certain documents

. . .

+ **You have lab on Friday!**

+ **Next time**: Sentiment analysis and topics models

+ Recommended Reading: [Text Mining With R](https://www.tidytextmining.com/), [Supervised Machine Learning for Text Analysis in R](https://smltar.com/)

---
title: "Statistical Inference for 1D Categorical Data"
author: "Prof Ron Yurko"
footer:  "[statds-36315-spring25](https://ryurko.github.io/statds-36315-spring25/)"
date: 2025-01-22
engine: knitr
format:
  revealjs:
    theme: theme.scss
    chalkboard: true
    pdf-separate-fragments: true
    slide-number: c/t
    smaller: true
    code-line-numbers: true
    linestretch: 1.25
    html-math-method:
      method: mathjax
      url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

```{r}
#| include: false
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center"
)

library(tidyverse)
library(palmerpenguins)
```

## Announcements, previously, and today...

::: {style="font-size: 80%;"}

+ **HW1 is posted and due Wednesday Jan 29** 

+ **You have Lab 2 again on Friday**

Office hours schedule:

+ My office hours (BH 132D): Wednesdays and Thursdays @ 2 PM

+ Anna (zoom): Mondays @ 2 PM

+ Perry (zoom): Wednesdays @ 11 AM

:::

. . .

::: {style="font-size: 80%;"}


+ Discussed 1D categorical data, basic summaries with counts and proportions

+ Introduced different area plots for visualizing 1D categorical data

+ **We make bar charts for categorical data** (I told you repeatedly that pie charts suck)

:::

. . .

::: {style="font-size: 80%;"}

**TODAY: quantify and display uncertainty for 1D categorical data**

+ Add confidence intervals to bar charts

+ Review the Chi-Squared Test

+ Discuss connections between visualizations and statistical significance

:::

---

## Crimes against bar charts

![](https://venngage-wordpress.s3.amazonaws.com/uploads/2021/01/womenicons.jpg){fig-align="center"width=45%}

---

## Crimes against bar charts

![](https://pbs.twimg.com/media/Ee6WX52WsAAP9wT.png){fig-align="center"width=45%}

---

## What does a bar chart show?

**Marginal Distribution**

+ Assume categorical variable $X$ has $K$ categories: $C_1, \dots, C_K$

+ **True** marginal distribution of $X$: 

$$
P(X = C_j) = p_j,\ j \in \{ 1, \dots, K \}
$$

. . .

**We have access to the Empirical Marginal Distribution**

+ Observed distribution of $X$, our best estimate (MLE) of the marginal distribution of $X$: $\hat{p}_1$, $\hat{p}_2$, $\dots$, $\hat{p}_K$


```{r}
table(penguins$species) / nrow(penguins)
```


---

## Bar charts with proportions

+ [`after_stat()`](https://ggplot2.tidyverse.org/reference/aes_eval.html) indicates the aesthetic mapping is performed after statistical transformation

+ Use `after_stat(count)` to access the `stat_count()` called by `geom_bar()`

```{r}
#| code-line-numbers: "3"
penguins |>
  ggplot(aes(x = species)) +
  geom_bar(aes(y = after_stat(count) / sum(after_stat(count)))) + 
  labs(y = "Proportion")
```


---

## Compute and display the proportions directly

+ Use `group_by()`, `summarize()`, and `mutate()` in a pipeline to compute then display the proportions directly

+ Need to indicate we are displaying the `y` axis as given, i.e., the identity function


```{r}
#| output-location: slide
#| code-line-numbers: "2-5,7"
penguins |>
  group_by(species) |> 
  summarize(count = n(), .groups = "drop") |> 
  mutate(total = sum(count), 
         prop = count / total) |> 
  ggplot(aes(x = species)) +
  geom_bar(aes(y = prop), stat = "identity") 
```


---

## What about uncertainty?

+ Quantify uncertainty for our estimate $\hat{p}_j = \frac{n_j}{n}$ with the **standard error**:

$$
SE(\hat{p}_j) = \sqrt{\frac{\hat{p}_j(1 - \hat{p}_j)}{n}}
$$

. . .

+ Compute $\alpha$-level __confidence interval__ (CI) as $\hat{p}_j \pm z_{1 - \alpha / 2} \cdot SE(\hat{p}_j)$

+ Good rule-of-thumb: construct 95% CI using $\hat{p}_j \pm 2 \cdot SE(\hat{p}_j)$

+ Approximation justified by CLT, so CI could include values outside of [0,1]



---

## Add standard errors to bars

+ Need to remember each CI is for each $\hat{p}_j$ marginally, **not** jointly

+ Have to be careful with __multiple testing__ 


```{r}
#| output-location: slide
#| code-line-numbers: "6-8,11-12"
penguins |>
  group_by(species) |> 
  summarize(count = n(), .groups = "drop") |> 
  mutate(total = sum(count), 
         prop = count / total,
         se = sqrt(prop * (1 - prop) / total), 
         lower = prop - 2 * se, 
         upper = prop + 2 * se) |> 
  ggplot(aes(x = species)) +
  geom_bar(aes(y = prop), stat = "identity") +
  geom_errorbar(aes(ymin = lower, ymax = upper), 
                color = "red") 
```


---

##  Useful to order categories by frequency with [`forcats`](https://forcats.tidyverse.org/)


```{r}
#| output-location: slide
#| code-line-numbers: "9"
penguins |>
  group_by(species) |> 
  summarize(count = n(), .groups = "drop") |> 
  mutate(total = sum(count), 
         prop = count / total,
         se = sqrt(prop * (1 - prop) / total), 
         lower = prop - 2 * se, 
         upper = prop + 2 * se,
         species = fct_reorder(species, prop)) |>
  ggplot(aes(x = species)) +
  geom_bar(aes(y = prop), stat = "identity") +
  geom_errorbar(aes(ymin = lower, ymax = upper), 
                color = "red") 
```


---

## [Don't do this...](https://retractionwatch.com/2022/12/05/a-paper-used-capital-ts-instead-of-error-bars-but-wait-theres-more/)


![](https://retractionwatch.com/wp-content/uploads/2022/12/3802603.fig_.009-768x512.png){fig-align="center"width=45%}


---

## Chi-squared test for 1D categorical data:

::: {style="font-size: 75%;"}

+ __Null hypothesis__ $H_0$: $p_1 = p_2 = \dots = p_K$, compute the test statistic:

$$
\chi^2 = \sum_{j=1}^K \frac{(O_j - E_j)^2}{E_j}
$$

+ $O_j$: observed counts in category $j$

+ $E_j$: expected counts under $H_0$, i.e., each category is equally to occur $n / K = p_1 = p_2 = \dots = p_K$

:::

. . .

```{r}
chisq.test(table(penguins$species))
```


<!-- - Large $\chi^2 \rightarrow$ observed counts are very different from expected counts -->

<!-- - Therefore we should just reject the null hypothesis -->

<!-- - i.e., the $p$-value is small because we would not expect to see observed counts so extreme if the null were true -->

<!-- - But if we reject, cannot tell _which_ probabilities are different... -->

<!-- - _Can we use graphs to tell us which are different...?_ -->

---

## Hypothesis testing review

:::: {.columns}

::: {.column width="50%"}

::: {style="font-size: 85%;"}

Computing $p$-values works like this:

- Choose a test statistic.

- Compute the test statistic in your dataset.

- Is test statistic "unusual" compared to what I would expect under $H_0$?

- Compare $p$-value to __target error rate__ $\alpha$ (typically referred to as target level $\alpha$ )

- Typically choose $\alpha = 0.05$ 

  - i.e., if we reject null  hypothesis at $\alpha = 0.05$ then, assuming $H_0$ is true, there is a 5% chance it is a false positive (aka Type 1 error)
  

:::

:::

::: {.column width="50%"}

::: {.fragment}

![](https://measuringu.com/wp-content/uploads/2021/04/042121-F2.jpg){fig-align="center"}

:::

:::

::::


---

## Chi-squared test for 1D categorical data:

::: {style="font-size: 75%;"}

+ __Null hypothesis__ $H_0$: $p_1 = p_2 = \dots = p_K$, compute the test statistic:

$$
\chi^2 = \sum_{j=1}^K \frac{(O_j - E_j)^2}{E_j}
$$

+ $O_j$: observed counts in category $j$

+ $E_j$: expected counts under $H_0$, i.e., each category is equally to occur $n / K = p_1 = p_2 = \dots = p_K$

:::


```{r}
chisq.test(table(penguins$species))
```


---

## Graphics versus Statistical Inference

- Reminder Anscombe's Quartet: where statistical inference was the same but the graphics were very different

![](https://upload.wikimedia.org/wikipedia/commons/e/ec/Anscombe%27s_quartet_3.svg){fig-align="center"width=55%}

- __The opposite can be true!__ Graphics are the same, but statistical inference is very different...


---

## Example: 3 categories, $p_1 = 1/2,\ p_2 = p_3 = 1/4$

```{r}
#| echo: false
#| fig-align: center
fake_counts <- tibble(fake_group = c("A", "A", "B", "C"))
fake_counts |>
  ggplot(aes(x = fake_group)) +
  geom_bar(fill = "darkblue") +
  theme_bw() +
  theme(axis.title = element_blank(),
        axis.text = element_text(size = 16))
```


---

## Example: 3 categories, $p_1 = 1/2,\ p_2 = p_3 = 1/4$

```{r}
#| echo: false
#| fig-align: center
# Init the fake data
fake_counts <- tibble(fake_group = c("A", "A", "B", "C"))
# Run and store the chi-square test:
fake_chisq_test_results <- chisq.test(table(fake_counts$fake_group))
# Create the plot
fake_counts |>
  ggplot(aes(x = fake_group)) +
  geom_bar(fill = "darkblue") +
  # Add the label with number of observations
  annotate(geom = "text", x = 2.5, y = 1.5, size = 5,
           label = paste0("n = ", nrow(fake_counts))) +
  # Add label with p-value from chi-square test
  annotate(geom = "text", x = 2.5, y = 1.25, size = 5,
           label = paste0("p-value = ", 
                          signif(fake_chisq_test_results$p.value,
                                 digits = 2))) +
  theme_bw() +
  theme(axis.title = element_blank(),
        axis.text = element_text(size = 16))

```


---

## Example: 3 categories, $p_1 = 1/2,\ p_2 = p_3 = 1/4$

```{r}
#| echo: false
#| fig-align: center
# Init the fake data
fake_counts <- tibble(fake_group = c(rep("A", 8), 
                                     rep("B", 4), rep("C", 4)))
# Run and store the chi-square test:
fake_chisq_test_results <- chisq.test(table(fake_counts$fake_group))
fake_counts |>
  ggplot(aes(x = fake_group)) +
  geom_bar(fill = "darkblue") +
  # Add labels but preserve location of vertical label based on change in n
  annotate(geom = "text", x = 2.5, y = 1.5 * 8 / 2, size = 5,
           label = paste0("n = ", nrow(fake_counts))) +
  annotate(geom = "text", x = 2.5, y = 1.25 * 8 / 2, size = 5,
           label = paste0("p-value = ", 
                          signif(fake_chisq_test_results$p.value,
                                 digits = 2))) +
  theme_bw() +
  theme(axis.title = element_blank(),
        axis.text = element_text(size = 16))

```


---

## Example: 3 categories, $p_1 = 1/2,\ p_2 = p_3 = 1/4$

```{r}
#| echo: false
#| fig-align: center
# Init the fake data
fake_counts <- tibble(fake_group = c(rep("A", 32), 
                                     rep("B", 16), rep("C", 16)))
# Run and store the chi-square test:
fake_chisq_test_results <- chisq.test(table(fake_counts$fake_group))
# Create the plot
fake_counts |>
  ggplot(aes(x = fake_group)) +
  geom_bar(fill = "darkblue") +
  # Add labels but preserve location of vertical label based on change in n
  annotate(geom = "text", x = 2.5, y = 1.5 * 32 / 2, size = 5,
           label = paste0("n = ", nrow(fake_counts))) +
  annotate(geom = "text", x = 2.5, y = 1.25 * 32 / 2, size = 5,
           label = paste0("p-value = ", 
                          signif(fake_chisq_test_results$p.value,
                                 digits = 2))) +
  theme_bw() +
  theme(axis.title = element_blank(),
        axis.text = element_text(size = 16))

```


---

## Power under this scenario: (2n/4, n/4, n/4)

```{r}
#| echo: false
#| fig-align: center
# Iterate through a sequence of sample sizes to compute the pvalue:
chisq_power_table <-
  map_dfr(seq(4, 100, by = 4),
          # Apply the following function to each value
          function(x) {
            
            # Init the fake data:
            fake_data <- tibble(fake_group = 
                                  c(rep("A", 2 * x / 4), 
                                    rep("B", x / 4), rep("C", x / 4)))
            
            # Run and store the chi-square test:
            fake_results <- chisq.test(table(fake_data$fake_group))
            
            # Return a table with the p-value and n:
            tibble("n" = x,
                   "pval" = fake_results$p.value)
          })

# Plot the results:
chisq_power_table |>
  # Note the use of 
  ggplot(aes(x = n, y = pval)) +
  # Draw a line layer to connect the points 
  geom_line(color = "gray") +
  geom_point(color = "black") +
  # Add a horizontal line at 0.05:
  geom_hline(yintercept = 0.05, linetype = "dashed", 
             color = "red") +
  scale_x_continuous(breaks = seq(0, 100, by = 20)) +
  theme_bw() +
  labs(x = "Sample size", y = "p-value")

```


---

## How do we combine graphs with inference?


1. Simply add $p$-values (or other info) to graph via text

2. Add confidence intervals to the graph

  - Need to remember what each CI is for! 
  
  - Our CIs on previous slides are for each $\hat{p}_j$ marginally, __NOT__ jointly

  - Have to be careful with __multiple testing__...

---

## CIs will visually capture uncertainty in estimates

:::: {.columns}

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-align: center
#| fig-height: 9
# Init the fake data
fake_counts <- tibble(fake_group = c("A", "A", "B", "C"))
# Compute the summary with standard errors:
fake_counts |>
  group_by(fake_group) |>
  summarize(count = n(), .groups = "drop") |>
  mutate(total = sum(count),
         prop = count / total,
         se = sqrt(prop * (1 - prop) / total),
         lower = prop - 2 * se,
         upper = prop + 2 * se) |>
  ggplot(aes(x = fake_group)) +
  geom_bar(fill = "darkblue",
           aes(y = prop),
           stat = "identity") +
  geom_errorbar(aes(ymin = lower,
                    ymax = upper),
                color = "red") +
  # Add the label with number of observations
  annotate(geom = "text", x = 2.5, y = .75, size = 10,
           label = paste0("n = ", nrow(fake_counts))) +
  theme_bw() +
  theme(axis.title = element_blank(),
        axis.text = element_text(size = 16))

```

:::

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-align: center
#| fig-height: 9
# Init the fake data
fake_counts <- tibble(fake_group = c(rep("A", 32), 
                                     rep("B", 16), rep("C", 16)))# Compute the summary with standard errors:
fake_counts |>
  group_by(fake_group) |>
  summarize(count = n(), .groups = "drop") |>
  mutate(total = sum(count),
         prop = count / total,
         se = sqrt(prop * (1 - prop) / total),
         lower = prop - 2 * se,
         upper = prop + 2 * se) |>
  ggplot(aes(x = fake_group)) +
  geom_bar(fill = "darkblue",
           aes(y = prop),
           stat = "identity") +
  geom_errorbar(aes(ymin = lower,
                    ymax = upper),
                color = "red") +
  # Add the label with number of observations
  annotate(geom = "text", x = 2.5, y = .75, size = 10,
           label = paste0("n = ", nrow(fake_counts))) +
  scale_y_continuous(limits = c(0, 1)) +
  theme_bw() +
  theme(axis.title = element_blank(),
        axis.text = element_text(size = 16))

```

:::

::::

---

## (Rough) Rules-of-thumb for comparing CIs on bar charts

::: {style="font-size: 75%;"}

- Comparing overlap of two CIs is __NOT__ exactly the same as directly testing for a significant difference...

  - Really you want CI( $\hat{p}_1 - \hat{p}_2$ ), not CI( $\hat{p_1}$ ) and CI( $\hat{p_2}$ )
  
  - CI( $\hat{p_1}$ ) and CI( $\hat{p_2}$ ) not overlapping implies $0 \notin$ CI( $\hat{p}_1 - \hat{p}_2$ )

  - _However_ CI( $\hat{p_1}$ ) and CI( $\hat{p_2}$ ) overlapping __DOES NOT__ imply $0 \in$ CI( $\hat{p}_1 - \hat{p}_2$ ) 


Roughly speaking:

  - If CIs don't overlap $\rightarrow$ significant difference
  
  - If CIs overlap a little $\rightarrow$ ambiguous
  
  - If CIs overlap a lot $\rightarrow$ no significant difference
  
:::
  
. . .

::: {style="font-size: 75%;"}

But if we're comparing more than two CIs simultaneously, we need to account for __multiple testing__!

  - When you look for all non-overlapping CIs: implicitly making $\binom{K}{2} = \frac{K!}{2!(K-2)!}$ pairwise tests in your head!
  
:::

---

## Corrections for multiple testing

::: {style="font-size: 75%;"}

- In those bar plots, when we determine whether CIs overlap we make 3 comparisons:

  1. A vs B
  
  2. A vs C
  
  3. B vs C
  
**This is a multiple testing issue**

:::

. . .

::: {style="font-size: 75%;"}

- In short: we will make Type 1 errors (chance of false rejecting) more than 5% of the time!

- Reminder: Type 1 error = Rejecting $H_0$ when $H_0$ is true

- e.g., CIs don't overlap but actually $H_0: p_A = p_B$ is true
  
- If only interested in A vs B __and nothing else__, then just construct 95% CI for A vs B and _control error rate_ at 5%
  
- However, if we construct several CIs, where A vs B is just one comparison we make, our Type 1 error rate > 5%!

:::

---

## Corrections for multiple testing

::: {style="font-size: 75%;"}

Vast literature on corrections for multiple testing (beyond the scope of this class... but in my thesis!)

But you should understand the following:

1. Corrections for multiple testing inflate $p$-values (i.e., make them bigger)

2. Equivalently, they inflate CIs (i.e., make them wider)

3. Purpose of these corrections is to control Type 1 error rate $\leq 5\%$

:::

. . .

::: {style="font-size: 75%;"}

We'll focus on the __Bonferroni correction__, which inflates $p$-values the most but is easy to implement and very popular:

+ We usually reject null hypothesis when $p$-value $\leq .05$

+ __Bonferroni__: if making $K$ comparisons, reject only if $p$-value $\leq .05/K$

+ For CIs: instead of plotting 95% CIs, we plot (1 - $0.05/K$)% CIs

  + e.g., for $K = 3$ then plot 98.3% CIs
  
:::
  
---

## Impact of Bonferroni correction on CIs...

:::: {.columns}

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-align: center
#| fig-height: 9
# Init the fake data
fake_counts <- tibble(fake_group = c(rep("A", 32), 
                                     rep("B", 16), rep("C", 16)))# Compute the summary with standard errors:
fake_counts |>
  group_by(fake_group) |>
  summarize(count = n(), .groups = "drop") |>
  mutate(total = sum(count),
         prop = count / total,
         se = sqrt(prop * (1 - prop) / total),
         lower = prop - 2 * se,
         upper = prop + 2 * se) |>
  ggplot(aes(x = fake_group)) +
  geom_bar(fill = "darkblue",
           aes(y = prop),
           stat = "identity") +
  geom_errorbar(aes(ymin = lower,
                    ymax = upper),
                color = "red") +
  # Add the label with number of observations
  annotate(geom = "text", x = 2.5, y = .75, size = 10,
           label = paste0("n = ", nrow(fake_counts))) +
  scale_y_continuous(limits = c(0, 1)) +
  theme_bw() +
  theme(axis.title = element_blank(),
        axis.text = element_text(size = 16))

```


:::

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-align: center
#| fig-height: 9
# Init the fake data
fake_counts <- tibble(fake_group = c(rep("A", 32), 
                                     rep("B", 16), rep("C", 16)))# Compute the summary with standard errors:
fake_counts |>
  group_by(fake_group) |>
  summarize(count = n(), .groups = "drop") |>
  mutate(total = sum(count),
         prop = count / total,
         se = sqrt(prop * (1 - prop) / total),
         lower = prop - qnorm(1 - .05 / 3 / 2) * se,
         upper = prop + qnorm(1 - .05 / 3 / 2) * se) |>
  ggplot(aes(x = fake_group)) +
  geom_bar(fill = "darkblue",
           aes(y = prop),
           stat = "identity") +
  geom_errorbar(aes(ymin = lower,
                    ymax = upper),
                color = "red") +
  # Add the label with number of observations
  annotate(geom = "text", x = 2.5, y = .75, size = 10,
           label = paste0("n = ", nrow(fake_counts))) +
  scale_y_continuous(limits = c(0, 1)) +
  theme_bw() +
  theme(axis.title = element_blank(),
        axis.text = element_text(size = 16))


```


:::

::::

---

## Recap and next steps

::: {style="font-size: 80%;"}

+ Bar charts display the empirical distribution of the categorical variable ( $\hat{p}_1, \dots, \hat{p}_K$ )

+ Chi-squared test is a _global test_ for 1D categorical data, testing $H_0 : p_1 = \cdot \cdot \cdot = p_K$

  + Does not tell us which probabilities differ! 

+ Can visualize CIs for each $\hat{p}_1$, $\dots$, $\hat{p}_K$, but need to deal with multiple testing
  
+ Graphs with the same trends can display very different statistical significance (largely due to sample size)

:::

. . .

::: {style="font-size: 80%;"}

+ **HW1 is due next week and you have Lab 2 on Friday!**

+ **Next time**: 2D categorical data

+ Recommended reading: 

  + [CW Chapter 16.2 Visualizing the uncertainty of point estimates](https://clauswilke.com/dataviz/visualizing-uncertainty.html#visualizing-the-uncertainty-of-point-estimates)

:::


---
title: "Inference with Linear Regression"
author: "Prof Ron Yurko"
footer:  "[statds-36315-spring25](https://ryurko.github.io/statds-36315-spring25/)"
date: 2025-02-19
engine: knitr
format:
  revealjs:
    theme: theme.scss
    chalkboard: true
    pdf-separate-fragments: true
    slide-number: c/t
    smaller: true
    code-line-numbers: true
    linestretch: 1.25
    html-math-method:
      method: mathjax
      url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

```{r}
#| include: false
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center"
)

library(tidyverse)
library(palmerpenguins)
```

## Announcements, previously, and today...

::: {style="font-size: 60%;"}

**HW4 is due TONIGHT by 11:59 PM and you have Lab 6 again on Friday!**

**Take-home exam is next week Wednesday Feb 26th**

Here's how the exam will work:

+ I'll post the exam Monday evening, and it's due Wednesday by 11:59 PM EDT (Feb 26th)
  
+ Exam will cover material from HW 1-4 and Labs 1-6
  
+ Questions will be similar to homeworks but more open-ended, e.g, instead of "make a side-by-side violin plot..." I'll ask "Make a plot that compares the conditional distributions..."
  
+ __There will NOT be class on Wednesday Feb 26th__ 
  
+ Conflict Feb 26th? __Let me know ASAP!__ Day-of accommodations will NOT be made, late submissions will NOT be accepted

:::

. . .

::: {style="font-size: 60%;"}


Scatterplots are the most common visual for 2D quantitative variables

+ Many ways to incorporate additional dimensions in scatterplots, e.g., color and shape

Linear regression is by far the most common model for describing the relationship between 2+ quantitative variables

+ Can also: transform the outcome, transform the covariates, do nonparametric "smoothing"

Use graphs to assess linear regression assumptions, i.e., residual diagnostic plots

:::

. . .

::: {style="font-size: 80%;"}

**TODAY:** More linear regression and inference with linear regression

:::


---

## Displaying trend lines: linear regression

```{r}
#| code-line-numbers: "4"
penguins |>
  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm")
```

---

## Assessing assumptions of linear regression

Linear regression assumes $Y_i \overset{iid}{\sim} N(\beta_0 + \beta_1 X_i, \sigma^2)$

- If this is true, then $Y_i - \hat{Y}_i \overset{iid}{\sim} N(0, \sigma^2)$

. . .

Plot residuals against $\hat{Y}_i$, __residuals vs fit__ plot

- Used to assess linearity, any divergence from mean 0

- Used to assess equal variance, i.e., if $\sigma^2$ is homogenous across predictions/fits $\hat{Y}_i$

. . .

More difficult to assess the independence and fixed $X$ assumptions

- Make these assumptions based on subject-matter knowledge

---

## Residual vs fit plots


```{r}
lin_reg <- lm(body_mass_g ~ flipper_length_mm, data = penguins)

tibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |>
  ggplot(aes(x = fits, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")
```


---

## Residual vs fit plots

```{r}
#| code-line-numbers: "5"
tibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |>
  ggplot(aes(x = fits, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  geom_smooth()
```

---

## Examples of Residual-vs-Fit Plots



---

## More fun with `penguins`...

Demo 03: Walk through an example of plotting/running different linear regression models

+ **Outcome**: bill depth (in mm)

+ **Covariates**: bill length (in mm) *and* species

. . .

Linear regression models we will consider:

1. `bill_depth_mm` ~ `bill_length_mm`

2.  `bill_depth_mm` ~ `bill_length_mm` + `species`

3.  `bill_depth_mm` ~ `bill_length_mm` + `species` + `bill_length_mm` $\times$ `species`

---

## Model 1: `bill_depth_mm` ~ `bill_length_mm`

```{r}
#| warning: false
#| message: false
#| echo: false
penguins |>
  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE)
```


---

## Model 1: `bill_depth_mm` ~ `bill_length_mm`

```{r}
#| attr-output: "style='font-size: 0.6em;'"
summary(lm(bill_depth_mm ~ bill_length_mm, data = penguins))
```

---

## How are the intercept and slope estimated?

::: {style="font-size: 80%;"}

We have data $(X_i, Y_i)$. Want to estimate $\beta_0$ and $\beta_1$, where $\mathbb{E}[Y | X] = \beta_0 + \beta_1 X$

If we had $\hat{\beta}_0$ and $\hat{\beta}_1$, then $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$

The estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ are obtained by solving

$$\arg \min_{\beta_0,\beta_1} \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i)^2$$

+ Remember that $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$, so the above is saying: "_Give me the $\hat{Y}_i$ such that $(Y_i - \hat{Y}_i)^2$ is minimized, on average_"


The estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ are:

$$\hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2} = \frac{\text{Cov}(X,Y)}{\text{Var}(X)}$$

$$\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}$$
            
:::

---

## Assessing the Fit of Linear Regression

::: {style="font-size: 80%;"}

Intuitively, the more $X$ and $Y$ are correlated, the better the fit of the linear regression

Correlation is defined as

$$\rho = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum_{i=1}^n (X_i - \bar{X})^2 \cdot \sum_{i=1}^n (Y_i - \bar{Y})^2}} = \frac{\text{Cov}(X,Y)}{ \sqrt{\text{Var}(X) \cdot \text{Var}(Y)} }$$

- Correlation is just a standardized covariance, where $-1 \leq \rho \leq 1$.

- More generally, $R^2$ measures the fraction of variability in the outcome _accounted by_ the covariates:

$$R^2 = 1 - \frac{\sum_{i=1}^n (Y_i - \hat{Y}_i)^2}{\sum_{i=1}^n (Y_i - \bar{Y})^2} = 1 - \frac{\text{SS}_{\text{residuals}}}{\text{SS}_{\text{total}}}$$

The higher $R^2$, the more the association. When linear regression has one covariate, $R = \rho$

:::

---

## Multiple Linear Regression

Let's say we have a bunch of covariates $X_1,X_2,\dots,X_p$

The statistical model for multiple linear regression is

$$Y_i \stackrel{iid}{\sim} N(\beta_0 + \beta_1 X_{i1} + \cdots + \beta_k X_{ip}, \sigma^2), \hspace{0.1in} \text{for all } i=1,\dots,n$$

- Covariates can be quadratic, cubic, etc. forms of other covariates, so this is quite flexible

- How do we know when we've included the "right" covariates?

- The higher $R^2$, the more the association. So, maximize $R^2$?

. . .

- However, adding more covariates _always_ increases $R^2$. Better to look at "adjusted $R^2$", which accounts for this

- Also common: AIC and BIC (smaller is better)

---

## Special Case - Categorical Variables

Can include categorical variables in multiple linear regression, but need to code them as "dummy variables" (i.e., indicator variables)

Say a categorical variable has $k \geq 2$ levels. Need to create $(k-1)$ indicator variables, equal to 1 for _one_ category and 0 otherwise

__Important__: Categorical variable may be coded numerically (e.g., Agree = 1, Disagree = -1, Not Sure = 0)

- If you put this variable straight into `lm()`, it will fit a very different model!


---

## Understanding the Categorical Variables Example

Example: Penguins species: _Adelie_, _Chinstrap_, _Gentoo._ There are $k = 3$ levels.

Create an indicator for _Chinstrap_ and _Gentoo_: $I_C$ and $I_G$.  

- If $I_C = I_G = 0$, then the penguin must be _Adelie_

The statistical model would be $Y_i \stackrel{iid}{\sim} N(\beta_0 + \beta_C I_C + \beta_G I_G, \sigma^2)$

- $\beta_0$: <!--Mean for _Adelie_-->

- $\beta_0 + \beta_C$: <!--Mean for _Chinstrap_-->

- $\beta_0 + \beta_G$: <!--Mean for _Gentoo_-->


- Significant $\beta_C$ $\rightarrow$ <!--Chinstrap and Adelie are different-->

- Significant $\beta_G$ $\rightarrow$ <!--Gentoo and Adelie are different-->

- How to compare Chinstrap and Gentoo? <!--Need to fit a different model.-->


---

## Understanding Interactions (Categorical Example)

::: {style="font-size: 80%;"}

Say we also have a quantitative variable $X$ (bill length). Consider two statistical models:

1. $Y_i \stackrel{iid}{\sim} N(\beta_0 + \beta_X X + \beta_C I_C + \beta_G I_G, \sigma^2)$

2. $Y_i \stackrel{iid}{\sim} N(\beta_0 + \beta_X X + \beta_C I_C + \beta_G I_G + \beta_{CX} I_C X + \beta_{GX} I_G X, \sigma^2)$

:::

. . .

::: {style="font-size: 80%;"}

For Model 1...

- The intercept for Adelie is $\beta_0$; for Chinstrap it is $\beta_0 + \beta_C$; for Gentoo it is $\beta_0 + \beta_G$

- The slope for all species is $\beta_X$.

:::

. . .

::: {style="font-size: 80%;"}

For Model 2...

- The intercept for Adelie is $\beta_0$; for Chinstrap it is $\beta_0 + \beta_C$; for Gentoo it is $\beta_0 + \beta_G$.

- The slope for Adelie is $\beta_X$; for Chinstrap it is $\beta_X + \beta_{CX}$; for Gentoo it is $\beta_X + \beta_{GX}$

:::

. . .

::: {style="font-size: 80%;"}

Significant coefficient for categorical variables by themselves? Significantly different intercepts

Significant coefficient for interactions with categorical variables? Significantly different slopes

:::

---

## Model 2: `bill_depth_mm` ~ `bill_length_mm` + `species`

```{r}
#| echo: false
#| fig-align: center
depth_lm_species_add <- lm(bill_depth_mm ~ bill_length_mm + species,
                           data = penguins)

# Calculate species-specific intercepts in order:
intercepts <- # First for `Adelie` it's just the initial intercept
  c(coef(depth_lm_species_add)["(Intercept)"],
    # Next for `Chinstrap` it's the intercept plus the `Chinstrap` term:
    coef(depth_lm_species_add)["(Intercept)"] + 
      coef(depth_lm_species_add)["speciesChinstrap"],
    # And finally for `Gentoo` it's again the intercept plus the `Gentoo` term
    coef(depth_lm_species_add)["(Intercept)"] + 
      coef(depth_lm_species_add)["speciesGentoo"])

# Create a small table to store the intercept, slopes, and species:
lines_tbl <- tibble("intercepts" = intercepts,
                    # Slopes are the same for each, thus use rep()
                    "slopes" = rep(coef(depth_lm_species_add)["bill_length_mm"],
                                   3),
                    # And the levels of species:
                    "species" = levels(penguins$species))
penguins |>
  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(alpha = 0.5) +
  geom_abline(data = lines_tbl,
              aes(intercept = intercepts, slope = slopes,
                  color = species)) +
  labs(x = "Bill length (mm)", y = "Bill depth (mm)", 
       title = "Bill depth versus weight by species")
```

---

```{r}
#| attr-output: "style='font-size: 0.6em;'"
summary(lm(bill_depth_mm ~ bill_length_mm + species, data = penguins))
```

---

## Model 3: `bill_depth_mm` ~ `bill_length_mm` + `species` + `bill_length_mm` $\times$ `species`

```{r}
#| echo: false
#| fig-align: center
penguins |>
  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(x = "Bill length (mm)", y = "Bill depth (mm)", 
       title = "Bill depth versus weight by species")
```

---

```{r}
#| attr-output: "style='font-size: 0.5em;'"
summary(lm(bill_depth_mm ~ bill_length_mm * species, data = penguins))
```

---

## A Few Linear Regression Warnings

::: {style="font-size: 90%;"}

**Simpson's Paradox**

+ There is a negative linear relationship between two variables but a positive linear relationship within every subpopulation

+ In these cases, subgroup analysis is especially important

:::

. . .

::: {style="font-size: 90%;"}

**Is the intercept meaningful?**

+ Think about whether $X = 0$ makes scientific sense for a particular variable before you interpret the intercept

:::

. . .

::: {style="font-size: 90%;"}

**Interpolation versus Extrapolation**

+ Interpolation is defined as prediction within the range of a variable

+ Extrapolation is defined as prediction outside the range of a variable

+ Generally speaking, interpolation is more reliable than extrapolation (less sensitive to model misspecification)

:::

---

## Extrapolation Example

```{r}
#| echo: false
#| fig-align: center
set.seed(1389)
fake_data <- tibble(x = runif(100, min = -10, max = -3)) |>
  mutate(y = rnorm(n = 100, mean = 3 * x^2, sd = 3))
fake_data |>
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw()
```

---

## Extrapolation Example

```{r}
#| echo: false
#| fig-align: center
fake_data |>
  ggplot(aes(x, y)) +
  geom_point() +
  scale_x_continuous(limits = c(-11, 11)) +
  geom_smooth(method = "lm") +
  theme_bw()
```


---

## Extrapolation Example

```{r}
#| echo: false
#| fig-align: center
fake_data |>
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(-11, 11)) +
  stat_function(
    fun = function (x) 3 * x^2,
    linetype = "dashed", color = "red") +
  theme_bw()
```



---

## Recap and next steps

::: {style="font-size: 90%;"}

Use graphs to assess linear regression assumptions, i.e., residual diagnostic plots

Discussed interpretation of coefficients, the role of categorical variables, and interactions

Highlighted common problems to consider: Simpson's Paradox, intercept meaning, and extrapolation

:::

. . .

::: {style="font-size: 90%;"}

+ **HW4 is due TONIGHT and you have Lab 6 on Friday**

+ **Graphics critique due Feb 28th!**

+ **Next time**: Midsemester Review (take-home exam on Feb 26th)

+ Recommended reading: [CW Chapter 12 Visualizing associations among two or more quantitative variables](https://clauswilke.com/dataviz/visualizing-associations.html)

:::


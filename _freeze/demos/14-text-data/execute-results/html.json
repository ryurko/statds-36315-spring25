{
  "hash": "5c3a48f32990025f65d0406f4895f1cf",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Demo 14: Tidy Text Data and Sentiment Analysis\"\nformat: html\n---\n\n\n\n\n# The Office Dinner Party script\n\nIn this demo we'll work with the script from the best episode of ['The Office': Season 4, Episode 13 - 'Dinner Party'](https://en.wikipedia.org/wiki/Dinner_Party_(The_Office)). We can access the script using the [`schrute` package (yes this is a real thing)](https://cran.r-project.org/web/packages/schrute/vignettes/theoffice.html):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"schrute\")\nlibrary(tidyverse)\nlibrary(schrute)\n\n# Create a table from this package just corresponding to the Dinner Party episode:\ndinner_party_table <- theoffice |>\n  dplyr::filter(season == 4) |>\n  dplyr::filter(episode == 13) |>\n  # Just select columns of interest:\n  dplyr::select(index, character, text)\n```\n:::\n\n\n\n\n# TidyText processing of script\n\nIt's very common to access data where a single column contains a long string of text, so that you have a long string of text in each row that you have to process. For instance, with the `dinner_party_table` dataset constructed above, the `text` column contains long strings which we can see by printing out the first so many rows:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(dinner_party_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  index character text                                                          \n  <int> <chr>     <chr>                                                         \n1 16791 Stanley   This is ridiculous.                                           \n2 16792 Phyllis   Do you have any idea what time we'll get out of here?         \n3 16793 Michael   Nobody likes to work late, least of all me. Do you have plans…\n4 16794 Jim       Nope I don't, remember when you told us not to make plans 'ca…\n5 16795 Michael   Yes I remember. Mmm, this is B.S. This is B.S. Why are we her…\n6 16796 Dwight    Thank you Michael.                                            \n```\n\n\n:::\n:::\n\n\n\n\nThere are many ways to process text, but by far the simplest is to use the [`tidytext` package which has a fantastic free online book demonstrating how to use it](http://tidytextmining.com/). Rather than working with document-term matrices directly like the [`tm` package](https://cran.r-project.org/web/packages/tm/tm.pdf), instead we'll consider a __long, tidy__ table with __one-token-per-document-per-row__. Following the definition in the [`tidytext` book](https://www.tidytextmining.com/tidytext.html#word-frequencies):\n\n> A token is a meaningful unit of text, most often a word, that we are interested in using for further analysis, and tokenization is the process of splitting text into tokens.\n\nWe can create this tidy dataset from our `dinner_party_table` using the `unnest_tokens()` function to break the various lines into tokens (aka tokenization). We specify what the new column will be called, `word`, given the source of the text, which is `text` here:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"tidytext\")\nlibrary(tidytext)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'tidytext' was built under R version 4.2.3\n```\n\n\n:::\n\n```{.r .cell-code}\ntidy_dinner_party_tokens <- dinner_party_table |>\n  unnest_tokens(word, text)\n# View the first so many rows:\nhead(tidy_dinner_party_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  index character word      \n  <int> <chr>     <chr>     \n1 16791 Stanley   this      \n2 16791 Stanley   is        \n3 16791 Stanley   ridiculous\n4 16792 Phyllis   do        \n5 16792 Phyllis   you       \n6 16792 Phyllis   have      \n```\n\n\n:::\n:::\n\n\n\n\nEach row of `tidy_dinner_party_tokens` corresponds to a single word spoken by a character (see `character` column) in the show during a single line (as reflected by the `index` column). We can see that the other columns are still in the data, but the `text` column has essentially been broken up into many rows. By default, punctuation marks are removed and all of the text is converted to lower case (which was helpful given the many lines in this episode that are in all caps!). \n\nBefore we continue to explore this dataset, we will remove stop words and perform stemming. First, we load a table of stop words from the `tidytext` package and simply filter them out from our data:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load stop words in the tidytext package\ndata(stop_words)\n\n# Next we can use the filter function to remove all stop words:\ntidy_dinner_party_tokens <- tidy_dinner_party_tokens |>\n  filter(!(word %in% stop_words$word))\n# Alternatively, we can do this with the anti_join function:\n# tidy_dinner_party_tokens <- tidy_dinner_party_tokens |>\n#   anti_join(stop_words)\n```\n:::\n\n\n\n\nNext, we will use the `SnowballC` package to perform stemming:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"SnowballC\")\nlibrary(SnowballC)\n\ntidy_dinner_party_tokens <- tidy_dinner_party_tokens |>\n  # Create a new column stem with the wordStem function:\n  mutate(stem = wordStem(word))\n\n# View the update:\nhead(tidy_dinner_party_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  index character word       stem   \n  <int> <chr>     <chr>      <chr>  \n1 16791 Stanley   ridiculous ridicul\n2 16792 Phyllis   idea       idea   \n3 16792 Phyllis   time       time   \n4 16793 Michael   likes      like   \n5 16793 Michael   late       late   \n6 16793 Michael   plans      plan   \n```\n\n\n:::\n:::\n\n\n\n\n# Frequency and TF-IDF review\n\nFrom this dataset, we technically have everything we need to construct the wider document-term (or term-document) matrices. But in the current format, we can take advantage of the `tidyverse` syntax we're used to with `group_by()` and `summarize()` to compute quantities we're interested in the ways we want. For instance, we can compute the frequency of each `stem` across all spoken lines and characters then create a word cloud by just grabbing the appropriate columns.\n\nWord clouds are probably the most well-known way of visualizing text data. Word clouds show the most common words in a set of documents, with more common words being bigger in the word cloud. To make word clouds in `R`, you'll need the `wordcloud` library. When you look at the help documentation for the `wordcloud()` function, you'll see that there are two main arguments: `words` and `freq`, which correspond to the words in the documents and their frequencies, respectively.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First create a dataset counting the number of each stem across the full script\ntoken_summary <- tidy_dinner_party_tokens |>\n  group_by(stem) |>\n  # Instead of summarize we can use the count function\n  count() |>\n  ungroup() \n\n# Now make a wordcloud :\nlibrary(wordcloud)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: RColorBrewer\n```\n\n\n:::\n\n```{.r .cell-code}\nwordcloud(words = token_summary$stem, freq = token_summary$n)\n```\n\n::: {.cell-output-display}\n![](14-text-data_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\nThere are a few things we can do to make the word cloud a bit prettier. First, it is more typical to place the biggest words in the center of the word cloud; this can be done by setting `random.order = FALSE` within `wordcloud()`. Furthermore, in the above word cloud we see that there are a bunch of tiny terms, and usually these are thrown out; you can control how many words are plotted on your word cloud by controlling `max.words` within `wordcloud()`. For example, we set `max.words = 100`, meaning the resulting word cloud displays the 100 most frequent words. We can also change the color palette; above you can see that loading the `wordcloud` library automatically loads the `RColorBrewer` library, and you can use functions like `brewer.pal()` like I do below to set the colors of the word cloud. There are lots of arguments within the `wordcloud()` function, so I encourage you to explore this function if you're interested.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwordcloud(words = token_summary$stem, freq = token_summary$n, \n          random.order = FALSE, max.words = 100, \n          colors = brewer.pal(8, \"Dark2\"))\n```\n\n::: {.cell-output-display}\n![](14-text-data_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\nWhile could treat each spoken line as a separate document (`index`), instead we can focus on a collection of documents based on the characters speaking the lines (`character`). This corresponds to grouping by `character` instead of `index`. The following creates the token/stem count summary by character:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncharacter_token_summary <- tidy_dinner_party_tokens |>\n  group_by(character, stem) |>\n  # Instead of summarize we can use the count function\n  count() |>\n  ungroup() \nhead(character_token_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  character stem       n\n  <chr>     <chr>  <int>\n1 All       cheer      1\n2 Andy      anim       1\n3 Andy      bet        1\n4 Andy      capit      1\n5 Andy      dinner     1\n6 Andy      flower     2\n```\n\n\n:::\n:::\n\n\n\n\nWe can easily compute and join the TF-IDF values for each `stem` treating the different characters as documents with the `bind_tf_idf()` function. All this requires is three input columns referring to words, documents, and counts. TF-IDF will down-weigh words that occur frequently across all documents and will upweigh words that uniquely occur in certain documents. In our example, these are: `stem`, `character`, `n`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncharacter_token_summary <- character_token_summary |>\n  bind_tf_idf(stem, character, n)\nhead(character_token_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  character stem       n     tf   idf tf_idf\n  <chr>     <chr>  <int>  <dbl> <dbl>  <dbl>\n1 All       cheer      1 1      2.77  2.77  \n2 Andy      anim       1 0.0476 2.77  0.132 \n3 Andy      bet        1 0.0476 2.08  0.0990\n4 Andy      capit      1 0.0476 2.77  0.132 \n5 Andy      dinner     1 0.0476 0.981 0.0467\n6 Andy      flower     2 0.0952 2.77  0.264 \n```\n\n\n:::\n:::\n\n\n\n\nNote that the `tf` column here refers to the term-frequency calculated as the number of times the character said that specific word/stem (by that character) divided by the total number of words/stems spoken by that character. The `idf` column refers to the inverse-document frequency column from lecture. \n\nWe can proceed to create a visual displaying the top 10 words/stems in terms of importance by TF-IDF for characters in the episode. For simplicity, we'll just consider the four characters with the most lines in the episode based on the following:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsort(table(dinner_party_table$character), decreasing = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n    Michael         Jan         Jim         Pam      Dwight      Angela \n         89          73          38          28          16          12 \n       Andy       Woman   Officer 1   Officer 2         All Hunter's CD \n         10           5           3           2           1           1 \n     Michae     Officer     Phyllis     Stanley \n          1           1           1           1 \n```\n\n\n:::\n:::\n\n\n\n\nThe following code chunk now displays the top 10 stems sorted by TF-IDF for Michael, Jan, Jim, and Pam. We use the [`reorder_within()` function](https://juliasilge.com/blog/reorder-within/) from the `tidytext` package to make sure the bars are sorted within the facets:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncharacter_token_summary |>\n  # Just grab the four characters of interest:\n  filter(character %in% c(\"Michael\", \"Jan\", \"Jim\", \"Pam\")) |>\n  # Group by character and just use the top 10 based on tf_idf with slice_max\n  group_by(character) |>\n  slice_max(tf_idf, n = 10, with_ties = FALSE) |>\n  ungroup() |>\n  mutate(stem = reorder_within(stem, tf_idf, character)) |>\n  ggplot(aes(y = tf_idf, \n             # We use the fct_reorder to sort the bars within the facets\n             x = stem),\n         fill = \"darkblue\", alpha = 0.5) +\n  geom_col() +\n  coord_flip() +\n  scale_x_reordered() +\n  facet_wrap(~ character, ncol = 2, scales = \"free\") +\n  labs(y = \"TF-IDF\", x = NULL)\n```\n\n::: {.cell-output-display}\n![](14-text-data_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n# Sentiment Analysis\n\nThere are also techniques for measuring the overall \"sentiment\" of a document. These techniques are usually dictionary-based; for example, one can come up with a list of \"positive\" words and \"negative\" words (i.e., positive and negative dictionaries), and then count the number of positive and negative words that occur in a document. Thus, you can get an aggregate measure of how \"positive\" or \"negative\" a piece of text is. There are many different ways to define the \"sentiment\" of text (see the References and Resources section below); for this example, we're just going to focus on \"positive\" versus \"negative\" text.\n\nAgain we're going to focus on the episode's script, and first, we're going to process the data as we did before in the previous section. However, unlike the previous section, we are *not* going to remove stop words or perform stemming - for sentiment analysis, it's important to keep individual words that may have certain types of sentiment.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_all_tokens <- dinner_party_table |>\n  unnest_tokens(word, text)\n```\n:::\n\n\n\n\nNow: Which of these words are \"positive\" words and which of these words are \"negative\" words? There's a function in the `tidytext` library called `get_sentiments()`. There are many different \"sentiment dictionaries\" available within this function, and we're going to just focus on `get_sentiments(\"bing\")`, which provides a bunch of words that are deemed either \"positive\" or \"negative\" in a binary fashion:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_sentiments(\"bing\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6,786 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows\n```\n\n\n:::\n:::\n\n\n\n\nIn what follows, we're going to join the binary sentiment assignment to the tidy token table - and only keep the words with an assignment based on the `inner_join()` function:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Label each word with a positive or negative sentiment\n# (if it has one)\ntidy_sentiment_tokens <- tidy_all_tokens |>\n  inner_join(get_sentiments(\"bing\"))\n# View the head of this:\nhead(tidy_sentiment_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  index character word       sentiment\n  <int> <chr>     <chr>      <chr>    \n1 16791 Stanley   ridiculous negative \n2 16793 Michael   likes      positive \n3 16793 Michael   work       positive \n4 16795 Michael   enough     positive \n5 16795 Michael   enough     positive \n6 16795 Michael   mad        negative \n```\n\n\n:::\n:::\n\n\n\n\nSo now we can see how the characters vary in terms of \"positive\" or \"negative\" words. We can simply count up the total number of \"positive\" and \"negative\" words said by each character:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncharacter_sentiment_summary <- tidy_sentiment_tokens |>\n  group_by(character, sentiment) |>\n  summarize(n_words = n()) |>\n  ungroup()\n```\n:::\n\n\n\n\nWe can then display how the characters compare with stacked bar charts indicating how many positive and negative words were said by each character (ordering the characters by the total number of assigned words):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncharacter_sentiment_summary |>\n  group_by(character) |>\n  mutate(total_assigned_words = sum(n_words)) |>\n  ungroup() |>\n  # Reorder the characters by the total number of words:\n  mutate(character = fct_reorder(character, total_assigned_words)) |>\n  # Display stacked bars\n  ggplot(aes(x = character, y = n_words, fill = sentiment)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  scale_fill_manual(values = c(\"darkred\", \"darkblue\")) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](14-text-data_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\nFrom this, we can see the most characters seem to say more positive words than negative words (except for Angela...). But we can see how the proportions within characters appears to vary, e.g., Pam has a much higher proportion of positive words than Jim. \n\nIt's also common to make word clouds of \"positive\" words and \"negative\" words, so that one can get an idea of the common words expressing \"positive\" or \"negative\" sentiment in a set of documents. Below, we separate words said by the two characters with the most dialogue in the episode, Michael and Jan, then separate out their positive and negative words to create word clouds.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First for Michael, make the positive summary:\nmichael_positive_summary <- tidy_sentiment_tokens |>\n  filter(character == \"Michael\", sentiment == \"positive\") |>\n  group_by(word) |>\n  count()\n# Now Michael's negative summary:\nmichael_negative_summary <- tidy_sentiment_tokens |>\n  filter(character == \"Michael\", sentiment == \"negative\") |>\n  group_by(word) |>\n  count()\n\n# Now make the word clouds for these two tables:\npar(mfrow = c(1,2))\n# word cloud for Michael's positive words\nwordcloud(words = michael_positive_summary$word,\n          freq = michael_positive_summary$n,\n          random.order = FALSE, color = \"darkblue\", \n          min.freq = 1)\ntitle(main = \"Michael's Positive Words\")\n# word cloud for Michael's negative words\nwordcloud(words = michael_negative_summary$word,\n          freq = michael_negative_summary$n,\n          random.order = FALSE, color = \"darkred\", \n          min.freq = 1)\ntitle(main = \"Michael's Negative Words\")\n```\n\n::: {.cell-output-display}\n![](14-text-data_files/figure-html/unnamed-chunk-17-1.png){width=960}\n:::\n\n```{.r .cell-code}\n# Next for Jan, make the positive summary:\njan_positive_summary <- tidy_sentiment_tokens |>\n  filter(character == \"Jan\", sentiment == \"positive\") |>\n  group_by(word) |>\n  count()\n# Now Jan's negative summary:\njan_negative_summary <- tidy_sentiment_tokens |>\n  filter(character == \"Jan\", sentiment == \"negative\") |>\n  group_by(word) |>\n  count()\n\n# Now make the word clouds for these two tables:\npar(mfrow = c(1,2))\n# word cloud for Jan's positive words\nwordcloud(words = jan_positive_summary$word,\n          freq = jan_positive_summary$n,\n          random.order = FALSE, color = \"darkblue\", \n          min.freq = 1)\ntitle(main = \"Jan's Positive Words\")\n# word cloud for Jan's negative words\nwordcloud(words = jan_negative_summary$word,\n          freq = jan_negative_summary$n,\n          random.order = FALSE, color = \"darkred\", \n          min.freq = 1)\ntitle(main = \"Jan's Negative Words\")\n```\n\n::: {.cell-output-display}\n![](14-text-data_files/figure-html/unnamed-chunk-17-2.png){width=960}\n:::\n:::\n\n\n\n\n\n# More resources\n\n+ More on using the [`tidytext` package](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html)\n\n+ [Tidy Text Mining with R](http://tidytextmining.com/)\n\n+ [Supervised Machine Learning for Text Analysis in R](https://smltar.com/)\n\n+ [`quanteda` package](https://quanteda.io/)\n\n\n\n\n",
    "supporting": [
      "14-text-data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "59a0208759962a65c35969571197a5b2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sentiment Analysis and Topic Models\"\nauthor: \"Prof Ron Yurko\"\nfooter:  \"[statds-36315-spring25](https://ryurko.github.io/statds-36315-spring25/)\"\ndate: 2025-04-16\nengine: knitr\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    pdf-separate-fragments: true\n    slide-number: c/t\n    smaller: true\n    code-line-numbers: true\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n\n## Announcements, previously, and today...\n\n::: {style=\"font-size: 95%;\"}\n\n**You should be working on your final projects!**\n\n**You must email me the slides for your presentation by 10 AM ET the day of your presentation - either a Google slides link or PDF file**\n\n**You do have lab this week!**\n\n:::\n\n. . .\n\n::: {style=\"font-size: 95%;\"}\n\n**Last time:** introduction to text data and word clouds\n\n\n**TODAY:** More text data!\n\n\n:::\n\n\n---\n\n## Reminder: The Office text analysis\n\n- We starting working with the script from the best episode of ['The Office': Season 4, Episode 13 - 'Dinner Party'](https://en.wikipedia.org/wiki/Dinner_Party_(The_Office))\n\n- We can access the script using the [`schrute` package (yes this is a real thing)](https://cran.r-project.org/web/packages/schrute/vignettes/theoffice.html):\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(schrute)\n# Create a table from this package just corresponding to the Dinner Party episode:\ndinner_party_table <- theoffice |>\n  filter(season == 4, episode == 13) |>\n  # Just select columns of interest:\n  dplyr::select(index, character, text)\nhead(dinner_party_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  index character text                                                          \n  <int> <chr>     <chr>                                                         \n1 16791 Stanley   This is ridiculous.                                           \n2 16792 Phyllis   Do you have any idea what time we'll get out of here?         \n3 16793 Michael   Nobody likes to work late, least of all me. Do you have plans…\n4 16794 Jim       Nope I don't, remember when you told us not to make plans 'ca…\n5 16795 Michael   Yes I remember. Mmm, this is B.S. This is B.S. Why are we her…\n6 16796 Dwight    Thank you Michael.                                            \n```\n\n\n:::\n:::\n\n\n\n\n\n---\n\n## Tokenize text into long format\n\n- Convert raw text into long, tidy table with one-token-per-document-per-row\n\n  - A __token__ equals a unit of text - typically a word\n  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidytext)\ntidy_dinner_party_tokens <- dinner_party_table |>\n  unnest_tokens(word, text)\nhead(tidy_dinner_party_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  index character word      \n  <int> <chr>     <chr>     \n1 16791 Stanley   this      \n2 16791 Stanley   is        \n3 16791 Stanley   ridiculous\n4 16792 Phyllis   do        \n5 16792 Phyllis   you       \n6 16792 Phyllis   have      \n```\n\n\n:::\n:::\n\n\n\n\nEasy to convert text into DTM format using [`tidytext` package](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html)\n\n---\n\n## Remove stop words\n\n- Load `stop_words` from `tidytext`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(stop_words)\n\ntidy_dinner_party_tokens <- tidy_dinner_party_tokens |>\n  filter(!(word %in% stop_words$word))\n\nhead(tidy_dinner_party_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  index character word      \n  <int> <chr>     <chr>     \n1 16791 Stanley   ridiculous\n2 16792 Phyllis   idea      \n3 16792 Phyllis   time      \n4 16793 Michael   likes     \n5 16793 Michael   late      \n6 16793 Michael   plans     \n```\n\n\n:::\n:::\n\n\n\n\n---\n\n## Apply stemming\n\n- Can use [`SnowballC` package](https://cran.r-project.org/web/packages/SnowballC/SnowballC.pdf) to perform stemming\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(SnowballC)\n\ntidy_dinner_party_tokens <- tidy_dinner_party_tokens |>\n  mutate(stem = wordStem(word))\n\nhead(tidy_dinner_party_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  index character word       stem   \n  <int> <chr>     <chr>      <chr>  \n1 16791 Stanley   ridiculous ridicul\n2 16792 Phyllis   idea       idea   \n3 16792 Phyllis   time       time   \n4 16793 Michael   likes      like   \n5 16793 Michael   late       late   \n6 16793 Michael   plans      plan   \n```\n\n\n:::\n:::\n\n\n\n\n---\n\n## TF-IDF weighting\n\n- We saw that `michael` was the largest word, but what if I'm interested in comparing text across characters (i.e., documents)?\n\n. . .\n\n- It’s arguably of more interest to understand which words are frequently used in one set of texts but not the other, i.e., which words are unique?\n\n- Many text analytics methods will __down-weight__ words that occur frequently across all documents\n\n. . .\n\n- __Inverse document frequency (IDF)__: for word $j$ we compute $\\text{idf}_j = \\log \\frac{N}{N_j}$\n\n  - where $N$ is number of documents, $N_j$ is number of documents with word $j$\n\n- Compute __TF-IDF__ $= w_{ij} \\times \\text{idf}_j$\n\n---\n\n## TF-IDF example with characters\n\nCompute and join TF-IDF using `bind_tf_idf()`:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncharacter_token_summary <- tidy_dinner_party_tokens |>\n  group_by(character, stem) |> \n  count() |>\n  ungroup() \n\ncharacter_token_summary <- character_token_summary |>\n  bind_tf_idf(stem, character, n) \ncharacter_token_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 597 × 6\n   character stem        n     tf   idf tf_idf\n   <chr>     <chr>   <int>  <dbl> <dbl>  <dbl>\n 1 All       cheer       1 1      2.77  2.77  \n 2 Andy      anim        1 0.0476 2.77  0.132 \n 3 Andy      bet         1 0.0476 2.08  0.0990\n 4 Andy      capit       1 0.0476 2.77  0.132 \n 5 Andy      dinner      1 0.0476 0.981 0.0467\n 6 Andy      flower      2 0.0952 2.77  0.264 \n 7 Andy      hei         1 0.0476 1.39  0.0660\n 8 Andy      helena      1 0.0476 2.77  0.132 \n 9 Andy      hump        2 0.0952 2.77  0.264 \n10 Andy      michael     1 0.0476 0.981 0.0467\n# ℹ 587 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n---\n\n## Top 10 words by TF-IDF for each character\n\n\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\ncharacter_token_summary |>\n  filter(character %in% c(\"Michael\", \"Jan\", \"Jim\", \"Pam\")) |>\n  group_by(character) |>\n  slice_max(tf_idf, n = 10, with_ties = FALSE) |>\n  ungroup() |>\n  mutate(stem = reorder_within(stem, tf_idf, character)) |>\n  ggplot(aes(y = tf_idf, x = stem),\n         fill = \"darkblue\", alpha = 0.5) +\n  geom_col() +\n  coord_flip() +\n  scale_x_reordered() +\n  facet_wrap(~ character, ncol = 2, scales = \"free\") +\n  labs(y = \"TF-IDF\", x = NULL)\n```\n\n::: {.cell-output-display}\n![](24-more-text_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## Other functions of text\n\n- We've just focused on word counts - __but there are many functions of text__\n\n- For example: __number of unique words__ is often used to measure vocabulary\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://pbs.twimg.com/media/DxCgsrxWwAAOWO3.jpg){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n---\n\n## Sentiment Analysis\n\n- The visualizations so far only look at word _frequency_ (possibly weighted with TF-IDF), but doesn't tell you _how_ words are used\n  \n. . .\n\n- A common goal in text analysis is to try to understand the overall __sentiment__ or \"feeling\" of text, i.e., __sentiment analysis__\n\n- Typical approach:\n\n  1.  Find a sentiment dictionary (e.g., \"positive\" and \"negative\" words)\n  \n  2. Count the number of words belonging to each sentiment\n  \n  3. Using the counts, you can compute an \"average sentiment\" (e.g., positive counts - negative counts)\n  \n. . .\n\n- This is called a __dictionary-based approach__\n\n- The __Bing__ dictionary (named after Bing Liu) provides 6,786 words that are either \"positive\" or \"negative\"\n\n---\n\n## Character sentiment analysis\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_sentiments(\"bing\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6,786 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n---\n\n## Character sentiment analysis\n\nJoin sentiment to token table (without stemming)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy_all_tokens <- dinner_party_table |>\n  unnest_tokens(word, text)\n\ntidy_sentiment_tokens <- tidy_all_tokens |>\n  inner_join(get_sentiments(\"bing\")) \n\nhead(tidy_sentiment_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  index character word       sentiment\n  <int> <chr>     <chr>      <chr>    \n1 16791 Stanley   ridiculous negative \n2 16793 Michael   likes      positive \n3 16793 Michael   work       positive \n4 16795 Michael   enough     positive \n5 16795 Michael   enough     positive \n6 16795 Michael   mad        negative \n```\n\n\n:::\n:::\n\n\n\n\n\n---\n\n## Character sentiment analysis\n\n\n\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\ntidy_sentiment_tokens |>\n  group_by(character, sentiment) |>\n  summarize(n_words = n()) |>\n  ungroup() |>\n  group_by(character) |>\n  mutate(total_assigned_words = sum(n_words)) |>\n  ungroup() |>\n  mutate(character = fct_reorder(character, total_assigned_words)) |>\n  ggplot(aes(x = character, y = n_words, fill = sentiment)) + \n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  scale_fill_manual(values = c(\"red\", \"blue\")) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](24-more-text_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## Topic Modeling\n\nEverything we've done still involves word counts in some way.\n\nStill have to deal with the high-dimensionality of individual words.\n\n. . .\n\n**Topic modeling: Envisions that there are just a few latent (\"hidden\") categories for each document**\n\n- These latent categories are called __topics__ \n\n- Each __topic__ encompasses a bunch of words that tend to occur together\n\n. . .\n\nThe workflow for topic modeling is:\n\n> Document-Term Matrix $\\rightarrow$ Latent Dirichlet Allocation $\\rightarrow$ $k$ topics\n\n**Allows you to find $k$ overall _topics_ that are being discussed across your documents**\n\n---\n\n## Hierarchy of Topic Modeling\n\n::: {style=\"font-size: 85%;\"}\n\nSay we have documents $D_1, \\dots, D_N$ and $1000$ words...\n\nHypothesis behind topic modeling: _Maybe these $N$ documents are really just about $k = 2$ topics (we could make $k$ bigger if we want)_\n\n:::\n\n. . .\n\n::: {style=\"font-size: 85%;\"}\n\n__Topic__: A collection of words with different probabilities of occurring.\n\n- Topic A: $\\beta_1^{A} =$ probability of Word 1, $\\beta_2^{A} =$ probability of Word 2, etc.\n\n- Topic B: $\\beta_1^{B} =$ probability of Word 1, $\\beta_2^{B} =$ probability of Word 2, etc.\n\n:::\n\n. . .\n\n::: {style=\"font-size: 85%;\"}\n\nWords may be prominent in both topics (e.g., $\\beta_1^{A} = \\beta_1^{B} = 0.8$) or rare in both topics. \n$\\sum_{j=1}^{1000} \\beta_j^{A} = 1$, but no constraint on $\\beta_j^{A} + \\beta_j^{B}$ for any $j$.\n\n:::\n\n. . .\n\n::: {style=\"font-size: 85%;\"}\n\n__Document__: A collection of topics with different proportions. \n\n- Document 1: \n\n  - $\\gamma_A =$ proportion of Topic A, \n  \n  - $\\gamma_B =$ proportion of Topic B.\n  \nFor each document, $\\gamma_A + \\gamma_B = 1$\n\n:::\n\n---\n\n## Generative model for text\n\nFor each document (assume number of words is known):\n\n+ For each word in the document:\n  \n  + Draw a topic assignment, e.g., pick Topic A with probability $\\gamma_A = 60\\%$ versus Topic B with $\\gamma_B = 40\\%$\n    \n  + Given a topic, choose the word with probabilities defined by $\\beta^{topic}$, e.g., if Topic A was picked then select a word using $\\beta^A$\n\n. . .\n\nThe result?\n\nIf $\\gamma_A = 60\\%, \\gamma_B = 40\\%$ for Document 1, then 60% of the time we'll use the $\\beta_1^{A}, \\beta_2^{A},\\dots,\\beta_{1000}^A$ probabilities for generating words, and 40% of the time we'll use the $\\beta_1^{B}, \\beta_2^{B},\\dots,\\beta_{1000}^B$\n\n---\n\n## What does topic modeling give us?\n\nFor simplicity, let's say that we have two topics, A and B.\n\nAfter we run topic modeling, we will get the following information:\n\n- For _each document_, we will get topic proportions $\\gamma_A$ and $\\gamma_B$\n\n- For _each topic_, we will get word probabilities $\\beta_1, \\dots, \\beta_J$\n\n. . .\n\nAfter running topic modeling, first you should understand what __topics__ have been identified.\n\nIntuition: For each topic, which words are most likely?\n\nCan compute and plot the top $\\beta_1, \\dots, \\beta_J$ for _each_ topic.:\n\n- This can be done with a bar plot, where the $\\beta_1,\\dots,\\beta_J$ are the height of the bars.\n  \n- Can also make a word cloud, where the $\\beta_1,\\dots,\\beta_J$ are the word sizes.\n\n---\n\n## Stranger Things topic model\n\nDemo using [dialogue from Stranger Things](https://github.com/rfordatascience/tidytuesday/tree/master/data/2022/2022-10-18)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nstranger_things_text <-\n  read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-10-18/stranger_things_all_dialogue.csv') |>\n  # Drop any lines with missing dialogue\n  filter(!is.na(dialogue))\nhead(stranger_things_text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 8\n  season episode  line raw_text     stage_direction dialogue start_time end_time\n   <dbl>   <dbl> <dbl> <chr>        <chr>           <chr>    <time>     <time>  \n1      1       1     9 [Mike] Some… [Mike]          Somethi… 01'44\"     01'48\"  \n2      1       1    10 A shadow gr… <NA>            A shado… 01'48\"     01'52\"  \n3      1       1    11 -It is almo… <NA>            It is a… 01'52\"     01'54\"  \n4      1       1    12 What if it'… <NA>            What if… 01'54\"     01'56\"  \n5      1       1    13 Oh, Jesus, … <NA>            Oh, Jes… 01'56\"     01'59\"  \n6      1       1    14 It's not th… <NA>            It's no… 01'59\"     02'00\"  \n```\n\n\n:::\n:::\n\n\n\n\nSee demo for all pre-processing steps\n\n\n\n\n\n\n\n\n\n---\n\n## Convert to input for `topicmodels` package\n\nNeed to covert `tidytext` output to `DocumentTermMatrix` object:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nepisode_dtm <- st_episode_word_summary |>\n  # Using the stems\n  cast_dtm(episode_id, word, n) #<<\n\nepisode_dtm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<<DocumentTermMatrix (documents: 34, terms: 7226)>>\nNon-/sparse entries: 22122/223562\nSparsity           : 91%\nMaximal term length: 27\nWeighting          : term frequency (tf)\n```\n\n\n:::\n:::\n\n\n\n\n\n---\n\n## Fit LDA model with `topicmodels`\n\nFit LDA model with specified `k` topics:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(topicmodels)\n\n# set a seed so that the output of the model is predictable\nst_lda <- LDA(episode_dtm, k = 2, control = list(seed = 1234))\nst_lda\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA LDA_VEM topic model with 2 topics.\n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\nThere are two quantities we'll grab from LDA:\n\n- `gamma`: Topic proportions for each document\n\n- `beta`: Word probabilities for each topic\n\n---\n\n## Working with $\\beta$s\n\nFor any topic $t$, we'll have probabilities $\\beta_1^{(t)},\\dots,\\beta_J^{(t)}$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nst_topics <- tidy(st_lda, matrix = \"beta\")\nst_topics\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 14,452 × 3\n   topic term      beta\n   <int> <chr>    <dbl>\n 1     1 00    8.62e- 4\n 2     2 00    6.96e- 4\n 3     1 10    2.40e- 4\n 4     2 10    4.64e- 5\n 5     1 100   3.32e- 4\n 6     2 100   9.72e- 5\n 7     1 12    2.38e- 4\n 8     2 12    4.89e- 5\n 9     1 12.3  4.81e- 5\n10     2 12.3  1.11e-90\n# ℹ 14,442 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n---\n\n## Working with $\\beta$s\n\nUsing `group_by()` and `top_n()`, can find the top $\\beta$s for each topic\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24-more-text_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n---\n\n## Finding Important Words\n\n::: {style=\"font-size: 85%;\"}\n\n**Which words are likely in Topic 2 _but not_ Topic 1?**\n\nThis is what TF-IDF weights are for, but unfortunately you can't use TF-IDF with topic modeling in this form...\n\n:::\n\n. . .\n\n::: {style=\"font-size: 85%;\"}\n\nIntuition: What $\\beta$s are big in Topic 2 _but not_ Topic 1?\n\n- Let $\\beta_1^{(1)}, \\dots, \\beta_J^{(1)}$ be the Topic 1 word probabilities\n\n- Let $\\beta_1^{(2)}, \\dots, \\beta_J^{(2)}$ be the Topic 2 word probabilities\n\n:::\n\n. . .\n\n::: {style=\"font-size: 95%;\"}\n\nConsider the following quantity:\n\n$$\n\\begin{align*}\n      \\log \\frac{\\beta_j^{(2)}}{\\beta_j^{(1)}}\n\\end{align*}\n$$\n<!-- - When $\\beta_j^{(2)} \\approx \\beta_j^{(1)}$, this will be close to 0. -->\n\n<!-- - When $\\beta_j^{(2)} >> \\beta_j^{(1)}$, this will be very positive. -->\n\n<!-- - When $\\beta_j^{(2)} << \\beta_j^{(1)}$, this will be very negative. -->\n\n:::\n\n---\n\n## Finding Important Words\n\nHere's a visual of $\\log \\frac{\\beta_j^{(2)}}{\\beta_j^{(1)}}$:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24-more-text_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## What to do after you've identified topics...\n\nRecall: Each document will have topic proportions $\\gamma_1,\\dots,\\gamma_k$ where for each document $i$, $\\sum_{j=1}^k \\gamma_j^{(i)} = 1$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nst_documents <- tidy(st_lda, matrix = \"gamma\") #<<\nst_documents |>\n  filter(document %in% c(\"1_1\", \"4_1\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  document topic     gamma\n  <chr>    <int>     <dbl>\n1 1_1          1 1.00     \n2 4_1          1 0.0000218\n3 1_1          2 0.0000337\n4 4_1          2 1.00     \n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\nIt can be helpful to see how these $\\gamma$ vary across different kinds of documents. For example:\n\n- Plot each $\\gamma$ over time (if your documents have a timestamp)\n\n- Examine the $\\gamma$ for different authors of documents.\n\n- Project the documents into a 2D space based on topic probabilities (e.g., via [Jensen-Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence))\n\n---\n\n## Recap and next steps\n\n- Can measure the \"sentiment\" of text with sentiment-based dictionaries\n\n- Topic modeling gives you two things:\n\n  + Topic Proportions: For each document, what is the proportion of each topic?\n\n  + Word Probabilities: For each topic, what is the probability of a certain word occurring?\n\n. . .\n\n+ **Your final lab is on Friday!**\n\n+ **Next time**: FINAL PROJECT PRESENTATIONS!\n\n+ Recommended Reading: [Text Mining With R](https://www.tidytextmining.com/), [Supervised Machine Learning for Text Analysis in R](https://smltar.com/)\n",
    "supporting": [
      "24-more-text_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "96441acbb7ac8643ae597e15161e6ec7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Graphical Inference for 1D Quantitative Data\"\nauthor: \"Prof Ron Yurko\"\nfooter:  \"[statds-36315-spring25](https://ryurko.github.io/statds-36315-spring25/)\"\ndate: 2025-02-10\nengine: knitr\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    pdf-separate-fragments: true\n    slide-number: c/t\n    smaller: true\n    code-line-numbers: true\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n\n## Announcements, previously, and today...\n\n::: {style=\"font-size: 75%;\"}\n\n+ **HW3 is due Wednesday by 11:59 PM and you have Lab 5 again on Friday!**\n\nOffice hours schedule:\n\n+ My office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\n\n+ Anna (zoom): Mondays @ 2 PM; and Perry (zoom): **Tuesdays** @ 11 AM\n\n:::\n\n. . .\n\n::: {style=\"font-size: 75%;\"}\n\n\n+ Smoothed densities are a flexible tool for visualizing 1D distribution\n\n+ There are two choices we need to make for kernel density estimation:\n\n  1. Bandwidth: Determines _smoothness_ of distribution, usually data-driven choice\n  \n  2. Kernel: Determines how much _influence_ each observation should have on each other during estimation, usually context driven\n\n+ Several other types of density-based displays: violins, ridges, beeswarm plots\n\n:::\n\n. . .\n\n::: {style=\"font-size: 75%;\"}\n\n**TODAY:**\n\n+ Graphical inference for 1D quantitative data\n\n+ Parametric density estimates\n\n+ ECDFs and Kolmogorov-Smirnov (KS) test\n\n:::\n\n---\n\n## Visualizing conditional distributions: violin plots\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"2-4\"}\npenguins |>\n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_violin() +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](08-1dquant-inference_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## Visualizing conditional distributions: violin plots\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\npenguins |>\n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_violin() + \n  geom_boxplot(width = .2) +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](08-1dquant-inference_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n---\n\n## Visualizing conditional distributions: [`ggridges`](https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1,3-4\"}\nlibrary(ggridges)\npenguins |>\n  ggplot(aes(x = flipper_length_mm, y = species)) +\n  geom_density_ridges(rel_min_height = 0.01)\n```\n\n::: {.cell-output-display}\n![](08-1dquant-inference_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## [Gallery of ggridges examples](https://wilkelab.org/ggridges/articles/gallery.html)\n\n![](https://wilkelab.org/ggridges/articles/gallery_files/figure-html/unnamed-chunk-4-1.png){fig-align=\"center\" width=100%}\n\n\n---\n\n## Visualizing conditional distributions: [`ggbeeswarm`](https://github.com/eclarke/ggbeeswarm)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1,3-4\"}\nlibrary(ggbeeswarm)\npenguins |>\n  ggplot(aes(x = flipper_length_mm, y = species)) +\n  geom_beeswarm(cex = 1.5) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](08-1dquant-inference_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n---\n\n## Kernel density estimation\n\n__Goal__: estimate PDF $f(x)$ for all possible values (assuming it is continuous & smooth)\n\n$$\n\\text{Kernel density estimate: } \\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\n$$\n\n::: {style=\"font-size: 75%;\"}\n\n- $n =$ sample size, $x =$ new point to estimate $f(x)$ (does NOT have to be in dataset!)\n\n- $h =$ __bandwidth__, analogous to histogram bin width, ensures $\\hat{f}(x)$ integrates to 1\n\n- $x_i =$ $i$th observation in dataset\n\n- $K_h(x - x_i)$ is the __Kernel__ function, creates __weight__ given distance of $i$th observation from new point \n  - as $|x - x_i| \\rightarrow \\infty$ then $K_h(x - x_i) \\rightarrow 0$, i.e. further apart $i$th row is from $x$, smaller the weight\n  \n  - as __bandwidth__ $h \\uparrow$ weights are more evenly spread out (as $h \\downarrow$ more concentrated around $x$) \n\n  - typically use [__Gaussian__ / Normal](https://en.wikipedia.org/wiki/Normal_distribution) kernel: $\\propto e^{-(x - x_i)^2 / 2h^2}$\n  \n  - $K_h(x - x_i)$ is large when $x_i$ is close to $x$\n  \n:::\n\n---\n\n## From the PDF to the CDF\n\n::: {style=\"font-size: 80%;\"}\n\n__Probability that continuous variable $X$ takes a particular value is 0__ \n\ne.g., $P$ (`flipper_length_mm` $= 200$) $= 0$\n\nInstead we use the __probability density function (PDF)__ to provide a __relative likelihood__ \n\n:::\n\n. . .\n\n::: {style=\"font-size: 80%;\"}\n\nFor continuous variables we can use the __cumulative distribution function (CDF)__,\n\n$$\nF(x) = P(X \\leq x)\n$$\n\n:::\n\n. . .\n\n::: {style=\"font-size: 80%;\"}\n\nFor $n$ observations we can easily compute the __Empirical CDF (ECDF)__:\n\n\n$$\\hat{F}_n(x)  = \\frac{\\text{# obs. with variable} \\leq x}{n} = \\frac{1}{n} \\sum_{i=1}^{n}1(x_i \\leq x)$$\n\n\n- where $1()$ is the indicator function, i.e. `ifelse(x_i <= x, 1, 0)`\n\n:::\n\n---\n\n## Display full distribution with ECDF plot\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\npenguins |>\n  ggplot(aes(x = flipper_length_mm)) + \n  stat_ecdf() +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](08-1dquant-inference_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## What's the relationship between these two?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-1dquant-inference_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## Comparing to theoretical distributions\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/2560px-Normal_Distribution_PDF.svg.png){fig-align=\"center\" width=100%}\n\n:::\n\n::: {.column width=\"50%\"}\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/Normal_Distribution_CDF.svg/2560px-Normal_Distribution_CDF.svg.png){fig-align=\"center\" width=100%}\n\n:::\n\n::::\n\n\n---\n\n## One-Sample Kolmogorov-Smirnov Test\n\n- We compare the ECDF $\\hat{F}(x)$ to a theoretical distribution's CDF $F(x)$\n\n- The one sample KS test statistic is: $\\text{max}_x |\\hat{F}(x) - F(x)|$\n\n![](https://upload.wikimedia.org/wikipedia/commons/c/cf/KS_Example.png){fig-align=\"center\" width=80%}\n\n---\n\n## Parametric Density Estimation\n\n+ Instead of trying to estimate the whole $f(x)$ non-parametrically, we can assume a particular $f(x)$ and estimate its parameters\n\n+ For example, assume $X_i \\sim N(\\mu, \\sigma^2)$. Then estimate the parameters:\n\n$$\n\\hat{\\mu} = \\bar{x}, \\hspace{0.1in} \\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}\n$$\n\n+ Then our density estimate is:\n\n$$\n\\hat{f}(x) = \\frac{1}{\\sqrt{2\\pi} \\hat{\\sigma}} \\exp \\left( - \\frac{(x - \\hat{\\mu})^2}{2\\hat{\\sigma}^2} \\right)\n$$\n\n---\n\n## Flipper length example\n\nWhat if we assume `flipper_length_mm` follows Normal distribution? \n\n+ i.e., `flipper_length_mm` $\\sim N(\\mu, \\sigma^2)$\n\nNeed estimates for mean $\\mu$ and standard deviation $\\sigma$:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nflipper_length_mean <- mean(penguins$flipper_length_mm, na.rm = TRUE)\nflipper_length_sd <- sd(penguins$flipper_length_mm, na.rm = TRUE)\n```\n:::\n\n\n\n\n. . .\n\nPerform one-sample KS test using [`ks.test()`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/ks.test.html):\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nks.test(x = penguins$flipper_length_mm, y = \"pnorm\",\n        mean = flipper_length_mean, sd = flipper_length_sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tAsymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  penguins$flipper_length_mm\nD = 0.12428, p-value = 5.163e-05\nalternative hypothesis: two-sided\n```\n\n\n:::\n:::\n\n\n\n\n\n---\n\n## Flipper length example\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-1dquant-inference_files/figure-revealjs/kstest-plot-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## Statistical Tests for Comparing Distributions\n\n::: {style=\"font-size: 85%;\"}\n\n+ We've focused on assessing if a single quantitative variable follows a particular distribution\n\n  + Logic of one-sample KS test: Compare empirical distribution to theoretical distribution\n  \n:::\n\n. . .\n\n::: {style=\"font-size: 85%;\"}\n\n**How do we compare multiple empirical distributions?**\n\nVery common scenario: Determine if a quantitative variable depends on a categorical variable, examples:\n  \n+ Clinical trials with multiple treatments\n  \n+ Assessing differences across race, gender, socioeconomic status\n  \n+ Industrial experiments, A/B testing\n  \n+ _Comparing song duration across different genres?_\n    \nCan use overlayed densities, side-by-side violin plots, facetted histograms\n\nRemember: plotting conditional distributions... but when are differences in a graphic _statistically significant_?\n\n:::\n\n---\n\n## [TidyTuesday](https://github.com/rfordatascience/tidytuesday) [Spotify Songs](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-21/readme.md) - Duration by Genre\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-1dquant-inference_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n---\n\n## [TidyTuesday](https://github.com/rfordatascience/tidytuesday) [Spotify Songs](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-21/readme.md) - Duration by Genre\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-1dquant-inference_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n---\n\n## Test difference between rap and rock?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-1dquant-inference_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n---\n\n## Kolmogorov-Smirnov Test... Again\n\nKS test can also be used to compare two empirical distributions $\\hat{F}_A(x)$ and $\\hat{F}_B$, via test statistic for __two samples__:\n\n$$\n\\text{max}_x |\\hat{F}_A(x) - \\hat{F}_B(x)|\n$$\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nspotify_songs <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')\nrap_duration <- spotify_songs |> filter(playlist_genre == \"rap\") |> pull(duration_ms)\nrock_duration <- spotify_songs |> filter(playlist_genre == \"rock\") |> pull(duration_ms)\n\nks.test(rap_duration, y = rock_duration)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tAsymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  rap_duration and rock_duration\nD = 0.22386, p-value < 2.2e-16\nalternative hypothesis: two-sided\n```\n\n\n:::\n:::\n\n\n\n\n\n\n---\n\n\n## Kolmogorov-Smirnov Test... Again\n\nKS test can also be used to compare two empirical distributions $\\hat{F}_A(x)$ and $\\hat{F}_B$, via test statistic for __two samples__:\n\n$$\n\\text{max}_x |\\hat{F}_A(x) - \\hat{F}_B(x)|\n$$\n\n\n\n\n---\n\n## Statistical Tests for Comparing Distributions\n\n::: {style=\"font-size: 75%;\"}\n\nInfinite number of ways that you can compare multiple quantitative distributions, 3 common ways:\n\n1. __Any difference at all?__ <!--Two sample KS test-->\n\n\n2. __Difference in means?__\n\n  - Null hypothesis: $H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_K$ (use `t.test` or `oneway.test()` functions)\n  \n  - Can assume the variances are all the same or differ\n  \n  - If reject, can only conclude __not all means are equal__\n  \n3. __Difference in variances?__\n\n  - Null hypothesis: $H_0: \\sigma^2_1 = \\sigma^2_2 = \\cdots = \\sigma^2_K$ (use `bartlett.test()` function)\n  \n  - If reject, can only conclude __not all variances are equal__\n  \nUnlike the KS test, __difference in means and variances are sensitive to non-Normality__\n\n  + Different distributions can yield insignificant results\n\n:::\n\n---\n\n## Test difference between rap and rock?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-1dquant-inference_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n---\n\n## Test difference between rap and rock?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-1dquant-inference_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n---\n\n## Test difference between pop and rap?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](08-1dquant-inference_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n---\n\n## Recap and next steps\n\n::: {style=\"font-size: 85%;\"}\n\n+ Introduced KS tests for testing differences in distributions\n\n+ But when are the differences we're seeing statistically significant?\n\n  + _Any_ distributional difference? $\\rightarrow$ KS test\n  \n  + Just care about mean differences? $\\rightarrow$ t-test\n  \n  + Just care about variance differences? $\\rightarrow$ Bartlett's test\n\n:::\n\n. . .\n\n::: {style=\"font-size: 85%;\"}\n\n+ **HW3 is due Wednesday and you have Lab 5 on Friday**\n\n+ **Next time**: Comparing Distributions and Statistical Power\n\n+ Recommended reading: [CW Chapter 8 Visualizing distributions: Empirical cumulative distribution functions and q-q plots](https://clauswilke.com/dataviz/ecdf-qq.html)\n\n:::\n\n",
    "supporting": [
      "08-1dquant-inference_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
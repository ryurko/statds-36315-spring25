{
  "hash": "c4fa04bace6e0fe7f6178843f6b32a9f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Inference with Linear Regression\"\nauthor: \"Prof Ron Yurko\"\nfooter:  \"[statds-36315-spring25](https://ryurko.github.io/statds-36315-spring25/)\"\ndate: 2025-02-19\nengine: knitr\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    pdf-separate-fragments: true\n    slide-number: c/t\n    smaller: true\n    code-line-numbers: true\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n\n## Announcements, previously, and today...\n\n::: {style=\"font-size: 60%;\"}\n\n**HW4 is due TONIGHT by 11:59 PM and you have Lab 6 again on Friday!**\n\n**Take-home exam is next week Wednesday Feb 26th**\n\nHere's how the exam will work:\n\n+ I'll post the exam Monday evening, and it's due Wednesday by 11:59 PM EDT (Feb 26th)\n  \n+ Exam will cover material from HW 1-4 and Labs 1-6\n  \n+ Questions will be similar to homeworks but more open-ended, e.g, instead of \"make a side-by-side violin plot...\" I'll ask \"Make a plot that compares the conditional distributions...\"\n  \n+ __There will NOT be class on Wednesday Feb 26th__ \n  \n+ Conflict Feb 26th? __Let me know ASAP!__ Day-of accommodations will NOT be made, late submissions will NOT be accepted\n\n:::\n\n. . .\n\n::: {style=\"font-size: 60%;\"}\n\n\nScatterplots are the most common visual for 2D quantitative variables\n\n+ Many ways to incorporate additional dimensions in scatterplots, e.g., color and shape\n\nLinear regression is by far the most common model for describing the relationship between 2+ quantitative variables\n\n+ Can also: transform the outcome, transform the covariates, do nonparametric \"smoothing\"\n\nUse graphs to assess linear regression assumptions, i.e., residual diagnostic plots\n\n:::\n\n. . .\n\n::: {style=\"font-size: 80%;\"}\n\n**TODAY:** More linear regression and inference with linear regression\n\n:::\n\n\n---\n\n## Displaying trend lines: linear regression\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\npenguins |>\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](11-regression-inference_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## Assessing assumptions of linear regression\n\nLinear regression assumes $Y_i \\overset{iid}{\\sim} N(\\beta_0 + \\beta_1 X_i, \\sigma^2)$\n\n- If this is true, then $Y_i - \\hat{Y}_i \\overset{iid}{\\sim} N(0, \\sigma^2)$\n\n. . .\n\nPlot residuals against $\\hat{Y}_i$, __residuals vs fit__ plot\n\n- Used to assess linearity, any divergence from mean 0\n\n- Used to assess equal variance, i.e., if $\\sigma^2$ is homogenous across predictions/fits $\\hat{Y}_i$\n\n. . .\n\nMore difficult to assess the independence and fixed $X$ assumptions\n\n- Make these assumptions based on subject-matter knowledge\n\n---\n\n## Residual vs fit plots\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlin_reg <- lm(body_mass_g ~ flipper_length_mm, data = penguins)\n\ntibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |>\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")\n```\n\n::: {.cell-output-display}\n![](11-regression-inference_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n---\n\n## Residual vs fit plots\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\ntibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |>\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth()\n```\n\n::: {.cell-output-display}\n![](11-regression-inference_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## Examples of Residual-vs-Fit Plots\n\n\n\n---\n\n## More fun with `penguins`...\n\nDemo 03: Walk through an example of plotting/running different linear regression models\n\n+ **Outcome**: bill depth (in mm)\n\n+ **Covariates**: bill length (in mm) *and* species\n\n. . .\n\nLinear regression models we will consider:\n\n1. `bill_depth_mm` ~ `bill_length_mm`\n\n2.  `bill_depth_mm` ~ `bill_length_mm` + `species`\n\n3.  `bill_depth_mm` ~ `bill_length_mm` + `species` + `bill_length_mm` $\\times$ `species`\n\n---\n\n## Model 1: `bill_depth_mm` ~ `bill_length_mm`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-regression-inference_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n---\n\n## Model 1: `bill_depth_mm` ~ `bill_length_mm`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(lm(bill_depth_mm ~ bill_length_mm, data = penguins))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```{style='font-size: 0.6em;'}\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1381 -1.4263  0.0164  1.3841  4.5255 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    20.88547    0.84388  24.749  < 2e-16 ***\nbill_length_mm -0.08502    0.01907  -4.459 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.922 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.05525,\tAdjusted R-squared:  0.05247 \nF-statistic: 19.88 on 1 and 340 DF,  p-value: 1.12e-05\n```\n\n\n:::\n:::\n\n\n\n\n---\n\n## How are the intercept and slope estimated?\n\n::: {style=\"font-size: 80%;\"}\n\nWe have data $(X_i, Y_i)$. Want to estimate $\\beta_0$ and $\\beta_1$, where $\\mathbb{E}[Y | X] = \\beta_0 + \\beta_1 X$\n\nIf we had $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, then $\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i$\n\nThe estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are obtained by solving\n\n$$\\arg \\min_{\\beta_0,\\beta_1} \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2$$\n\n+ Remember that $\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i$, so the above is saying: \"_Give me the $\\hat{Y}_i$ such that $(Y_i - \\hat{Y}_i)^2$ is minimized, on average_\"\n\n\nThe estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are:\n\n$$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}$$\n\n$$\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X}$$\n            \n:::\n\n---\n\n## Assessing the Fit of Linear Regression\n\n::: {style=\"font-size: 80%;\"}\n\nIntuitively, the more $X$ and $Y$ are correlated, the better the fit of the linear regression\n\nCorrelation is defined as\n\n$$\\rho = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^n (X_i - \\bar{X})^2 \\cdot \\sum_{i=1}^n (Y_i - \\bar{Y})^2}} = \\frac{\\text{Cov}(X,Y)}{ \\sqrt{\\text{Var}(X) \\cdot \\text{Var}(Y)} }$$\n\n- Correlation is just a standardized covariance, where $-1 \\leq \\rho \\leq 1$.\n\n- More generally, $R^2$ measures the fraction of variability in the outcome _accounted by_ the covariates:\n\n$$R^2 = 1 - \\frac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2} = 1 - \\frac{\\text{SS}_{\\text{residuals}}}{\\text{SS}_{\\text{total}}}$$\n\nThe higher $R^2$, the more the association. When linear regression has one covariate, $R = \\rho$\n\n:::\n\n---\n\n## Multiple Linear Regression\n\nLet's say we have a bunch of covariates $X_1,X_2,\\dots,X_p$\n\nThe statistical model for multiple linear regression is\n\n$$Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_1 X_{i1} + \\cdots + \\beta_k X_{ip}, \\sigma^2), \\hspace{0.1in} \\text{for all } i=1,\\dots,n$$\n\n- Covariates can be quadratic, cubic, etc. forms of other covariates, so this is quite flexible\n\n- How do we know when we've included the \"right\" covariates?\n\n- The higher $R^2$, the more the association. So, maximize $R^2$?\n\n. . .\n\n- However, adding more covariates _always_ increases $R^2$. Better to look at \"adjusted $R^2$\", which accounts for this\n\n- Also common: AIC and BIC (smaller is better)\n\n---\n\n## Special Case - Categorical Variables\n\nCan include categorical variables in multiple linear regression, but need to code them as \"dummy variables\" (i.e., indicator variables)\n\nSay a categorical variable has $k \\geq 2$ levels. Need to create $(k-1)$ indicator variables, equal to 1 for _one_ category and 0 otherwise\n\n__Important__: Categorical variable may be coded numerically (e.g., Agree = 1, Disagree = -1, Not Sure = 0)\n\n- If you put this variable straight into `lm()`, it will fit a very different model!\n\n\n---\n\n## Understanding the Categorical Variables Example\n\nExample: Penguins species: _Adelie_, _Chinstrap_, _Gentoo._ There are $k = 3$ levels.\n\nCreate an indicator for _Chinstrap_ and _Gentoo_: $I_C$ and $I_G$.  \n\n- If $I_C = I_G = 0$, then the penguin must be _Adelie_\n\nThe statistical model would be $Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_C I_C + \\beta_G I_G, \\sigma^2)$\n\n- $\\beta_0$: <!--Mean for _Adelie_-->\n\n- $\\beta_0 + \\beta_C$: <!--Mean for _Chinstrap_-->\n\n- $\\beta_0 + \\beta_G$: <!--Mean for _Gentoo_-->\n\n\n- Significant $\\beta_C$ $\\rightarrow$ <!--Chinstrap and Adelie are different-->\n\n- Significant $\\beta_G$ $\\rightarrow$ <!--Gentoo and Adelie are different-->\n\n- How to compare Chinstrap and Gentoo? <!--Need to fit a different model.-->\n\n\n---\n\n## Understanding Interactions (Categorical Example)\n\n::: {style=\"font-size: 80%;\"}\n\nSay we also have a quantitative variable $X$ (bill length). Consider two statistical models:\n\n1. $Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_X X + \\beta_C I_C + \\beta_G I_G, \\sigma^2)$\n\n2. $Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_X X + \\beta_C I_C + \\beta_G I_G + \\beta_{CX} I_C X + \\beta_{GX} I_G X, \\sigma^2)$\n\n:::\n\n. . .\n\n::: {style=\"font-size: 80%;\"}\n\nFor Model 1...\n\n- The intercept for Adelie is $\\beta_0$; for Chinstrap it is $\\beta_0 + \\beta_C$; for Gentoo it is $\\beta_0 + \\beta_G$\n\n- The slope for all species is $\\beta_X$.\n\n:::\n\n. . .\n\n::: {style=\"font-size: 80%;\"}\n\nFor Model 2...\n\n- The intercept for Adelie is $\\beta_0$; for Chinstrap it is $\\beta_0 + \\beta_C$; for Gentoo it is $\\beta_0 + \\beta_G$.\n\n- The slope for Adelie is $\\beta_X$; for Chinstrap it is $\\beta_X + \\beta_{CX}$; for Gentoo it is $\\beta_X + \\beta_{GX}$\n\n:::\n\n. . .\n\n::: {style=\"font-size: 80%;\"}\n\nSignificant coefficient for categorical variables by themselves? Significantly different intercepts\n\nSignificant coefficient for interactions with categorical variables? Significantly different slopes\n\n:::\n\n---\n\n## Model 2: `bill_depth_mm` ~ `bill_length_mm` + `species`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-regression-inference_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(lm(bill_depth_mm ~ bill_length_mm + species, data = penguins))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```{style='font-size: 0.6em;'}\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm + species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4529 -0.6864 -0.0508  0.5519  3.5915 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      10.59218    0.68302  15.508  < 2e-16 ***\nbill_length_mm    0.19989    0.01749  11.427  < 2e-16 ***\nspeciesChinstrap -1.93319    0.22416  -8.624 2.55e-16 ***\nspeciesGentoo    -5.10602    0.19142 -26.674  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9533 on 338 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.769,\tAdjusted R-squared:  0.7669 \nF-statistic: 375.1 on 3 and 338 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n---\n\n## Model 3: `bill_depth_mm` ~ `bill_length_mm` + `species` + `bill_length_mm` $\\times$ `species`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-regression-inference_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(lm(bill_depth_mm ~ bill_length_mm * species, data = penguins))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```{style='font-size: 0.5em;'}\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm * species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6574 -0.6675 -0.0524  0.5383  3.5032 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                     11.40912    1.13812  10.025  < 2e-16 ***\nbill_length_mm                   0.17883    0.02927   6.110 2.76e-09 ***\nspeciesChinstrap                -3.83998    2.05398  -1.870 0.062419 .  \nspeciesGentoo                   -6.15812    1.75451  -3.510 0.000509 ***\nbill_length_mm:speciesChinstrap  0.04338    0.04558   0.952 0.341895    \nbill_length_mm:speciesGentoo     0.02601    0.04054   0.642 0.521590    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9548 on 336 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7697,\tAdjusted R-squared:  0.7662 \nF-statistic: 224.5 on 5 and 336 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n---\n\n## A Few Linear Regression Warnings\n\n::: {style=\"font-size: 90%;\"}\n\n**Simpson's Paradox**\n\n+ There is a negative linear relationship between two variables but a positive linear relationship within every subpopulation\n\n+ In these cases, subgroup analysis is especially important\n\n:::\n\n. . .\n\n::: {style=\"font-size: 90%;\"}\n\n**Is the intercept meaningful?**\n\n+ Think about whether $X = 0$ makes scientific sense for a particular variable before you interpret the intercept\n\n:::\n\n. . .\n\n::: {style=\"font-size: 90%;\"}\n\n**Interpolation versus Extrapolation**\n\n+ Interpolation is defined as prediction within the range of a variable\n\n+ Extrapolation is defined as prediction outside the range of a variable\n\n+ Generally speaking, interpolation is more reliable than extrapolation (less sensitive to model misspecification)\n\n:::\n\n---\n\n## Extrapolation Example\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-regression-inference_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## Extrapolation Example\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-regression-inference_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n---\n\n## Extrapolation Example\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-regression-inference_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n\n---\n\n## Recap and next steps\n\n::: {style=\"font-size: 90%;\"}\n\nUse graphs to assess linear regression assumptions, i.e., residual diagnostic plots\n\nDiscussed interpretation of coefficients, the role of categorical variables, and interactions\n\nHighlighted common problems to consider: Simpson's Paradox, intercept meaning, and extrapolation\n\n:::\n\n. . .\n\n::: {style=\"font-size: 90%;\"}\n\n+ **HW4 is due TONIGHT and you have Lab 6 on Friday**\n\n+ **Graphics critique due Feb 28th!**\n\n+ **Next time**: Midsemester Review (take-home exam on Feb 26th)\n\n+ Recommended reading: [CW Chapter 12 Visualizing associations among two or more quantitative variables](https://clauswilke.com/dataviz/visualizing-associations.html)\n\n:::\n\n",
    "supporting": [
      "11-regression-inference_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
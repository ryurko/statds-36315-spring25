{
  "hash": "9d6bb404e5f2cc1ae84662db39e5e5a3",
  "result": {
    "markdown": "---\ntitle: \"Visualizing Text Data\"\nauthor: \"Prof Ron Yurko\"\nfooter:  \"[statds-36315-spring25](https://ryurko.github.io/statds-36315-spring25/)\"\ndate: 2025-04-14\nengine: knitr\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    pdf-separate-fragments: true\n    slide-number: c/t\n    smaller: true\n    code-line-numbers: true\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n## Announcements, previously, and today...\n\n::: {style=\"font-size: 95%;\"}\n\n**You should be working on your final projects!**\n\n**You must email me the slides for your presentation by 10 AM ET the day of your presentation - either a Google slides link or PDF file**\n\n**You do have lab this week!**\n\n:::\n\n. . .\n\n::: {style=\"font-size: 95%;\"}\n\n**Last time:** visualizing areal data\n\n\n**TODAY:** Text data, starting with:\n\n- Bag of Words representation of text.\n\n- Word clouds\n\nWill be able to answer the following questions:\n\n- Which words are most frequent in a set of documents?\n\n- How do two (or more) sets of documents compare in their word usage?\n\n- Which unique words occur most frequently in a set of documents?\n\n:::\n\n\n---\n\n## Working with raw text data \n\n- We'll work with script from the best episode of ['The Office': Season 4, Episode 13 - 'Dinner Party'](https://en.wikipedia.org/wiki/Dinner_Party_(The_Office))\n\n- We can access the script using the [`schrute` package (yes this is a real thing)](https://cran.r-project.org/web/packages/schrute/vignettes/theoffice.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(schrute)\n# Create a table from this package just corresponding to the Dinner Party episode:\ndinner_party_table <- theoffice |>\n  filter(season == 4, episode == 13) |>\n  # Just select columns of interest:\n  dplyr::select(index, character, text)\nhead(dinner_party_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  index character text                                                          \n  <int> <chr>     <chr>                                                         \n1 16791 Stanley   This is ridiculous.                                           \n2 16792 Phyllis   Do you have any idea what time we'll get out of here?         \n3 16793 Michael   Nobody likes to work late, least of all me. Do you have plans…\n4 16794 Jim       Nope I don't, remember when you told us not to make plans 'ca…\n5 16795 Michael   Yes I remember. Mmm, this is B.S. This is B.S. Why are we her…\n6 16796 Dwight    Thank you Michael.                                            \n```\n:::\n:::\n\n\n---\n\n## Reducing the dimensionality of text\n\nHow can we *quantify* text data such that it can be used in statistical models, analyses, and graphs?\n\n. . .\n\n**Bag of Words** representation of text is the most common representation\n\n+ In a document, grammar and order of words don't matter\n\n+ All that matters is the number of times each word occurs\n\n> \"Do I need to be liked? Absolutely not. I like to be liked. I enjoy being liked. I have to be liked, but it's not like this compulsive need to be liked, like my need to be praised.\" - Michael Scott\n\n. . .\n\n+ Do = 1, I = 4, need = 3, to = 5, be = 5, liked = 5\n\n+ Absolutely = 1, not = 2, like = 3, enjoy = 1\n\n+ being = 1, have = 1, but = 1, it's = 1, this = 1\n\n+ compulsive = 1, my = 1, praised = 2\n\n\n\n---\n\n## Bag of Words representation of text\n\n- Most common way to store text data is with a __document-term matrix__ (DTM):\n\n|            | Word 1   | Word 2   | $\\dots$  | Word $J$ |\n| ---------- | -------- | -------- | -------- | -------- |\n| Document 1 | $w_{11}$ | $w_{12}$ | $\\dots$  | $w_{1J}$ |\n| Document 2 | $w_{21}$ | $w_{22}$ | $\\dots$  | $w_{2J}$ |\n| $\\dots$    | $\\dots$  | $\\dots$  | $\\dots$  | $\\dots$  |\n| Document N | $w_{N1}$ | $w_{N2}$ | $\\dots$  | $w_{NJ}$ |\n\n- $w_{ij}$: count of word $j$ in document $i$, aka _term frequencies_\n\n. . .\n\nTwo additional ways to reduce number of columns:\n\n1. __Stop words__: remove extremely common words (e.g., of, the, a)\n\n2. __Stemming__: Reduce all words to their \"stem\"\n\n  - For example: Reducing = reduc. Reduce = reduc. Reduces = reduc.\n\n\n---\n\n## Tokenize text into long format\n\n- Convert raw text into long, tidy table with one-token-per-document-per-row\n\n  - A __token__ equals a unit of text - typically a word\n  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidytext)\ntidy_dinner_party_tokens <- dinner_party_table |>\n  unnest_tokens(word, text)\nhead(tidy_dinner_party_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  index character word      \n  <int> <chr>     <chr>     \n1 16791 Stanley   this      \n2 16791 Stanley   is        \n3 16791 Stanley   ridiculous\n4 16792 Phyllis   do        \n5 16792 Phyllis   you       \n6 16792 Phyllis   have      \n```\n:::\n:::\n\n\nEasy to convert text into DTM format using [`tidytext` package](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html)\n\n---\n\n## Remove stop words\n\n- Load `stop_words` from `tidytext`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(stop_words)\n\ntidy_dinner_party_tokens <- tidy_dinner_party_tokens |>\n  filter(!(word %in% stop_words$word))\n\nhead(tidy_dinner_party_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  index character word      \n  <int> <chr>     <chr>     \n1 16791 Stanley   ridiculous\n2 16792 Phyllis   idea      \n3 16792 Phyllis   time      \n4 16793 Michael   likes     \n5 16793 Michael   late      \n6 16793 Michael   plans     \n```\n:::\n:::\n\n\n---\n\n## Apply stemming\n\n- Can use [`SnowballC` package](https://cran.r-project.org/web/packages/SnowballC/SnowballC.pdf) to perform stemming\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(SnowballC)\n\ntidy_dinner_party_tokens <- tidy_dinner_party_tokens |>\n  mutate(stem = wordStem(word))\n\nhead(tidy_dinner_party_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  index character word       stem   \n  <int> <chr>     <chr>      <chr>  \n1 16791 Stanley   ridiculous ridicul\n2 16792 Phyllis   idea       idea   \n3 16792 Phyllis   time       time   \n4 16793 Michael   likes      like   \n5 16793 Michael   late       late   \n6 16793 Michael   plans      plan   \n```\n:::\n:::\n\n\n---\n\n## Create word cloud using term frequencies\n\n__Word Cloud__: Displays all words mentioned across documents, where more common words are larger\n\n- To do this, you must compute the _total_ word counts:\n\n$$w_{\\cdot 1} = \\sum_{i=1}^N w_{i1} \\hspace{0.1in} \\dots \\hspace{0.1in} w_{\\cdot J} = \\sum_{i=1}^N w_{iJ}$$\n\n- Then, the size of Word $j$ is proportional to $w_{\\cdot j}$\n\n. . .\n\nCreate word clouds in `R` using [`wordcloud` package](https://cran.r-project.org/web/packages/wordcloud/wordcloud.pdf)\n\nTakes in two main arguments to create word clouds:\n\n1. `words`: vector of unique words\n\n2. `freq`: vector of frequencies\n\n\n---\n\n## Create word cloud using term frequencies\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\ntoken_summary <- tidy_dinner_party_tokens |>\n  group_by(stem) |>\n  count() |>\n  ungroup() \n\nlibrary(wordcloud)\nwordcloud(words = token_summary$stem, \n          freq = token_summary$n, \n          random.order = FALSE, \n          max.words = 100, \n          colors = brewer.pal(8, \"Dark2\"))\n```\n\n::: {.cell-output-display}\n![](23-text-data_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n- Set `random.order = FALSE` to place biggest words in center\n\n- Can customize to display limited # words (`max.words`)\n\n- Other options as well like `colors`\n\n\n---\n\n## Comparison clouds\n\nImagine we have two different collections of documents, $\\mathcal{A}$ and $\\mathcal{B}$, that we wish to visually compare.\n\n. . .\n\nImagine we create the word clouds for the two collections of documents. Then this means we constructed vectors of total words for each collection:\n\n+ $\\mathbf{w}^{\\mathcal{A}} = (w_{\\cdot 1}^{\\mathcal{A}}, \\dots, w_{\\cdot J}^{\\mathcal{A}})$\n\n+ $\\mathbf{w}^{\\mathcal{B}} = (w_{\\cdot 1}^{\\mathcal{B}}, \\dots, w_{\\cdot J}^{\\mathcal{B}})$\n\nConsider the $j$th word, let's pretend it's \"dinner\":\n\n+ If $w_{\\cdot j}^{\\mathcal{A}}$ is large, then \"dinner\" is large in the word cloud for $\\mathcal{A}$.\n\n+ If $w_{\\cdot j}^{\\mathcal{B}}$ is large, then \"dinner\" is large in the word cloud for $\\mathcal{B}$.\n\n+ But if both are large, this doesn't tell us whether $w_{\\cdot j}^{\\mathcal{A}}$ or $w_{\\cdot j}^{\\mathcal{B}}$ is bigger.\n\n\n---\n\n## Comparison clouds\n\nThis motivates the construction of __comparison word clouds__: \n\n1. For word $j$, compute $\\bar{w}_{\\cdot j} = \\text{average}(w_{\\cdot j}^{\\mathcal{A}}, w_{\\cdot j}^{\\mathcal{B}})$\n\n2. Compute $w_{\\cdot j}^{\\mathcal{A}} - \\bar{w}_{\\cdot j}$ and $w_{\\cdot j}^{\\mathcal{B}} - \\bar{w}_{\\cdot j}$\n\n3. If $w_{\\cdot j}^{\\mathcal{A}} - \\bar{w}_{\\cdot j}$ is very positive, make it large for the $\\mathcal{A}$ word cloud. If $w_{\\cdot j}^{\\mathcal{B}} - \\bar{w}_{\\cdot j}$ is very positive, make it large for the $\\mathcal{B}$ word cloud.\n\n\n---\n\n## Comparison clouds\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](23-text-data_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## TF-IDF weighting\n\n- We saw that `michael` was the largest word, but what if I'm interested in comparing text across characters (i.e., documents)?\n\n. . .\n\n- It’s arguably of more interest to understand which words are frequently used in one set of texts but not the other, i.e., which words are unique?\n\n- Many text analytics methods will __down-weight__ words that occur frequently across all documents\n\n. . .\n\n- __Inverse document frequency (IDF)__: for word $j$ we compute $\\text{idf}_j = \\log \\frac{N}{N_j}$\n\n  - where $N$ is number of documents, $N_j$ is number of documents with word $j$\n\n- Compute __TF-IDF__ $= w_{ij} \\times \\text{idf}_j$\n\n---\n\n## TF-IDF example with characters\n\nCompute and join TF-IDF using `bind_tf_idf()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncharacter_token_summary <- tidy_dinner_party_tokens |>\n  group_by(character, stem) |> \n  count() |>\n  ungroup() \n\ncharacter_token_summary <- character_token_summary |>\n  bind_tf_idf(stem, character, n) \ncharacter_token_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 597 × 6\n   character stem        n     tf   idf tf_idf\n   <chr>     <chr>   <int>  <dbl> <dbl>  <dbl>\n 1 All       cheer       1 1      2.77  2.77  \n 2 Andy      anim        1 0.0476 2.77  0.132 \n 3 Andy      bet         1 0.0476 2.08  0.0990\n 4 Andy      capit       1 0.0476 2.77  0.132 \n 5 Andy      dinner      1 0.0476 0.981 0.0467\n 6 Andy      flower      2 0.0952 2.77  0.264 \n 7 Andy      hei         1 0.0476 1.39  0.0660\n 8 Andy      helena      1 0.0476 2.77  0.132 \n 9 Andy      hump        2 0.0952 2.77  0.264 \n10 Andy      michael     1 0.0476 0.981 0.0467\n# ℹ 587 more rows\n```\n:::\n:::\n\n\n\n---\n\n## Top 10 words by TF-IDF for each character\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\ncharacter_token_summary |>\n  filter(character %in% c(\"Michael\", \"Jan\", \"Jim\", \"Pam\")) |>\n  group_by(character) |>\n  slice_max(tf_idf, n = 10, with_ties = FALSE) |>\n  ungroup() |>\n  mutate(stem = reorder_within(stem, tf_idf, character)) |>\n  ggplot(aes(y = tf_idf, x = stem),\n         fill = \"darkblue\", alpha = 0.5) +\n  geom_col() +\n  coord_flip() +\n  scale_x_reordered() +\n  facet_wrap(~ character, ncol = 2, scales = \"free\") +\n  labs(y = \"TF-IDF\", x = NULL)\n```\n\n::: {.cell-output-display}\n![](23-text-data_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n---\n\n## Other functions of text\n\n- We've just focused on word counts - __but there are many functions of text__\n\n- For example: __number of unique words__ is often used to measure vocabulary\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://pbs.twimg.com/media/DxCgsrxWwAAOWO3.jpg){fig-align='center' width=70%}\n:::\n:::\n\n\n---\n\n## Recap and next steps\n\n- Most common representation: Bag of words and term frequencies (possibly weighted by TF-IDF)\n\n- Word clouds are the most common way to visualize the most frequent\nwords in a set of documents\n\n- TF-IDF weighting allows you to detect words that are uniquely used\nin certain documents\n\n. . .\n\n+ **You have lab on Friday!**\n\n+ **Next time**: Sentiment analysis and topics models\n\n+ Recommended Reading: [Text Mining With R](https://www.tidytextmining.com/), [Supervised Machine Learning for Text Analysis in R](https://smltar.com/)\n",
    "supporting": [
      "23-text-data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
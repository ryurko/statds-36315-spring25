{
  "hash": "307a6b69e6eed208818be8f0fd36a5ba",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Principal Component Analysis\"\nauthor: \"Prof Ron Yurko\"\nfooter:  \"[statds-36315-spring25](https://ryurko.github.io/statds-36315-spring25/)\"\ndate: 2025-03-24\nengine: knitr\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    pdf-separate-fragments: true\n    slide-number: c/t\n    smaller: true\n    code-line-numbers: true\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n\n## Announcements, previously, and today...\n\n::: {style=\"font-size: 75%;\"}\n\n**HW6 is due Wednesday March 26th by 11:59 PM ET**\n\n**You have Lab 8 this Friday**\n\n**We do NOT have in-class lecture on Wednesday! I will post a recording**\n\n:::\n\n. . .\n\n::: {style=\"font-size: 75%;\"}\n\nCommon workflow:\n\n+ Reduce the data to a few \"useful\" dimensions\n\n+ Plot those \"useful\" dimensions\n\n\nLast two classes:\n\n1. Reduce the data by summarizing pairs of subjects with one distance.\n\n2. Visualize distances using multi-dimensional scaling or dendrograms.\n\nHow can we reduce the data without distances?\n\n**Principal Component Analysis (PCA) is by far the most popular way**\n\n:::\n\n---\n\n## Dimension reduction - searching for variance\n\n__GOAL__: Focus on reducing dimensionality of feature space while __retaining__ most of the information in a lower dimensional space\n\n- $n \\times p$ matrix $\\rightarrow$ dimension reduction technique $\\rightarrow$ $n \\times k$ matrix\n\n. . .\n\nSpecial case we just discussed: __MDS__\n\n- $n \\times n$ __distance__ matrix $\\rightarrow$ MDS $\\rightarrow$ $n \\times k$ matrix (usually $k = 2$)\n\n1. Reduce data to a distance matrix\n\n2. Reduce distance matrix to $k = 2$ dimensions\n\n---\n\n## Principal Component Analysis (PCA)\n\n$$\n\\begin{pmatrix}\n& & \\text{really} & & \\\\\n& & \\text{wide} & & \\\\\n& & \\text{matrix} & &\n\\end{pmatrix}\n\\rightarrow \\text{matrix algebra stuff} \\rightarrow \n\\begin{pmatrix}\n\\text{much}  \\\\\n\\text{thinner}  \\\\\n\\text{matrix} \n\\end{pmatrix}\n$$\n\n- Start with $n \\times p$ matrix of __correlated__ variables $\\rightarrow$ $n \\times k$ matrix of __uncorrelated__ variables\n\n. . .\n\n- Each of the $k$ columns in the right-hand matrix are __principal components__, all uncorrelated with each other\n\n- First column accounts for most variation in the data, second column for second-most variation, and so on\n\n**Intuition: first few principal components account for most of the variation in the data**\n\n---\n\n## What are principal components?\n\n- Assume $\\boldsymbol{X}$ is a $n \\times p$ matrix that is __centered__ and __stardardized__\n\n- _Total variation_ $= p$, since Var( $\\boldsymbol{x}_j$ ) = 1 for all $j = 1, \\dots, p$\n\n- PCA will give us $p$ principal components that are $n$-length columns - call these $Z_1, \\dots, Z_p$\n\n. . .\n\n__First principal component__ (aka PC1):\n\n$$Z_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + \\dots + \\phi_{p1} X_p$$\n\n. . .\n\n  - $\\phi_{j1}$ are the weights indicating contributions of each variable $j \\in 1, \\dots, p$\n  \n  - Weights are normalized $\\sum_{j=1}^p \\phi_{j1}^2 = 1$\n  \n  - $\\phi_{1} = (\\phi_{11}, \\phi_{21}, \\dots, \\phi_{p1})$ is the __loading vector__ for PC1\n\n. . .\n  \n  - $Z_1$ is a linear combination of the $p$ variables that has the __largest variance__\n\n---\n\n## What are principal components?\n\n__Second principal component__:\n\n$$Z_2 = \\phi_{12} X_1 + \\phi_{22} X_2 + \\dots + \\phi_{p2} X_p$$\n\n  - $\\phi_{j2}$ are the weights indicating the contributions of each variable $j \\in 1, \\dots, p$\n  \n  - Weights are normalized $\\sum_{j=1}^p \\phi_{j1}^2 = 1$\n  \n  - $\\phi_{2} = (\\phi_{12}, \\phi_{22}, \\dots, \\phi_{p2})$ is the __loading vector__ for PC2\n  \n  - $Z_2$ is a linear combination of the $p$ variables that has the __largest variance__\n  \n    - __Subject to constraint it is uncorrelated with $Z_1$__ \n\n---\n\n## What are principal components?\n\nWe repeat this process to create $p$ principal components\n\n- __Uncorrelated__: Each ($Z_j, Z_{j'}$) is uncorrelated with each other\n\n- __Ordered Variance__: Var( $Z_1$ ) $>$ Var( $Z_2$ ) $> \\dots >$ Var( $Z_p$ )\n\n- __Total Variance__: $\\sum_{j=1}^p \\text{Var}(Z_j) = p$\n\n\n**Intuition: pick some $k << p$ such that if $\\sum_{j=1}^k \\text{Var}(Z_j) \\approx p$, then just using $Z_1, \\dots, Z_k$**\n\n---\n\n## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](17-pca_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](17-pca_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](17-pca_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](17-pca_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](17-pca_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## So what do we do with the principal components?\n\n__The point__: given a dataset with $p$ variables, we can find $k$ variables $(k << p)$ that account for most of the variation in the data\n\n. . .\n\nNote that the principal components are NOT easy to interpret - these are combinations of all variables\n\nPCA is similar to MDS with these main differences:\n\n1. MDS reduces a _distance_ matrix while PCA reduces a _data_ matrix\n\n2. PCA has a principled way to choose $k$\n\n3. Can visualize how the principal components are related to variables in data\n\n---\n\n## Working with PCA on Starbucks drinks\n\nUse the `prcomp()` function (based on SVD) for PCA on __centered__ and __scaled__ data\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstarbucks_pca <- prcomp(dplyr::select(starbucks, serv_size_m_l:caffeine_mg),\n                        center = TRUE, scale. = TRUE)\nsummary(starbucks_pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000\n```\n\n\n:::\n:::\n\n\n\n\n---\n\n## Computing Principal Components\n\nExtract the matrix of principal components $\\boldsymbol{Z} = XV$ (dimension of $\\boldsymbol{Z}$ will match original data)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstarbucks_pc_matrix <- starbucks_pca$x\nhead(starbucks_pc_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9        PC10         PC11\n[1,] -0.02812472 0.006489978 0.05145094 -0.06678083 -0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 -0.08080545 -0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 -0.09389227 -0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 -0.11582260 -0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 -0.03631676 -0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 -0.03497690 -0.002469611\n```\n\n\n:::\n:::\n\n\n\n\nColumns are uncorrelated, such that Var( $Z_1$ ) $>$ Var( $Z_2$ ) $> \\dots >$ Var( $Z_p$ ) - can start with a scatterplot of $Z_1, Z_2$\n\n---\n\n## Starbucks drinks: PC1 and PC2\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstarbucks <- starbucks |>\n  mutate(pc1 = starbucks_pc_matrix[,1], pc2 = starbucks_pc_matrix[,2])\nstarbucks |>\n  ggplot(aes(x = pc1, y = pc2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"PC 1\", y = \"PC 2\")\n```\n\n::: {.cell-output-display}\n![](17-pca_files/figure-revealjs/pca-plot-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n---\n\n## Making PCs interpretable with biplots ([`factoextra`](http://www.sthda.com/english/wiki/factoextra-r-package-easy-multivariate-data-analyses-and-elegant-visualization))\n\n\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\nlibrary(factoextra)\n# Designate to only label the variables:\nfviz_pca_biplot(starbucks_pca, \n                label = \"var\",\n                # Change the alpha for observations which is represented by ind\n                alpha.ind = .25,\n                # Modify the alpha for variables (var):\n                alpha.var = .75,\n                col.var = \"darkblue\")\n```\n\n::: {.cell-output-display}\n![](17-pca_files/figure-revealjs/pca-biplot-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n---\n\n## How many principal components to use?\n\n**Intuition: Additional principal components will add smaller and smaller variance**\n\n- Keep adding components until the added variance _drops off_\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(starbucks_pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000\n```\n\n\n:::\n:::\n\n\n\n\n---\n\n## Create scree plot (aka \"elbow plot\") to choose\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfviz_eig(starbucks_pca, addlabels = TRUE) + \n  geom_hline(yintercept = 100 * (1 / ncol(starbucks_pca$x)), linetype = \"dashed\", color = \"darkred\")\n```\n\n::: {.cell-output-display}\n![](17-pca_files/figure-revealjs/scree-plot-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n---\n\n\n## Recap and next steps\n\n+ Walked through PCA for dimension reduction\n\n+ PCA is a very common way to define \"most important dimensions\"\n\n+ Especially useful to plot principal components with a biplot (e.g., with `factoextra`)\n\n. . .\n\n+ **HW6 is due Wednesday March 26th and you have lab on Friday!**\n\n+ **I will not have office hours on Wednesday**\n\n+ **We do NOT have lecture this Wednesday - instead I will post a recording**\n\n. . .\n\n+ **Next time**: Visualizing trends\n\n+ Recommended reading: [CW Chapter 12 Visualizing associations among two or more quantitative variables](https://clauswilke.com/dataviz/visualizing-associations.html)\n\n---\n\n## PCA: [__singular value decomposition (SVD)__](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n\n$$\nX = U D V^T\n$$\n\n- Matrices $U$ and $V$ contain the left and right (respectively) __singular vectors of scaled matrix $X$__\n\n- $D$ is the diagonal matrix of the __singular values__\n\n- SVD simplifies matrix-vector multiplication as __rotate, scale, and rotate again__\n\n$V$ is called the __loading matrix__ for $X$ with $\\phi_{j}$ as columns, \n\n  - $Z = X  V$ is the PC matrix\n  \n---\n\n## Eigenvalue decomposition (aka spectral decomposition)\n\n$$\nX = U D V^T\n$$\n\n- $V$ are the __eigenvectors__ of $X^TX$ (covariance matrix, $^T$ means _transpose_)\n\n- $U$ are the __eigenvectors__ of $XX^T$\n\n- The singular values (diagonal of $D$) are square roots of the __eigenvalues__ of $X^TX$ or $XX^T$\n\n- Meaning that $Z = UD$\n\n---\n\n## Eigenvalues guide dimension reduction\n\nWe want to choose $p^* < p$ such that we are explaining variation in the data\n\nEigenvalues $\\lambda_j$ for $j \\in 1, \\dots, p$ indicate __the variance explained by each component__\n\n  - $\\sum_j^p \\lambda_j = p$, meaning $\\lambda_j \\geq 1$ indicates $\\text{PC}j$ contains at least one variable's worth in variability\n  \n  - $\\lambda_j / p$ equals proportion of variance explained by $\\text{PC}j$\n  \n  - Arranged in descending order so that $\\lambda_1$ is largest eigenvalue and corresponds to PC1\n  \n  - Can compute the cumulative proportion of variance explained (CVE) with $p^*$ components:\n  \n$$\\text{CVE}_{p^*} = \\frac{\\sum_j^{p*} \\lambda_j}{p}$$\n\n\n\n",
    "supporting": [
      "17-pca_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
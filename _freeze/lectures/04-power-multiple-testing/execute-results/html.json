{
  "hash": "a8b1e184c5373bf11a65988d3748a195",
  "result": {
    "markdown": "---\ntitle: \"Power and multiple testing\"\nauthor: \"Prof Ron Yurko\"\nfooter:  \"[statds-36315-spring25](https://ryurko.github.io/statds-36315-spring25/)\"\ndate: 2025-01-27\nengine: knitr\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    pdf-separate-fragments: true\n    slide-number: c/t\n    smaller: true\n    code-line-numbers: true\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n## Announcements, previously, and today...\n\n::: {style=\"font-size: 80%;\"}\n\n+ **HW1 is due this Wednesday Jan 29 by 11:59 PM** \n\n+ **You have Lab 3 again on Friday**\n\nOffice hours schedule:\n\n+ My office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\n\n+ Anna (zoom): Mondays @ 2 PM; and Perry (zoom): Wednesdays @ 11 AM\n\n:::\n\n. . .\n\n::: {style=\"font-size: 80%;\"}\n\n\n+ Main estimators: $\\underbrace{\\hat{p}_1,\\dots,\\hat{p}_K}_{\\text{proportions}}$ for $K$-many categories\n\n+ Chi-square test is the main statistical test for 1D categorical data, tests $H_0: p_1 = \\cdots = p_K$\n\n+ Can also make confidence intervals for $\\hat{p}_1,\\dots,\\hat{p}_K$ (just multiply CIs by $n$)\n\n:::\n\n. . .\n\n::: {style=\"font-size: 80%;\"}\n\n**TODAY:**\n\n+ Interpreting CIs on graphs is tricky and have to be careful with multiple testing\n\n:::\n\n---\n\n## Graphics versus Statistical Inference\n\n- Reminder Anscombe's Quartet: where statistical inference was the same but the graphics were very different\n\n![](https://upload.wikimedia.org/wikipedia/commons/e/ec/Anscombe%27s_quartet_3.svg){fig-align=\"center\"width=55%}\n\n- __The opposite can be true!__ Graphics are the same, but statistical inference is very different...\n\n\n---\n\n## Example: 3 categories, $p_A = 1/2,\\ p_B = p_C = 1/4$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-power-multiple-testing_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## Example: 3 categories, $p_A = 1/2,\\ p_B = p_C = 1/4$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-power-multiple-testing_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## Example: 3 categories, $p_A = 1/2,\\ p_B = p_C = 1/4$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-power-multiple-testing_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## Example: 3 categories, $p_A = 1/2,\\ p_B = p_C = 1/4$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-power-multiple-testing_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## Power under this scenario: (2n/4, n/4, n/4)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-power-multiple-testing_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n---\n\n## How do we combine graphs with inference?\n\n\n1. Simply add $p$-values (or other info) to graph via text\n\n2. Add confidence intervals to the graph\n\n  - Need to remember what each CI is for! \n  \n  - Our CIs on previous slides are for each $\\hat{p}_j$ marginally, __NOT__ jointly\n\n  - Have to be careful with __multiple testing__...\n\n---\n\n## CIs will visually capture uncertainty in estimates\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-power-multiple-testing_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-power-multiple-testing_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n\n::::\n\n---\n\n## Rough rules for comparing CIs on bar charts\n\n::: {style=\"font-size: 75%;\"}\n\n- Comparing overlap of two CIs is __NOT__ exactly the same as directly testing for a significant difference...\n\n  - Really you want CI( $\\hat{p}_1 - \\hat{p}_2$ ), not CI( $\\hat{p_1}$ ) and CI( $\\hat{p_2}$ )\n  \n  - CI( $\\hat{p_1}$ ) and CI( $\\hat{p_2}$ ) not overlapping implies $0 \\notin$ CI( $\\hat{p}_1 - \\hat{p}_2$ )\n\n  - _However_ CI( $\\hat{p_1}$ ) and CI( $\\hat{p_2}$ ) overlapping __DOES NOT__ imply $0 \\in$ CI( $\\hat{p}_1 - \\hat{p}_2$ ) \n\n\nRoughly speaking:\n\n  - If CIs don't overlap $\\rightarrow$ significant difference\n  \n  - If CIs overlap a little $\\rightarrow$ ambiguous\n  \n  - If CIs overlap a lot $\\rightarrow$ no significant difference\n  \n:::\n  \n. . .\n\n::: {style=\"font-size: 75%;\"}\n\nBut if we're comparing more than two CIs simultaneously, we need to account for __multiple testing__!\n\n  - When you look for all non-overlapping CIs: making $\\binom{K}{2} = \\frac{K!}{2!(K-2)!}$ pairwise tests in your head!\n  \n:::\n\n---\n\n## Corrections for multiple testing\n\n::: {style=\"font-size: 75%;\"}\n\n- In those bar plots, when we determine whether CIs overlap we make 3 comparisons:\n\n  1. A vs B\n  \n  2. A vs C\n  \n  3. B vs C\n  \n**This is a multiple testing issue**\n\n:::\n\n. . .\n\n::: {style=\"font-size: 75%;\"}\n\n- In short: we will make Type 1 errors (chance of false rejecting) more than 5% of the time!\n\n- Reminder: Type 1 error = Rejecting $H_0$ when $H_0$ is true\n\n- e.g., CIs don't overlap but actually $H_0: p_A = p_B$ is true\n  \n- If only interested in A vs B __and nothing else__, then just construct 95% CI for A vs B and _control error rate_ at 5%\n  \n- However, if we construct several CIs, where A vs B is just one comparison we make, our Type 1 error rate > 5%!\n\n:::\n\n---\n\n## Corrections for multiple testing\n\n::: {style=\"font-size: 75%;\"}\n\nVast literature on corrections for multiple testing (beyond the scope of this class... but in my thesis!)\n\nBut you should understand the following:\n\n1. Corrections for multiple testing inflate $p$-values (i.e., make them bigger)\n\n2. Equivalently, they inflate CIs (i.e., make them wider)\n\n3. Purpose of these corrections is to control Type 1 error rate $\\leq 5\\%$\n\n:::\n\n. . .\n\n::: {style=\"font-size: 75%;\"}\n\nWe'll focus on the __Bonferroni correction__, which inflates $p$-values the most but is easy to implement and very popular:\n\n+ We usually reject null hypothesis when $p$-value $\\leq .05$\n\n+ __Bonferroni__: if making $K$ comparisons, reject only if $p$-value $\\leq .05/K$\n\n+ For CIs: instead of plotting 95% CIs, we plot (1 - $0.05/K$)% CIs\n\n  + e.g., for $K = 3$ then plot 98.3% CIs\n  \n:::\n  \n---\n\n## Impact of Bonferroni correction on CIs...\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-power-multiple-testing_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-power-multiple-testing_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n\n::::\n\n---\n\n## Recap and next steps\n\n::: {style=\"font-size: 80%;\"}\n\n+ Graphs with the same trends can display very different statistical significance (largely due to sample size)\n\n+ Can visualize CIs for each $\\hat{p}_1$, $\\dots$, $\\hat{p}_K$, but need to deal with multiple testing\n\n:::\n\n. . .\n\n::: {style=\"font-size: 80%;\"}\n\n+ **HW1 is due Wednesday and you have Lab 3 on Friday!**\n\n+ **Next time**: Visualizations and inference for 2D categorical data\n\n+ Recommended reading: [CW Chapter 11 Visualizing nested proportions](https://clauswilke.com/dataviz/nested-proportions.html)\n\n:::\n\n",
    "supporting": [
      "04-power-multiple-testing_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
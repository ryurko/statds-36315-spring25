[
  {
    "objectID": "lectures/24-more-text.html#announcements-previously-and-today",
    "href": "lectures/24-more-text.html#announcements-previously-and-today",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nYou should be working on your final projects!\nYou must email me the slides for your presentation by 10 AM ET the day of your presentation - either a Google slides link or PDF file\nYou do have lab this week!\n\n\n\nLast time: introduction to text data and word clouds\nTODAY: More text data!"
  },
  {
    "objectID": "lectures/24-more-text.html#reminder-the-office-text-analysis",
    "href": "lectures/24-more-text.html#reminder-the-office-text-analysis",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Reminder: The Office text analysis",
    "text": "Reminder: The Office text analysis\n\nWe starting working with the script from the best episode of ‘The Office’: Season 4, Episode 13 - ‘Dinner Party’\nWe can access the script using the schrute package (yes this is a real thing):\n\n\nlibrary(schrute)\n# Create a table from this package just corresponding to the Dinner Party episode:\ndinner_party_table &lt;- theoffice |&gt;\n  filter(season == 4, episode == 13) |&gt;\n  # Just select columns of interest:\n  dplyr::select(index, character, text)\nhead(dinner_party_table)\n\n# A tibble: 6 × 3\n  index character text                                                          \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;                                                         \n1 16791 Stanley   This is ridiculous.                                           \n2 16792 Phyllis   Do you have any idea what time we'll get out of here?         \n3 16793 Michael   Nobody likes to work late, least of all me. Do you have plans…\n4 16794 Jim       Nope I don't, remember when you told us not to make plans 'ca…\n5 16795 Michael   Yes I remember. Mmm, this is B.S. This is B.S. Why are we her…\n6 16796 Dwight    Thank you Michael."
  },
  {
    "objectID": "lectures/24-more-text.html#tokenize-text-into-long-format",
    "href": "lectures/24-more-text.html#tokenize-text-into-long-format",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Tokenize text into long format",
    "text": "Tokenize text into long format\n\nConvert raw text into long, tidy table with one-token-per-document-per-row\n\nA token equals a unit of text - typically a word\n\n\n\nlibrary(tidytext)\ntidy_dinner_party_tokens &lt;- dinner_party_table |&gt;\n  unnest_tokens(word, text)\nhead(tidy_dinner_party_tokens)\n\n# A tibble: 6 × 3\n  index character word      \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     \n1 16791 Stanley   this      \n2 16791 Stanley   is        \n3 16791 Stanley   ridiculous\n4 16792 Phyllis   do        \n5 16792 Phyllis   you       \n6 16792 Phyllis   have      \n\n\nEasy to convert text into DTM format using tidytext package"
  },
  {
    "objectID": "lectures/24-more-text.html#remove-stop-words",
    "href": "lectures/24-more-text.html#remove-stop-words",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Remove stop words",
    "text": "Remove stop words\n\nLoad stop_words from tidytext\n\n\ndata(stop_words)\n\ntidy_dinner_party_tokens &lt;- tidy_dinner_party_tokens |&gt;\n  filter(!(word %in% stop_words$word))\n\nhead(tidy_dinner_party_tokens)\n\n# A tibble: 6 × 3\n  index character word      \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     \n1 16791 Stanley   ridiculous\n2 16792 Phyllis   idea      \n3 16792 Phyllis   time      \n4 16793 Michael   likes     \n5 16793 Michael   late      \n6 16793 Michael   plans"
  },
  {
    "objectID": "lectures/24-more-text.html#apply-stemming",
    "href": "lectures/24-more-text.html#apply-stemming",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Apply stemming",
    "text": "Apply stemming\n\nCan use SnowballC package to perform stemming\n\n\nlibrary(SnowballC)\n\ntidy_dinner_party_tokens &lt;- tidy_dinner_party_tokens |&gt;\n  mutate(stem = wordStem(word))\n\nhead(tidy_dinner_party_tokens)\n\n# A tibble: 6 × 4\n  index character word       stem   \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;  \n1 16791 Stanley   ridiculous ridicul\n2 16792 Phyllis   idea       idea   \n3 16792 Phyllis   time       time   \n4 16793 Michael   likes      like   \n5 16793 Michael   late       late   \n6 16793 Michael   plans      plan"
  },
  {
    "objectID": "lectures/24-more-text.html#tf-idf-weighting",
    "href": "lectures/24-more-text.html#tf-idf-weighting",
    "title": "Sentiment Analysis and Topic Models",
    "section": "TF-IDF weighting",
    "text": "TF-IDF weighting\n\nWe saw that michael was the largest word, but what if I’m interested in comparing text across characters (i.e., documents)?\n\n\n\nIt’s arguably of more interest to understand which words are frequently used in one set of texts but not the other, i.e., which words are unique?\nMany text analytics methods will down-weight words that occur frequently across all documents\n\n\n\n\nInverse document frequency (IDF): for word \\(j\\) we compute \\(\\text{idf}_j = \\log \\frac{N}{N_j}\\)\n\nwhere \\(N\\) is number of documents, \\(N_j\\) is number of documents with word \\(j\\)\n\nCompute TF-IDF \\(= w_{ij} \\times \\text{idf}_j\\)"
  },
  {
    "objectID": "lectures/24-more-text.html#tf-idf-example-with-characters",
    "href": "lectures/24-more-text.html#tf-idf-example-with-characters",
    "title": "Sentiment Analysis and Topic Models",
    "section": "TF-IDF example with characters",
    "text": "TF-IDF example with characters\nCompute and join TF-IDF using bind_tf_idf():\n\ncharacter_token_summary &lt;- tidy_dinner_party_tokens |&gt;\n  group_by(character, stem) |&gt; \n  count() |&gt;\n  ungroup() \n\ncharacter_token_summary &lt;- character_token_summary |&gt;\n  bind_tf_idf(stem, character, n) \ncharacter_token_summary\n\n# A tibble: 597 × 6\n   character stem        n     tf   idf tf_idf\n   &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 All       cheer       1 1      2.77  2.77  \n 2 Andy      anim        1 0.0476 2.77  0.132 \n 3 Andy      bet         1 0.0476 2.08  0.0990\n 4 Andy      capit       1 0.0476 2.77  0.132 \n 5 Andy      dinner      1 0.0476 0.981 0.0467\n 6 Andy      flower      2 0.0952 2.77  0.264 \n 7 Andy      hei         1 0.0476 1.39  0.0660\n 8 Andy      helena      1 0.0476 2.77  0.132 \n 9 Andy      hump        2 0.0952 2.77  0.264 \n10 Andy      michael     1 0.0476 0.981 0.0467\n# ℹ 587 more rows"
  },
  {
    "objectID": "lectures/24-more-text.html#top-10-words-by-tf-idf-for-each-character",
    "href": "lectures/24-more-text.html#top-10-words-by-tf-idf-for-each-character",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Top 10 words by TF-IDF for each character",
    "text": "Top 10 words by TF-IDF for each character\n\ncharacter_token_summary |&gt;\n  filter(character %in% c(\"Michael\", \"Jan\", \"Jim\", \"Pam\")) |&gt;\n  group_by(character) |&gt;\n  slice_max(tf_idf, n = 10, with_ties = FALSE) |&gt;\n  ungroup() |&gt;\n  mutate(stem = reorder_within(stem, tf_idf, character)) |&gt;\n  ggplot(aes(y = tf_idf, x = stem),\n         fill = \"darkblue\", alpha = 0.5) +\n  geom_col() +\n  coord_flip() +\n  scale_x_reordered() +\n  facet_wrap(~ character, ncol = 2, scales = \"free\") +\n  labs(y = \"TF-IDF\", x = NULL)"
  },
  {
    "objectID": "lectures/24-more-text.html#top-10-words-by-tf-idf-for-each-character-output",
    "href": "lectures/24-more-text.html#top-10-words-by-tf-idf-for-each-character-output",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Top 10 words by TF-IDF for each character",
    "text": "Top 10 words by TF-IDF for each character"
  },
  {
    "objectID": "lectures/24-more-text.html#other-functions-of-text",
    "href": "lectures/24-more-text.html#other-functions-of-text",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Other functions of text",
    "text": "Other functions of text\n\nWe’ve just focused on word counts - but there are many functions of text\nFor example: number of unique words is often used to measure vocabulary"
  },
  {
    "objectID": "lectures/24-more-text.html#sentiment-analysis",
    "href": "lectures/24-more-text.html#sentiment-analysis",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\nThe visualizations so far only look at word frequency (possibly weighted with TF-IDF), but doesn’t tell you how words are used\n\n\n\nA common goal in text analysis is to try to understand the overall sentiment or “feeling” of text, i.e., sentiment analysis\nTypical approach:\n\nFind a sentiment dictionary (e.g., “positive” and “negative” words)\nCount the number of words belonging to each sentiment\nUsing the counts, you can compute an “average sentiment” (e.g., positive counts - negative counts)\n\n\n\n\n\nThis is called a dictionary-based approach\nThe Bing dictionary (named after Bing Liu) provides 6,786 words that are either “positive” or “negative”"
  },
  {
    "objectID": "lectures/24-more-text.html#character-sentiment-analysis",
    "href": "lectures/24-more-text.html#character-sentiment-analysis",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Character sentiment analysis",
    "text": "Character sentiment analysis\n\nget_sentiments(\"bing\")\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows"
  },
  {
    "objectID": "lectures/24-more-text.html#character-sentiment-analysis-1",
    "href": "lectures/24-more-text.html#character-sentiment-analysis-1",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Character sentiment analysis",
    "text": "Character sentiment analysis\nJoin sentiment to token table (without stemming)\n\ntidy_all_tokens &lt;- dinner_party_table |&gt;\n  unnest_tokens(word, text)\n\ntidy_sentiment_tokens &lt;- tidy_all_tokens |&gt;\n  inner_join(get_sentiments(\"bing\")) \n\nhead(tidy_sentiment_tokens)\n\n# A tibble: 6 × 4\n  index character word       sentiment\n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    \n1 16791 Stanley   ridiculous negative \n2 16793 Michael   likes      positive \n3 16793 Michael   work       positive \n4 16795 Michael   enough     positive \n5 16795 Michael   enough     positive \n6 16795 Michael   mad        negative"
  },
  {
    "objectID": "lectures/24-more-text.html#character-sentiment-analysis-2",
    "href": "lectures/24-more-text.html#character-sentiment-analysis-2",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Character sentiment analysis",
    "text": "Character sentiment analysis\n\ntidy_sentiment_tokens |&gt;\n  group_by(character, sentiment) |&gt;\n  summarize(n_words = n()) |&gt;\n  ungroup() |&gt;\n  group_by(character) |&gt;\n  mutate(total_assigned_words = sum(n_words)) |&gt;\n  ungroup() |&gt;\n  mutate(character = fct_reorder(character, total_assigned_words)) |&gt;\n  ggplot(aes(x = character, y = n_words, fill = sentiment)) + \n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  scale_fill_manual(values = c(\"red\", \"blue\")) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/24-more-text.html#character-sentiment-analysis-2-output",
    "href": "lectures/24-more-text.html#character-sentiment-analysis-2-output",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Character sentiment analysis",
    "text": "Character sentiment analysis"
  },
  {
    "objectID": "lectures/24-more-text.html#topic-modeling",
    "href": "lectures/24-more-text.html#topic-modeling",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Topic Modeling",
    "text": "Topic Modeling\nEverything we’ve done still involves word counts in some way.\nStill have to deal with the high-dimensionality of individual words.\n\nTopic modeling: Envisions that there are just a few latent (“hidden”) categories for each document\n\nThese latent categories are called topics\nEach topic encompasses a bunch of words that tend to occur together\n\n\n\nThe workflow for topic modeling is:\n\nDocument-Term Matrix \\(\\rightarrow\\) Latent Dirichlet Allocation \\(\\rightarrow\\) \\(k\\) topics\n\nAllows you to find \\(k\\) overall topics that are being discussed across your documents"
  },
  {
    "objectID": "lectures/24-more-text.html#hierarchy-of-topic-modeling",
    "href": "lectures/24-more-text.html#hierarchy-of-topic-modeling",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Hierarchy of Topic Modeling",
    "text": "Hierarchy of Topic Modeling\n\nSay we have documents \\(D_1, \\dots, D_N\\) and \\(1000\\) words…\nHypothesis behind topic modeling: Maybe these \\(N\\) documents are really just about \\(k = 2\\) topics (we could make \\(k\\) bigger if we want)\n\n\n\nTopic: A collection of words with different probabilities of occurring.\n\nTopic A: \\(\\beta_1^{A} =\\) probability of Word 1, \\(\\beta_2^{A} =\\) probability of Word 2, etc.\nTopic B: \\(\\beta_1^{B} =\\) probability of Word 1, \\(\\beta_2^{B} =\\) probability of Word 2, etc.\n\n\n\n\n\nWords may be prominent in both topics (e.g., \\(\\beta_1^{A} = \\beta_1^{B} = 0.8\\)) or rare in both topics. \\(\\sum_{j=1}^{1000} \\beta_j^{A} = 1\\), but no constraint on \\(\\beta_j^{A} + \\beta_j^{B}\\) for any \\(j\\).\n\n\n\n\nDocument: A collection of topics with different proportions.\n\nDocument 1:\n\n\\(\\gamma_A =\\) proportion of Topic A,\n\\(\\gamma_B =\\) proportion of Topic B.\n\n\nFor each document, \\(\\gamma_A + \\gamma_B = 1\\)"
  },
  {
    "objectID": "lectures/24-more-text.html#generative-model-for-text",
    "href": "lectures/24-more-text.html#generative-model-for-text",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Generative model for text",
    "text": "Generative model for text\nFor each document (assume number of words is known):\n\nFor each word in the document:\n\nDraw a topic assignment, e.g., pick Topic A with probability \\(\\gamma_A = 60\\%\\) versus Topic B with \\(\\gamma_B = 40\\%\\)\nGiven a topic, choose the word with probabilities defined by \\(\\beta^{topic}\\), e.g., if Topic A was picked then select a word using \\(\\beta^A\\)\n\n\n\nThe result?\nIf \\(\\gamma_A = 60\\%, \\gamma_B = 40\\%\\) for Document 1, then 60% of the time we’ll use the \\(\\beta_1^{A}, \\beta_2^{A},\\dots,\\beta_{1000}^A\\) probabilities for generating words, and 40% of the time we’ll use the \\(\\beta_1^{B}, \\beta_2^{B},\\dots,\\beta_{1000}^B\\)"
  },
  {
    "objectID": "lectures/24-more-text.html#what-does-topic-modeling-give-us",
    "href": "lectures/24-more-text.html#what-does-topic-modeling-give-us",
    "title": "Sentiment Analysis and Topic Models",
    "section": "What does topic modeling give us?",
    "text": "What does topic modeling give us?\nFor simplicity, let’s say that we have two topics, A and B.\nAfter we run topic modeling, we will get the following information:\n\nFor each document, we will get topic proportions \\(\\gamma_A\\) and \\(\\gamma_B\\)\nFor each topic, we will get word probabilities \\(\\beta_1, \\dots, \\beta_J\\)\n\n\nAfter running topic modeling, first you should understand what topics have been identified.\nIntuition: For each topic, which words are most likely?\nCan compute and plot the top \\(\\beta_1, \\dots, \\beta_J\\) for each topic.:\n\nThis can be done with a bar plot, where the \\(\\beta_1,\\dots,\\beta_J\\) are the height of the bars.\nCan also make a word cloud, where the \\(\\beta_1,\\dots,\\beta_J\\) are the word sizes."
  },
  {
    "objectID": "lectures/24-more-text.html#stranger-things-topic-model",
    "href": "lectures/24-more-text.html#stranger-things-topic-model",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Stranger Things topic model",
    "text": "Stranger Things topic model\nDemo using dialogue from Stranger Things\n\nlibrary(tidyverse)\nstranger_things_text &lt;-\n  read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-10-18/stranger_things_all_dialogue.csv') |&gt;\n  # Drop any lines with missing dialogue\n  filter(!is.na(dialogue))\nhead(stranger_things_text)\n\n# A tibble: 6 × 8\n  season episode  line raw_text     stage_direction dialogue start_time end_time\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;           &lt;chr&gt;    &lt;time&gt;     &lt;time&gt;  \n1      1       1     9 [Mike] Some… [Mike]          Somethi… 01'44\"     01'48\"  \n2      1       1    10 A shadow gr… &lt;NA&gt;            A shado… 01'48\"     01'52\"  \n3      1       1    11 -It is almo… &lt;NA&gt;            It is a… 01'52\"     01'54\"  \n4      1       1    12 What if it'… &lt;NA&gt;            What if… 01'54\"     01'56\"  \n5      1       1    13 Oh, Jesus, … &lt;NA&gt;            Oh, Jes… 01'56\"     01'59\"  \n6      1       1    14 It's not th… &lt;NA&gt;            It's no… 01'59\"     02'00\"  \n\n\nSee demo for all pre-processing steps"
  },
  {
    "objectID": "lectures/24-more-text.html#convert-to-input-for-topicmodels-package",
    "href": "lectures/24-more-text.html#convert-to-input-for-topicmodels-package",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Convert to input for topicmodels package",
    "text": "Convert to input for topicmodels package\nNeed to covert tidytext output to DocumentTermMatrix object:\n\nepisode_dtm &lt;- st_episode_word_summary |&gt;\n  # Using the stems\n  cast_dtm(episode_id, word, n) #&lt;&lt;\n\nepisode_dtm\n\n&lt;&lt;DocumentTermMatrix (documents: 34, terms: 7226)&gt;&gt;\nNon-/sparse entries: 22122/223562\nSparsity           : 91%\nMaximal term length: 27\nWeighting          : term frequency (tf)"
  },
  {
    "objectID": "lectures/24-more-text.html#fit-lda-model-with-topicmodels",
    "href": "lectures/24-more-text.html#fit-lda-model-with-topicmodels",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Fit LDA model with topicmodels",
    "text": "Fit LDA model with topicmodels\nFit LDA model with specified k topics:\n\nlibrary(topicmodels)\n\n# set a seed so that the output of the model is predictable\nst_lda &lt;- LDA(episode_dtm, k = 2, control = list(seed = 1234))\nst_lda\n\nA LDA_VEM topic model with 2 topics.\n\n\n\nThere are two quantities we’ll grab from LDA:\n\ngamma: Topic proportions for each document\nbeta: Word probabilities for each topic"
  },
  {
    "objectID": "lectures/24-more-text.html#working-with-betas",
    "href": "lectures/24-more-text.html#working-with-betas",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Working with \\(\\beta\\)s",
    "text": "Working with \\(\\beta\\)s\nFor any topic \\(t\\), we’ll have probabilities \\(\\beta_1^{(t)},\\dots,\\beta_J^{(t)}\\)\n\nst_topics &lt;- tidy(st_lda, matrix = \"beta\")\nst_topics\n\n# A tibble: 14,452 × 3\n   topic term      beta\n   &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 00    8.62e- 4\n 2     2 00    6.96e- 4\n 3     1 10    2.40e- 4\n 4     2 10    4.64e- 5\n 5     1 100   3.32e- 4\n 6     2 100   9.72e- 5\n 7     1 12    2.38e- 4\n 8     2 12    4.89e- 5\n 9     1 12.3  4.81e- 5\n10     2 12.3  1.11e-90\n# ℹ 14,442 more rows"
  },
  {
    "objectID": "lectures/24-more-text.html#working-with-betas-1",
    "href": "lectures/24-more-text.html#working-with-betas-1",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Working with \\(\\beta\\)s",
    "text": "Working with \\(\\beta\\)s\nUsing group_by() and top_n(), can find the top \\(\\beta\\)s for each topic"
  },
  {
    "objectID": "lectures/24-more-text.html#finding-important-words",
    "href": "lectures/24-more-text.html#finding-important-words",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Finding Important Words",
    "text": "Finding Important Words\n\nWhich words are likely in Topic 2 but not Topic 1?\nThis is what TF-IDF weights are for, but unfortunately you can’t use TF-IDF with topic modeling in this form…\n\n\n\nIntuition: What \\(\\beta\\)s are big in Topic 2 but not Topic 1?\n\nLet \\(\\beta_1^{(1)}, \\dots, \\beta_J^{(1)}\\) be the Topic 1 word probabilities\nLet \\(\\beta_1^{(2)}, \\dots, \\beta_J^{(2)}\\) be the Topic 2 word probabilities\n\n\n\n\n\nConsider the following quantity:\n\\[\n\\begin{align*}\n      \\log \\frac{\\beta_j^{(2)}}{\\beta_j^{(1)}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "lectures/24-more-text.html#finding-important-words-1",
    "href": "lectures/24-more-text.html#finding-important-words-1",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Finding Important Words",
    "text": "Finding Important Words\nHere’s a visual of \\(\\log \\frac{\\beta_j^{(2)}}{\\beta_j^{(1)}}\\):"
  },
  {
    "objectID": "lectures/24-more-text.html#what-to-do-after-youve-identified-topics",
    "href": "lectures/24-more-text.html#what-to-do-after-youve-identified-topics",
    "title": "Sentiment Analysis and Topic Models",
    "section": "What to do after you’ve identified topics…",
    "text": "What to do after you’ve identified topics…\nRecall: Each document will have topic proportions \\(\\gamma_1,\\dots,\\gamma_k\\) where for each document \\(i\\), \\(\\sum_{j=1}^k \\gamma_j^{(i)} = 1\\)\n\nst_documents &lt;- tidy(st_lda, matrix = \"gamma\") #&lt;&lt;\nst_documents |&gt;\n  filter(document %in% c(\"1_1\", \"4_1\"))\n\n# A tibble: 4 × 3\n  document topic     gamma\n  &lt;chr&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 1_1          1 1.00     \n2 4_1          1 0.0000218\n3 1_1          2 0.0000337\n4 4_1          2 1.00     \n\n\n\nIt can be helpful to see how these \\(\\gamma\\) vary across different kinds of documents. For example:\n\nPlot each \\(\\gamma\\) over time (if your documents have a timestamp)\nExamine the \\(\\gamma\\) for different authors of documents.\nProject the documents into a 2D space based on topic probabilities (e.g., via Jensen-Shannon divergence)"
  },
  {
    "objectID": "lectures/24-more-text.html#recap-and-next-steps",
    "href": "lectures/24-more-text.html#recap-and-next-steps",
    "title": "Sentiment Analysis and Topic Models",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nCan measure the “sentiment” of text with sentiment-based dictionaries\nTopic modeling gives you two things:\n\nTopic Proportions: For each document, what is the proportion of each topic?\nWord Probabilities: For each topic, what is the probability of a certain word occurring?\n\n\n\n\nYour final lab is on Friday!\nNext time: FINAL PROJECT PRESENTATIONS!\nRecommended Reading: Text Mining With R, Supervised Machine Learning for Text Analysis in R"
  },
  {
    "objectID": "lectures/16-clustering.html#announcements-previously-and-today",
    "href": "lectures/16-clustering.html#announcements-previously-and-today",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW5 is due TONIGHT by 11:59 PM ET\nYou have Lab 7 this Friday\nHW6 is due next Wednesday March 26th by 11:59 PM ET\n\n\n\nLast time: General approach for visualizing distance matrices\n\nPuts \\(n\\) observations in a \\(k\\)-dimensional space such that the distances are preserved as much as possible\n\nwhere \\(k &lt;&lt; p\\) typically choose \\(k = 2\\)\n\n\nMDS attempts to create new point \\(\\boldsymbol{y}_i = (y_{i1}, y_{i2})\\) for each unit such that:\n\\[\\sqrt{(y_{i1} - y_{j1})^2 + (y_{i2} - y_{j2})^2} \\approx D_{ij}\\]\n\ni.e., distance in 2D MDS world is approximately equal to the actual distance\n\nThen plot the new \\(\\boldsymbol{y}\\)s on a scatterplot\n\nUse the scale() function to ensure variables are comparable\nMake a distance matrix for this dataset\nVisualize it with MDS"
  },
  {
    "objectID": "lectures/16-clustering.html#demo-mcu-movie-data",
    "href": "lectures/16-clustering.html#demo-mcu-movie-data",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Demo: MCU movie data",
    "text": "Demo: MCU movie data\n\nmcu_movies &lt;- read_csv(\"https://raw.githubusercontent.com/ryurko/DataViz-Class-Data/main/mcu_movies.csv\")\nmcu_movies\n\n# A tibble: 30 × 18\n   film  category worldwide_gross_m percent_budget_recov…¹ critics_percent_score\n   &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;                  &lt;dbl&gt;                 &lt;dbl&gt;\n 1 Ant-… Ant-Man                518                    398                    83\n 2 Ant-… Ant-Man                623                    479                    87\n 3 Aven… Avengers              1395                    382                    76\n 4 Aven… Avengers              2797                    699                    94\n 5 Aven… Avengers              2048                    683                    85\n 6 Blac… Black P…              1336                    668                    96\n 7 Blac… Black P…               855                    342                    84\n 8 Blac… Unique                 379                    190                    79\n 9 Capt… Captain…               370                    264                    79\n10 Capt… Captain…              1151                    460                    90\n# ℹ 20 more rows\n# ℹ abbreviated name: ¹​percent_budget_recovered\n# ℹ 13 more variables: audience_percent_score &lt;dbl&gt;,\n#   audience_vs_critics_percent_deviance &lt;dbl&gt;, budget &lt;dbl&gt;,\n#   domestic_gross_m &lt;dbl&gt;, international_gross_m &lt;dbl&gt;,\n#   opening_weekend_m &lt;dbl&gt;, second_weekend_m &lt;dbl&gt;,\n#   x1st_vs_2nd_weekend_drop_off &lt;dbl&gt;, …"
  },
  {
    "objectID": "lectures/16-clustering.html#mcu-meets-mds",
    "href": "lectures/16-clustering.html#mcu-meets-mds",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "MCU meets MDS",
    "text": "MCU meets MDS\n\nmcu_quant &lt;- mcu_movies |&gt; dplyr::select(-c(film, category, year))\n\nmcu_quant &lt;- apply(mcu_quant, MARGIN = 2, FUN = function(x) x / sd(x))\nrownames(mcu_quant) &lt;- mcu_movies$film\n\nmcu_dist &lt;- dist(mcu_quant)\nmcu_mds &lt;- cmdscale(d = dist(mcu_quant), k = 2)\nmcu_movies &lt;- mcu_movies |&gt; mutate(mds1 = mcu_mds[,1], mds2 = mcu_mds[,2])\n\nmcu_movies |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  # Use text labels instead of points:\n  geom_text(aes(label = film), alpha = .75) +\n  labs(x = \"MDS Coordinate 1\", y = \"MDS Coordinate 2\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/16-clustering.html#mcu-meets-mds-output",
    "href": "lectures/16-clustering.html#mcu-meets-mds-output",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "MCU meets MDS",
    "text": "MCU meets MDS"
  },
  {
    "objectID": "lectures/16-clustering.html#other-ways-to-visualize-distances-and-clusters",
    "href": "lectures/16-clustering.html#other-ways-to-visualize-distances-and-clusters",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Other ways to visualize distances and clusters",
    "text": "Other ways to visualize distances and clusters\nMDS can be a great way to visualize distances and identify clusters…\n\nHowever, requires picking certain variables that identify clusters well\n\nIs there a way to automatically identify clusters in the dataset?\n\n\nDendrograms are a nice way to visualize distances\nAutomatically clusters different units together based on distance\n\n\\[\\overbrace{\\text{Dendro}}^{\\text{tree}}\\underbrace{\\text{gram}}_{\\text{drawing}}\\]\nFirst, let’s look at dendrograms and learn how to interpret them - then we’ll discuss how they’re made"
  },
  {
    "objectID": "lectures/16-clustering.html#textbook-example",
    "href": "lectures/16-clustering.html#textbook-example",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Textbook example",
    "text": "Textbook example"
  },
  {
    "objectID": "lectures/16-clustering.html#textbook-example-1",
    "href": "lectures/16-clustering.html#textbook-example-1",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Textbook example",
    "text": "Textbook example\n\n\n\nObservations that are closer together are on the same branch\nDoesn’t tell you how many clusters there are, but does tell you which observations are clustered together\nFor now: What is the computer doing to make dendrograms?"
  },
  {
    "objectID": "lectures/16-clustering.html#agglomerative-hierarchical-clustering",
    "href": "lectures/16-clustering.html#agglomerative-hierarchical-clustering",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "(Agglomerative) Hierarchical clustering",
    "text": "(Agglomerative) Hierarchical clustering\nLet’s pretend all \\(N\\) observations are in their own cluster\nStep 1: Compute the pairwise dissimilarities between each cluster\n\ne.g., distance matrix on previous slides\n\n\nStep 2: Identify the pair of clusters that are least dissimilar\n\n\nStep 3: Fuse these two clusters into a new cluster!\n\n\nRepeat Steps 1 to 3 until all observations are in the same cluster\n“Bottom-up”, agglomerative clustering that forms a tree / hierarchy of merging"
  },
  {
    "objectID": "lectures/16-clustering.html#agglomerative-hierarchical-clustering-1",
    "href": "lectures/16-clustering.html#agglomerative-hierarchical-clustering-1",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "(Agglomerative) Hierarchical clustering",
    "text": "(Agglomerative) Hierarchical clustering"
  },
  {
    "objectID": "lectures/16-clustering.html#forms-a-dendrogram",
    "href": "lectures/16-clustering.html#forms-a-dendrogram",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Forms a dendrogram",
    "text": "Forms a dendrogram"
  },
  {
    "objectID": "lectures/16-clustering.html#how-do-we-define-dissimilarity-between-clusters",
    "href": "lectures/16-clustering.html#how-do-we-define-dissimilarity-between-clusters",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "How do we define dissimilarity between clusters?",
    "text": "How do we define dissimilarity between clusters?\n\nWe know how to compute distance / dissimilarity between two observations\nBut how do we handle clusters?\n\nDissimilarity between a cluster and an observation, or between two clusters\n\n\n\n\nWe need to choose a linkage function! Clusters are built up by linking them together\nCompute all pairwise dissimilarities between observations in cluster 1 with observations in cluster 2\ni.e. Compute the distance matrix between observations, \\(d(x_i, x_j)\\) for \\(i \\in C_1\\) and \\(j \\in C_2\\)\n\n\n\n\n\nComplete linkage: Use the maximum value of these dissimilarities: \\(\\underset{i \\in C_1, j \\in C_2}{\\text{max}} d(x_i, x_j)\\)\nSingle linkage: Use the minimum value: \\(\\underset{i \\in C_1, j \\in C_2}{\\text{min}} d(x_i, x_j)\\)\nAverage linkage: Use the average value: \\(\\frac{1}{|C_1| \\cdot |C_2|} \\sum_{i \\in C_1} \\sum_{j \\in C_2} d(x_i, x_j)\\)\n\nDefine dissimilarity between two clusters based on our initial dissimilarity matrix between observations"
  },
  {
    "objectID": "lectures/16-clustering.html#ggdendro-version",
    "href": "lectures/16-clustering.html#ggdendro-version",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "ggdendro version",
    "text": "ggdendro version\n\nlibrary(ggdendro)\nggdendrogram(hc_complete, theme_dendro = FALSE) + #&lt;&lt;\n  labs(y = \"Cluster Dissimilarity (based on complete linkage)\", \n       title = \"Which MCU movies are similar to each other?\") + \n  coord_flip() + theme_bw() + theme(axis.title.y = element_blank())"
  },
  {
    "objectID": "lectures/16-clustering.html#display-mds-plot-with-dendrogram-side-by-side",
    "href": "lectures/16-clustering.html#display-mds-plot-with-dendrogram-side-by-side",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Display MDS plot with dendrogram side-by-side",
    "text": "Display MDS plot with dendrogram side-by-side\n\nlibrary(patchwork)\nhc_complete_ggdendro &lt;- ggdendrogram(hc_complete, theme_dendro = FALSE) +\n  labs(y = \"Cluster Dissimilarity (based on complete linkage)\", \n       title = \"Which MCU movies are similar to each other?\") + \n  coord_flip() +\n  theme_bw() +\n  # Remove the y-axis title (changed from x to y since we flipped it!)\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_text(size = 6),\n        plot.title = element_text(size = 10))\n\nmcu_mds_plot &lt;- mcu_movies |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  geom_text(aes(label = film),\n            alpha = .75, size = 2) +\n  labs(x = \"MDS Coordinate 1\", y = \"MDS Coordinate 2\") +\n  theme_bw()\n\nmcu_mds_plot + hc_complete_ggdendro"
  },
  {
    "objectID": "lectures/16-clustering.html#display-mds-plot-with-dendrogram-side-by-side-output",
    "href": "lectures/16-clustering.html#display-mds-plot-with-dendrogram-side-by-side-output",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Display MDS plot with dendrogram side-by-side",
    "text": "Display MDS plot with dendrogram side-by-side"
  },
  {
    "objectID": "lectures/16-clustering.html#how-do-we-assign-cluster-labels",
    "href": "lectures/16-clustering.html#how-do-we-assign-cluster-labels",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "How do we assign cluster labels?",
    "text": "How do we assign cluster labels?\nWe cut the dendrogram to return cluster labels\nTwo ways to specify how to cut the tree using the cutree function:\n\nvia the height using h, e.g., cut the tree at height = 10\n\n\ncutree(hc_complete, h = 10)\n\n\n\nvia the desired number of clusters k - and let the computer figure out the height for us, e.g., k = 2\n\n\ncutree(hc_complete, k = 2)"
  },
  {
    "objectID": "lectures/16-clustering.html#view-results-with-cut-on-dendrogram",
    "href": "lectures/16-clustering.html#view-results-with-cut-on-dendrogram",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "View results with cut on dendrogram",
    "text": "View results with cut on dendrogram\n\ncut_dendro &lt;- hc_complete_ggdendro +\n  # This is a horizontal line since its considered before the flip:\n  geom_hline(yintercept = 10, linetype = \"dashed\", \n             color = \"darkred\")\n\ncluster_mcu_mds_plot &lt;- mcu_movies |&gt;\n  mutate(cluster = as.factor(mcu_clusters)) |&gt;\n  ggplot(aes(x = mds1, y = mds2,\n             color = cluster)) +\n  geom_text(aes(label = film),\n            alpha = .75, size = 2) +\n  ggthemes::scale_color_colorblind() +\n  labs(x = \"MDS Coordinate 1\", y = \"MDS Coordinate 2\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\ncluster_mcu_mds_plot + cut_dendro"
  },
  {
    "objectID": "lectures/16-clustering.html#view-results-with-cut-on-dendrogram-output",
    "href": "lectures/16-clustering.html#view-results-with-cut-on-dendrogram-output",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "View results with cut on dendrogram",
    "text": "View results with cut on dendrogram"
  },
  {
    "objectID": "lectures/16-clustering.html#factoextra-package-version",
    "href": "lectures/16-clustering.html#factoextra-package-version",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "factoextra package version",
    "text": "factoextra package version\n\nlibrary(factoextra)\nfviz_dend(hc_complete, cex = 0.5, k = 3, color_labels_by_k = TRUE)"
  },
  {
    "objectID": "lectures/16-clustering.html#recap-and-next-steps",
    "href": "lectures/16-clustering.html#recap-and-next-steps",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nDendrograms are a great way to visualize distances and the clustering structure in the dataset\nHowever there are several decisions to be made!\nWhat type of linkage is appropriate for the problem?\nHow do we choose the number of clusters?\n\nThere is NOT a one size fits all solution to any of this!\n\n\n\n\nHW5 is due TONIGHT and you have lab this Friday!\nHW6 is posted and due next Wednesday March 26th\nNext time: PCA\nReview more code in lecture demos!"
  },
  {
    "objectID": "lectures/22-areal-data.html#announcements-previously-and-today",
    "href": "lectures/22-areal-data.html#announcements-previously-and-today",
    "title": "Visualizations and Inference for Areal Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW8 is due TONIGHT by 11:59 PM ET\nYou do NOT have lab this week\n\n\n\nLast time:\nThree main types of spatial data:\n\nPoint Pattern Data: lat-long coordinates where events have occurred\nPoint-Referenced data: Latitude-longitude (lat-long) coordinates as well as one or more variables specific to those coordinates.\nAreal Data: Geographic regions with one or more variables associated with those regions.\n\nWalked through how to plot point-referenced and point pattern data.\nTODAY: Visualizations and Inference for Areal Data"
  },
  {
    "objectID": "lectures/22-areal-data.html#thinking-about-areal-data",
    "href": "lectures/22-areal-data.html#thinking-about-areal-data",
    "title": "Visualizations and Inference for Areal Data",
    "section": "Thinking about areal data",
    "text": "Thinking about areal data\n\nAreal Data: Geographic regions associated with one or more variables specific to those regions\nAreal data will have the following form (example US states data from 1970s):\n\n\nstate_data |&gt; dplyr::slice(1:3)\n\n# A tibble: 3 × 9\n  Population Income Illiteracy `Life Exp` Murder `HS Grad` Frost   Area state  \n       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  \n1       3615   3624        2.1       69.0   15.1      41.3    20  50708 alabama\n2        365   6315        1.5       69.3   11.3      66.7   152 566432 alaska \n3       2212   4530        1.8       70.6    7.8      58.1    15 113417 arizona"
  },
  {
    "objectID": "lectures/22-areal-data.html#high-level-overview-of-steps",
    "href": "lectures/22-areal-data.html#high-level-overview-of-steps",
    "title": "Visualizations and Inference for Areal Data",
    "section": "High-level overview of steps",
    "text": "High-level overview of steps\n\nNeed to match the region with the actual geographic boundaries\nMany geographic boundaries/features are stored as “shapefiles”\n\ni.e., complicated polygons\n\nCan contain the lines, points, etc. to represent any geographic feature\nShapefiles are readily available for countries, states, counties, etc."
  },
  {
    "objectID": "lectures/22-areal-data.html#access-shapefiles-using-map_data",
    "href": "lectures/22-areal-data.html#access-shapefiles-using-map_data",
    "title": "Visualizations and Inference for Areal Data",
    "section": "Access shapefiles using map_data()",
    "text": "Access shapefiles using map_data()\n\nlibrary(maps)\nstate_borders &lt;- map_data(\"state\") \nhead(state_borders)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\n\nFor example: map_data(\"world\"), map_data(\"state\"), map_data(\"county\") (need to install maps package)\nContains lat/lon coordinates to draw geographic boundaries"
  },
  {
    "objectID": "lectures/22-areal-data.html#typica-workflow-for-plotting-areal-data",
    "href": "lectures/22-areal-data.html#typica-workflow-for-plotting-areal-data",
    "title": "Visualizations and Inference for Areal Data",
    "section": "Typica workflow for plotting areal data",
    "text": "Typica workflow for plotting areal data\n\nGet state-specific data\nGet state boundaries\nMerge state-specific data with state boundaries (using left_join())\n\n\nstate_plot_data &lt;- state_borders |&gt;\n  left_join(state_data, by = c(\"region\" = \"state\"))\nhead(state_plot_data)\n\n       long      lat group order  region subregion Population Income Illiteracy\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;       3615   3624        2.1\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;       3615   3624        2.1\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;       3615   3624        2.1\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;       3615   3624        2.1\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;       3615   3624        2.1\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;       3615   3624        2.1\n  Life Exp Murder HS Grad Frost  Area\n1    69.05   15.1    41.3    20 50708\n2    69.05   15.1    41.3    20 50708\n3    69.05   15.1    41.3    20 50708\n4    69.05   15.1    41.3    20 50708\n5    69.05   15.1    41.3    20 50708\n6    69.05   15.1    41.3    20 50708\n\n\n\nPlot the data"
  },
  {
    "objectID": "lectures/22-areal-data.html#create-a-choropleth-map-with-geom_polygon",
    "href": "lectures/22-areal-data.html#create-a-choropleth-map-with-geom_polygon",
    "title": "Visualizations and Inference for Areal Data",
    "section": "Create a choropleth map with geom_polygon()",
    "text": "Create a choropleth map with geom_polygon()\n\nstate_plot_data |&gt;\n  ggplot() + \n  geom_polygon(aes(x = long, y = lat, group = group, fill = Illiteracy), \n               color = \"black\") + \n  scale_fill_gradient2(low = \"darkgreen\", mid = \"lightgrey\", \n                       high = \"darkorchid4\", midpoint = 0.95) +\n  theme_void() +\n  coord_map(\"polyconic\") + \n  labs(fill = \"Illiteracy %\") + \n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/22-areal-data.html#create-a-choropleth-map-with-geom_polygon-output",
    "href": "lectures/22-areal-data.html#create-a-choropleth-map-with-geom_polygon-output",
    "title": "Visualizations and Inference for Areal Data",
    "section": "Create a choropleth map with geom_polygon()",
    "text": "Create a choropleth map with geom_polygon()"
  },
  {
    "objectID": "lectures/22-areal-data.html#uniform-size-with-statebins",
    "href": "lectures/22-areal-data.html#uniform-size-with-statebins",
    "title": "Visualizations and Inference for Areal Data",
    "section": "Uniform size with statebins",
    "text": "Uniform size with statebins\n\nlibrary(statebins)\nstate_data$new_state &lt;- str_to_title(state_data$state)\nstatebins(state_data = state_data, \n          state_col = \"new_state\", value_col = \"Illiteracy\") +\n  theme_statebins()"
  },
  {
    "objectID": "lectures/22-areal-data.html#many-choices-for-displaying-maps",
    "href": "lectures/22-areal-data.html#many-choices-for-displaying-maps",
    "title": "Visualizations and Inference for Areal Data",
    "section": "Many choices for displaying maps…",
    "text": "Many choices for displaying maps…"
  },
  {
    "objectID": "lectures/22-areal-data.html#inference-for-areal-data",
    "href": "lectures/22-areal-data.html#inference-for-areal-data",
    "title": "Visualizations and Inference for Areal Data",
    "section": "Inference for Areal Data",
    "text": "Inference for Areal Data\nFor areal data, we have the following variables:\n\nGeographic region: \\(g\\)\nOutcome variable: \\(z\\)\n\n\n\\(g\\) is categorical, so visualization/inference involves categorical data.\nIf \\(g\\) only has a few categories, can just do ANOVA and side-by-side violins (or other displays we’ve talked about).\nWhat to do if there are many regions?\nTwo approaches: Dendrograms and randomization tests."
  },
  {
    "objectID": "lectures/22-areal-data.html#dendrograms-for-areal-data",
    "href": "lectures/22-areal-data.html#dendrograms-for-areal-data",
    "title": "Visualizations and Inference for Areal Data",
    "section": "Dendrograms for Areal Data",
    "text": "Dendrograms for Areal Data\nRecall: Dendrograms allow you to see which subjects are similar and which are dissimilar in terms of one or more variables\nIntuition: Allows you to see which geographic regions are similar\n\nTo create a dendrogram:\n\nDefine a distance metric in terms of the outcome.\nPlot a dendrogram.\nMake the leaf labels correspond to geographic regions."
  },
  {
    "objectID": "lectures/22-areal-data.html#visual-randomization-test",
    "href": "lectures/22-areal-data.html#visual-randomization-test",
    "title": "Visualizations and Inference for Areal Data",
    "section": "Visual randomization test",
    "text": "Visual randomization test"
  },
  {
    "objectID": "lectures/22-areal-data.html#visual-randomization-test-1",
    "href": "lectures/22-areal-data.html#visual-randomization-test-1",
    "title": "Visualizations and Inference for Areal Data",
    "section": "Visual randomization test",
    "text": "Visual randomization test"
  },
  {
    "objectID": "lectures/22-areal-data.html#recap-and-next-steps",
    "href": "lectures/22-areal-data.html#recap-and-next-steps",
    "title": "Visualizations and Inference for Areal Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nCreate choropleths for areal data: color regions by variable of interest\n\nRequires workflow to join region level data with polygon boundaries for regions\n\nCan perform classical type categorical type inference with areal data\nUse dendrograms to visualize differences between regions based on variable of interest\nCan perform visual randomization test to test signficance of observed data\n\n\n\nHW8 is due Wednesday! You do NOT have lab this week\nNext time: Visualizations for text data\nRecommended reading: CW Chapter 15 Visualizing geospatial data, KH Chapter 7 Draw Maps"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#announcements-previously-and-today",
    "href": "lectures/08-1dquant-inference.html#announcements-previously-and-today",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW3 is due Wednesday by 11:59 PM and you have Lab 5 again on Friday!\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Tuesdays @ 11 AM\n\n\n\n\n\nSmoothed densities are a flexible tool for visualizing 1D distribution\nThere are two choices we need to make for kernel density estimation:\n\nBandwidth: Determines smoothness of distribution, usually data-driven choice\nKernel: Determines how much influence each observation should have on each other during estimation, usually context driven\n\nSeveral other types of density-based displays: violins, ridges, beeswarm plots\n\n\n\n\n\nTODAY:\n\nGraphical inference for 1D quantitative data\nParametric density estimates\nECDFs and Kolmogorov-Smirnov (KS) test"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-violin-plots",
    "href": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-violin-plots",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Visualizing conditional distributions: violin plots",
    "text": "Visualizing conditional distributions: violin plots\n\npenguins |&gt;\n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_violin() +\n  coord_flip()"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-violin-plots-1",
    "href": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-violin-plots-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Visualizing conditional distributions: violin plots",
    "text": "Visualizing conditional distributions: violin plots\n\npenguins |&gt;\n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_violin() + \n  geom_boxplot(width = .2) +\n  coord_flip()"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-ggridges",
    "href": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-ggridges",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Visualizing conditional distributions: ggridges",
    "text": "Visualizing conditional distributions: ggridges\n\nlibrary(ggridges)\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = species)) +\n  geom_density_ridges(rel_min_height = 0.01)"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#gallery-of-ggridges-examples",
    "href": "lectures/08-1dquant-inference.html#gallery-of-ggridges-examples",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Gallery of ggridges examples",
    "text": "Gallery of ggridges examples"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-ggbeeswarm",
    "href": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-ggbeeswarm",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Visualizing conditional distributions: ggbeeswarm",
    "text": "Visualizing conditional distributions: ggbeeswarm\n\nlibrary(ggbeeswarm)\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = species)) +\n  geom_beeswarm(cex = 1.5) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#kernel-density-estimation",
    "href": "lectures/08-1dquant-inference.html#kernel-density-estimation",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Kernel density estimation",
    "text": "Kernel density estimation\nGoal: estimate PDF \\(f(x)\\) for all possible values (assuming it is continuous & smooth)\n\\[\n\\text{Kernel density estimate: } \\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\n\\]\n\n\n\\(n =\\) sample size, \\(x =\\) new point to estimate \\(f(x)\\) (does NOT have to be in dataset!)\n\\(h =\\) bandwidth, analogous to histogram bin width, ensures \\(\\hat{f}(x)\\) integrates to 1\n\\(x_i =\\) \\(i\\)th observation in dataset\n\\(K_h(x - x_i)\\) is the Kernel function, creates weight given distance of \\(i\\)th observation from new point\n\nas \\(|x - x_i| \\rightarrow \\infty\\) then \\(K_h(x - x_i) \\rightarrow 0\\), i.e. further apart \\(i\\)th row is from \\(x\\), smaller the weight\nas bandwidth \\(h \\uparrow\\) weights are more evenly spread out (as \\(h \\downarrow\\) more concentrated around \\(x\\))\ntypically use Gaussian / Normal kernel: \\(\\propto e^{-(x - x_i)^2 / 2h^2}\\)\n\\(K_h(x - x_i)\\) is large when \\(x_i\\) is close to \\(x\\)"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#from-the-pdf-to-the-cdf",
    "href": "lectures/08-1dquant-inference.html#from-the-pdf-to-the-cdf",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "From the PDF to the CDF",
    "text": "From the PDF to the CDF\n\nProbability that continuous variable \\(X\\) takes a particular value is 0\ne.g., \\(P\\) (flipper_length_mm \\(= 200\\)) \\(= 0\\)\nInstead we use the probability density function (PDF) to provide a relative likelihood\n\n\n\nFor continuous variables we can use the cumulative distribution function (CDF),\n\\[\nF(x) = P(X \\leq x)\n\\]\n\n\n\n\nFor \\(n\\) observations we can easily compute the Empirical CDF (ECDF):\n\\[\\hat{F}_n(x)  = \\frac{\\text{# obs. with variable} \\leq x}{n} = \\frac{1}{n} \\sum_{i=1}^{n}1(x_i \\leq x)\\]\n\nwhere \\(1()\\) is the indicator function, i.e. ifelse(x_i &lt;= x, 1, 0)"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#display-full-distribution-with-ecdf-plot",
    "href": "lectures/08-1dquant-inference.html#display-full-distribution-with-ecdf-plot",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Display full distribution with ECDF plot",
    "text": "Display full distribution with ECDF plot\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  stat_ecdf() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#whats-the-relationship-between-these-two",
    "href": "lectures/08-1dquant-inference.html#whats-the-relationship-between-these-two",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "What’s the relationship between these two?",
    "text": "What’s the relationship between these two?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#comparing-to-theoretical-distributions",
    "href": "lectures/08-1dquant-inference.html#comparing-to-theoretical-distributions",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Comparing to theoretical distributions",
    "text": "Comparing to theoretical distributions"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#one-sample-kolmogorov-smirnov-test",
    "href": "lectures/08-1dquant-inference.html#one-sample-kolmogorov-smirnov-test",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "One-Sample Kolmogorov-Smirnov Test",
    "text": "One-Sample Kolmogorov-Smirnov Test\n\nWe compare the ECDF \\(\\hat{F}(x)\\) to a theoretical distribution’s CDF \\(F(x)\\)\nThe one sample KS test statistic is: \\(\\text{max}_x |\\hat{F}(x) - F(x)|\\)"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#parametric-density-estimation",
    "href": "lectures/08-1dquant-inference.html#parametric-density-estimation",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Parametric Density Estimation",
    "text": "Parametric Density Estimation\n\nInstead of trying to estimate the whole \\(f(x)\\) non-parametrically, we can assume a particular \\(f(x)\\) and estimate its parameters\nFor example, assume \\(X_i \\sim N(\\mu, \\sigma^2)\\). Then estimate the parameters:\n\n\\[\n\\hat{\\mu} = \\bar{x}, \\hspace{0.1in} \\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}\n\\]\n\nThen our density estimate is:\n\n\\[\n\\hat{f}(x) = \\frac{1}{\\sqrt{2\\pi} \\hat{\\sigma}} \\exp \\left( - \\frac{(x - \\hat{\\mu})^2}{2\\hat{\\sigma}^2} \\right)\n\\]"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#flipper-length-example",
    "href": "lectures/08-1dquant-inference.html#flipper-length-example",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Flipper length example",
    "text": "Flipper length example\nWhat if we assume flipper_length_mm follows Normal distribution?\n\ni.e., flipper_length_mm \\(\\sim N(\\mu, \\sigma^2)\\)\n\nNeed estimates for mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n\nflipper_length_mean &lt;- mean(penguins$flipper_length_mm, na.rm = TRUE)\nflipper_length_sd &lt;- sd(penguins$flipper_length_mm, na.rm = TRUE)\n\n\nPerform one-sample KS test using ks.test():\n\nks.test(x = penguins$flipper_length_mm, y = \"pnorm\",\n        mean = flipper_length_mean, sd = flipper_length_sd)\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  penguins$flipper_length_mm\nD = 0.12428, p-value = 5.163e-05\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#flipper-length-example-1",
    "href": "lectures/08-1dquant-inference.html#flipper-length-example-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Flipper length example",
    "text": "Flipper length example"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#statistical-tests-for-comparing-distributions",
    "href": "lectures/08-1dquant-inference.html#statistical-tests-for-comparing-distributions",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Statistical Tests for Comparing Distributions",
    "text": "Statistical Tests for Comparing Distributions\n\n\nWe’ve focused on assessing if a single quantitative variable follows a particular distribution\n\nLogic of one-sample KS test: Compare empirical distribution to theoretical distribution\n\n\n\n\n\nHow do we compare multiple empirical distributions?\nVery common scenario: Determine if a quantitative variable depends on a categorical variable, examples:\n\nClinical trials with multiple treatments\nAssessing differences across race, gender, socioeconomic status\nIndustrial experiments, A/B testing\nComparing song duration across different genres?\n\nCan use overlayed densities, side-by-side violin plots, facetted histograms\nRemember: plotting conditional distributions… but when are differences in a graphic statistically significant?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#tidytuesday-spotify-songs---duration-by-genre",
    "href": "lectures/08-1dquant-inference.html#tidytuesday-spotify-songs---duration-by-genre",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "TidyTuesday Spotify Songs - Duration by Genre",
    "text": "TidyTuesday Spotify Songs - Duration by Genre"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#tidytuesday-spotify-songs---duration-by-genre-1",
    "href": "lectures/08-1dquant-inference.html#tidytuesday-spotify-songs---duration-by-genre-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "TidyTuesday Spotify Songs - Duration by Genre",
    "text": "TidyTuesday Spotify Songs - Duration by Genre"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock",
    "href": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#kolmogorov-smirnov-test-again",
    "href": "lectures/08-1dquant-inference.html#kolmogorov-smirnov-test-again",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Kolmogorov-Smirnov Test… Again",
    "text": "Kolmogorov-Smirnov Test… Again\nKS test can also be used to compare two empirical distributions \\(\\hat{F}_A(x)\\) and \\(\\hat{F}_B\\), via test statistic for two samples:\n\\[\n\\text{max}_x |\\hat{F}_A(x) - \\hat{F}_B(x)|\n\\]\n\nspotify_songs &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')\nrap_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"rap\") |&gt; pull(duration_ms)\nrock_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"rock\") |&gt; pull(duration_ms)\n\nks.test(rap_duration, y = rock_duration)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  rap_duration and rock_duration\nD = 0.22386, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#kolmogorov-smirnov-test-again-1",
    "href": "lectures/08-1dquant-inference.html#kolmogorov-smirnov-test-again-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Kolmogorov-Smirnov Test… Again",
    "text": "Kolmogorov-Smirnov Test… Again\nKS test can also be used to compare two empirical distributions \\(\\hat{F}_A(x)\\) and \\(\\hat{F}_B\\), via test statistic for two samples:\n\\[\n\\text{max}_x |\\hat{F}_A(x) - \\hat{F}_B(x)|\n\\]"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#statistical-tests-for-comparing-distributions-1",
    "href": "lectures/08-1dquant-inference.html#statistical-tests-for-comparing-distributions-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Statistical Tests for Comparing Distributions",
    "text": "Statistical Tests for Comparing Distributions\n\nInfinite number of ways that you can compare multiple quantitative distributions, 3 common ways:\n\nAny difference at all? \nDifference in means?\n\n\nNull hypothesis: \\(H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_K\\) (use t.test or oneway.test() functions)\nCan assume the variances are all the same or differ\nIf reject, can only conclude not all means are equal\n\n\nDifference in variances?\n\n\nNull hypothesis: \\(H_0: \\sigma^2_1 = \\sigma^2_2 = \\cdots = \\sigma^2_K\\) (use bartlett.test() function)\nIf reject, can only conclude not all variances are equal\n\nUnlike the KS test, difference in means and variances are sensitive to non-Normality\n\nDifferent distributions can yield insignificant results"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock-1",
    "href": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock-2",
    "href": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock-2",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#test-difference-between-pop-and-rap",
    "href": "lectures/08-1dquant-inference.html#test-difference-between-pop-and-rap",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Test difference between pop and rap?",
    "text": "Test difference between pop and rap?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#recap-and-next-steps",
    "href": "lectures/08-1dquant-inference.html#recap-and-next-steps",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nIntroduced KS tests for testing differences in distributions\nBut when are the differences we’re seeing statistically significant?\n\nAny distributional difference? \\(\\rightarrow\\) KS test\nJust care about mean differences? \\(\\rightarrow\\) t-test\nJust care about variance differences? \\(\\rightarrow\\) Bartlett’s test\n\n\n\n\n\n\nHW3 is due Wednesday and you have Lab 5 on Friday\nNext time: Comparing Distributions and Statistical Power\nRecommended reading: CW Chapter 8 Visualizing distributions: Empirical cumulative distribution functions and q-q plots"
  },
  {
    "objectID": "lectures/21-spatial-data.html#announcements-previously-and-today",
    "href": "lectures/21-spatial-data.html#announcements-previously-and-today",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW8 is due Wednesday April 9th by 11:59 PM ET\nYou do NOT have lab this week\n\n\n\nLast time:\n\nDiscussed the role of animation in visualizations\nDiscussed various aspects of making high-quality graphics and relevant tools\n\nTODAY: Visualizations and Inference for Spatial Data"
  },
  {
    "objectID": "lectures/21-spatial-data.html#how-should-we-think-about-spatial-data",
    "href": "lectures/21-spatial-data.html#how-should-we-think-about-spatial-data",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "How should we think about spatial data?",
    "text": "How should we think about spatial data?\nTypically location is measured with latitude / longitude (2D)\n\n\n\nLatitude: Measures North / South (the “y-axis”)\n\nRange is \\((-90^{\\circ}, 90^{\\circ})\\)\nMeasures degrees from the equator \\((0^{\\circ})\\)\n\\((-90^{\\circ}, 0^{\\circ})\\) = southern hemisphere\n\\((0^{\\circ}, 90^{\\circ})\\) = northern hemisphere\n\n\n\n\nLongitude: Measures East/West (the “x-axis”)\n\nRange is \\((-180^{\\circ}, 180^{\\circ})\\)\nMeasures degrees from the prime meridian \\((0^{\\circ})\\) in Greenwich, England\n\\((-180^{\\circ}, 0^{\\circ})\\) = eastern hemisphere\n\\((0^{\\circ}, 180^{\\circ})\\) = western hemisphere"
  },
  {
    "objectID": "lectures/21-spatial-data.html#latitude-and-longitude",
    "href": "lectures/21-spatial-data.html#latitude-and-longitude",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Latitude and Longitude",
    "text": "Latitude and Longitude"
  },
  {
    "objectID": "lectures/21-spatial-data.html#map-projections",
    "href": "lectures/21-spatial-data.html#map-projections",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Map Projections",
    "text": "Map Projections\nMap projections: Transformation of the lat / long coordinates on a sphere (the earth) to a 2D plane\n\nThere are many different projections - each will distort the map in different ways.\nThe most common projections are:\n\nMercator\nRobinson\nConic\nCylindrical\nPlanar\nInterrupted projections"
  },
  {
    "objectID": "lectures/21-spatial-data.html#mercator-projection-1500s",
    "href": "lectures/21-spatial-data.html#mercator-projection-1500s",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Mercator Projection (1500s)",
    "text": "Mercator Projection (1500s)"
  },
  {
    "objectID": "lectures/21-spatial-data.html#mercator-projection-tissot-indicatrix",
    "href": "lectures/21-spatial-data.html#mercator-projection-tissot-indicatrix",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Mercator Projection (Tissot indicatrix)",
    "text": "Mercator Projection (Tissot indicatrix)"
  },
  {
    "objectID": "lectures/21-spatial-data.html#robinson-projection-standard-from-1963-1998",
    "href": "lectures/21-spatial-data.html#robinson-projection-standard-from-1963-1998",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Robinson Projection (Standard from 1963-1998)",
    "text": "Robinson Projection (Standard from 1963-1998)"
  },
  {
    "objectID": "lectures/21-spatial-data.html#robinson-projection-tissot-indicatrix",
    "href": "lectures/21-spatial-data.html#robinson-projection-tissot-indicatrix",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Robinson Projection (Tissot indicatrix)",
    "text": "Robinson Projection (Tissot indicatrix)"
  },
  {
    "objectID": "lectures/21-spatial-data.html#winkel-tripel-projection-proposed-1921-now-the-standard",
    "href": "lectures/21-spatial-data.html#winkel-tripel-projection-proposed-1921-now-the-standard",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Winkel Tripel Projection (proposed 1921, now the standard)",
    "text": "Winkel Tripel Projection (proposed 1921, now the standard)"
  },
  {
    "objectID": "lectures/21-spatial-data.html#winkel-tripel-projection-tissot-indicatrix",
    "href": "lectures/21-spatial-data.html#winkel-tripel-projection-tissot-indicatrix",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Winkel Tripel Projection (Tissot indicatrix)",
    "text": "Winkel Tripel Projection (Tissot indicatrix)"
  },
  {
    "objectID": "lectures/21-spatial-data.html#and-many-more-see-xkcd-comic",
    "href": "lectures/21-spatial-data.html#and-many-more-see-xkcd-comic",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "And many more… (see xkcd comic)",
    "text": "And many more… (see xkcd comic)"
  },
  {
    "objectID": "lectures/21-spatial-data.html#visualizing-spatial-data-on-maps-using-ggmap",
    "href": "lectures/21-spatial-data.html#visualizing-spatial-data-on-maps-using-ggmap",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Visualizing spatial data on maps using ggmap",
    "text": "Visualizing spatial data on maps using ggmap\n\nlibrary(ggmap)\n# First, we'll draw a \"box\" around the US (in terms of latitude and longitude)\nUS &lt;- c(left = -125, bottom = 10, right = -67, top = 49)\nmap &lt;- get_stadiamap(US, zoom = 5, maptype = \"stamen_toner_lite\")\n\n# Visualize the basic map\nggmap(map)\n\n\n\nDraw map based on lat / lon coordinates\nPut the box into get_stadiamap() to access Stamen Maps (you need an API key!)\nDraw the map using ggmap() to serve as base"
  },
  {
    "objectID": "lectures/21-spatial-data.html#visualizing-spatial-data-on-maps-using-ggmap-output",
    "href": "lectures/21-spatial-data.html#visualizing-spatial-data-on-maps-using-ggmap-output",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Visualizing spatial data on maps using ggmap",
    "text": "Visualizing spatial data on maps using ggmap"
  },
  {
    "objectID": "lectures/21-spatial-data.html#three-main-types-of-spatial-data",
    "href": "lectures/21-spatial-data.html#three-main-types-of-spatial-data",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Three main types of spatial data",
    "text": "Three main types of spatial data\n\nPoint Pattern Data: lat-long coordinates where events have occurred\nPoint-Referenced data: Latitude-longitude (lat-long) coordinates as well as one or more variables specific to those coordinates.\nAreal Data: Geographic regions with one or more variables associated with those regions.\n\n\n\nEach type is structured differently within a dataset\nEach type requires a different kind of graph(s)\n\n\n\nWe’re going to review each type of data. Then, we’re going to demonstrate how to plot these different data types\n\nToday: Point-referenced and point pattern\nNext time: Areal data"
  },
  {
    "objectID": "lectures/21-spatial-data.html#point-pattern-data",
    "href": "lectures/21-spatial-data.html#point-pattern-data",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Point-Pattern data",
    "text": "Point-Pattern data\n\nPoint Pattern Data: lat-long coordinates where events have occurred\nPoint pattern data simply records the lat-long of events; thus, there are only two columns\nAgain, latitude and longitude are represented with dots, sometimes called a dot or bubble map.\n\n\n\nThe goal is to understand how the density of events varies across space\nThe density of the dots can also be visualized (e.g., with contours)\n\nUse methods we’ve discussed before for visualizing 2D joint distribution"
  },
  {
    "objectID": "lectures/21-spatial-data.html#point-referenced-data",
    "href": "lectures/21-spatial-data.html#point-referenced-data",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Point-Referenced data",
    "text": "Point-Referenced data\n\nPoint-Referenced data: Latitude-longitude (lat-long) coordinates as well as one or more variables specific to those coordinates\nPoint-referenced data will have the following form:\n\n\nairports |&gt; dplyr::select(lat, lon, altitude, n_depart, n_arrive, name) |&gt; slice(1:3)\n\n# A tibble: 3 × 6\n    lat   lon altitude n_depart n_arrive name                        \n  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt; &lt;chr&gt;                       \n1 -6.08  145.     5282        5        5 Goroka Airport              \n2 -5.21  146.       20        8        8 Madang Airport              \n3 -5.83  144.     5388       10       12 Mount Hagen Kagamuga Airport\n\n\n\n\nThe goal is to understand how the variable(s) (e.g., altitude) vary across different spatial locations\nTypically, the latitude and longitude are represented with dots, and the variable(s) are represented with size and/or colors"
  },
  {
    "objectID": "lectures/21-spatial-data.html#adding-points-to-the-map-as-usual",
    "href": "lectures/21-spatial-data.html#adding-points-to-the-map-as-usual",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Adding points to the map as usual",
    "text": "Adding points to the map as usual\n\nggmap(map) +\n  geom_point(data = airports, aes(x = lon, y = lat), alpha = 0.25)"
  },
  {
    "objectID": "lectures/21-spatial-data.html#altering-points-on-the-map-in-the-usual-way",
    "href": "lectures/21-spatial-data.html#altering-points-on-the-map-in-the-usual-way",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Altering points on the map (in the usual way)",
    "text": "Altering points on the map (in the usual way)\n\nggmap(map) +\n  geom_point(data = airports, \n             aes(x = lon, y = lat, \n                 size = sqrt(n_depart), color = sqrt(n_arrive)),\n             alpha = .5) +\n  scale_size_area(breaks = sqrt(c(1, 5, 10, 50, 100, 500)), \n                  labels = c(1, 5, 10, 50, 100, 500), \n                  name = \"# departures\") +\n  scale_color_distiller(palette = \"Spectral\") +\n  labs(color = \"sqrt(# arrivals)\") +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/21-spatial-data.html#altering-points-on-the-map-in-the-usual-way-output",
    "href": "lectures/21-spatial-data.html#altering-points-on-the-map-in-the-usual-way-output",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Altering points on the map (in the usual way)",
    "text": "Altering points on the map (in the usual way)"
  },
  {
    "objectID": "lectures/21-spatial-data.html#inference-for-spatial-data",
    "href": "lectures/21-spatial-data.html#inference-for-spatial-data",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Inference for Spatial Data",
    "text": "Inference for Spatial Data\nThere are whole courses, textbooks, and careers dedicated to this. We’re not going to cover everything!\nHowever, there are some straightforward analyses that can be done for spatial data.\nPoint-Referenced Data:\n\nDivide geography into groups (e.g., north/south/east/west) and use regression to test if there are significant differences.\nRegression of \\(\\text{outcome} \\sim \\text{latitude} + \\text{longitude}\\). Smoothing regression (e.g., loess) is particularly useful here."
  },
  {
    "objectID": "lectures/21-spatial-data.html#visualizing-inference-for-point-reference-data",
    "href": "lectures/21-spatial-data.html#visualizing-inference-for-point-reference-data",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Visualizing Inference for Point-Reference Data",
    "text": "Visualizing Inference for Point-Reference Data\nFor basic linear regression:\n\nPlot \\((x, y)\\) as points\nFit the regression model \\(y \\sim x\\), to give us \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x\\)\nPlot \\((x, \\hat{y})\\) as a line\n\n\nFor point reference data, we have the following variables:\n\nInputs are longitude \\(x\\) and latitude \\(y\\), and outcome variable is \\(z\\)\n\nConsider the following linear regression model: \\(z \\sim \\text{lat} + \\text{long}\\)\nGoal: Make a visual involving \\((\\text{long}, \\text{lat}, \\hat{z})\\), and possibly \\(z\\)."
  },
  {
    "objectID": "lectures/21-spatial-data.html#kriging",
    "href": "lectures/21-spatial-data.html#kriging",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Kriging",
    "text": "Kriging\nGoal: Make a visual involving (long, lat, \\(\\hat{z}\\)) and possibly \\(z\\)\nWant \\(\\hat{z}\\) for many (long, lat) combos (not just the observed one!)\nTo do this, follow this procedure:\n\nFit the model \\(z \\sim \\text{lat} + \\text{long}\\)\nCreate a grid of \\((\\text{long}, \\text{lat})_{ij}\\)\nGenerate \\(\\hat{z}_{ij}\\) for each \\((\\text{long}, \\text{lat})_{ij}\\)\nPlot a heat map or contour plot of (long, lat, \\(\\hat{z}\\))\n\n\nYou can also add the actual \\(z\\) values (e.g., via size) on the heat map\n\nThis is known as kriging, or spatial interpolation"
  },
  {
    "objectID": "lectures/21-spatial-data.html#kriging-airline-data-example",
    "href": "lectures/21-spatial-data.html#kriging-airline-data-example",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Kriging: airline data example",
    "text": "Kriging: airline data example"
  },
  {
    "objectID": "lectures/21-spatial-data.html#kriging-creating-the-map",
    "href": "lectures/21-spatial-data.html#kriging-creating-the-map",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Kriging: creating the map",
    "text": "Kriging: creating the map"
  },
  {
    "objectID": "lectures/21-spatial-data.html#kriging-generating-the-grid",
    "href": "lectures/21-spatial-data.html#kriging-generating-the-grid",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Kriging: generating the grid",
    "text": "Kriging: generating the grid"
  },
  {
    "objectID": "lectures/21-spatial-data.html#kriging-generating-predicted-values",
    "href": "lectures/21-spatial-data.html#kriging-generating-predicted-values",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Kriging: generating predicted values",
    "text": "Kriging: generating predicted values"
  },
  {
    "objectID": "lectures/21-spatial-data.html#kriging-plotting-heat-map-of-predicted-values",
    "href": "lectures/21-spatial-data.html#kriging-plotting-heat-map-of-predicted-values",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Kriging: plotting heat map of predicted values",
    "text": "Kriging: plotting heat map of predicted values"
  },
  {
    "objectID": "lectures/21-spatial-data.html#kriging-overview",
    "href": "lectures/21-spatial-data.html#kriging-overview",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Kriging overview",
    "text": "Kriging overview\nThe steps used to create this map are…\n\nFit an interactive regression model using loess()\nMake a grid of lat/long coordinates, using seq() and expand.grid()\nGet estimated outcomes across the grid using predict()\nUse geom_contour_filled() to color map by estimated outcomes"
  },
  {
    "objectID": "lectures/21-spatial-data.html#recap-and-next-steps",
    "href": "lectures/21-spatial-data.html#recap-and-next-steps",
    "title": "Visualizations and Inference for Spatial Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nSpatial data is most commonly encoded in a 2D plane (latitude/longitude), i.e., maps\nDecisions to make: what projection to use? do we need all specific geolocations, or just general areas (e.g., states)?\nWhat kind of data do we have?\n\nPoint pattern: Scatterplots with density contours.\nPoint-referenced: Scatterplots with color/size, use regression/loess for inference.\n\n\n\n\nHW8 is due Wednesday! You do NOT have lab this week\nNext time: Areal data\nRecommended reading: CW Chapter 15 Visualizing geospatial data, KH Chapter 7 Draw Maps"
  },
  {
    "objectID": "lectures/06-1dquant.html#announcements-previously-and-today",
    "href": "lectures/06-1dquant.html#announcements-previously-and-today",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW2 is due Wednesday by 11:59 PM\nYou have Lab 4 again on Friday\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Wednesdays @ 11 AM\n\n\n\n\n\nCan create stacked and side-by-side bar charts to visualize 2D categorical data\nPerform 2D Chi-squared test to test if two categorical variables are associated with each other\nCreate mosaic plots to visualize 2D categorical data, shade by Pearson residuals\n\n\n\n\n\nTODAY:\n\nHow do we visualize 1D quantitative data?\nFor this week, we’ll focus on visualization issues and move to inference next week"
  },
  {
    "objectID": "lectures/06-1dquant.html#d-quantitative-data",
    "href": "lectures/06-1dquant.html#d-quantitative-data",
    "title": "Visualizing 1D Quantitative Data",
    "section": "1D Quantitative Data",
    "text": "1D Quantitative Data\nObservations are collected into a vector \\((x_1, \\dots, x_n)\\), \\(x_i \\in \\mathbb{R}\\) (or \\(\\mathbb{R}^+\\), \\(\\mathbb{Z}\\))\nCommon summary statistics for 1D quantitative data:\n\n\nCenter: Mean, median, weighted mean, mode\n\nRelated to the first moment, i.e., \\(\\mathbb{E}[X]\\)\n\n\n\n\n\nSpread: Variance, range, min/max, quantiles, IQR\n\nRelated to the second moment, i.e., \\(\\mathbb{E}[X^2]\\)\n\n\n\n\n\nShape: symmetry, skew, kurtosis (“peakedness”)\n\nRelated to higher order moments, i.e., skewness is \\(\\mathbb{E}[X^3]\\), kurtosis is \\(\\mathbb{E}[X^4]\\)\n\n\nCompute various statistics with summary(), mean(), median(), quantile(), range(), sd(), var(), etc."
  },
  {
    "objectID": "lectures/06-1dquant.html#box-plots-visualize-summary-statistics",
    "href": "lectures/06-1dquant.html#box-plots-visualize-summary-statistics",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Box plots visualize summary statistics",
    "text": "Box plots visualize summary statistics\n\npenguins |&gt;\n  ggplot(aes(y = flipper_length_mm)) +\n  geom_boxplot(aes(x = \"\")) +\n  coord_flip()"
  },
  {
    "objectID": "lectures/06-1dquant.html#histograms-display-1d-continuous-distributions",
    "href": "lectures/06-1dquant.html#histograms-display-1d-continuous-distributions",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Histograms display 1D continuous distributions",
    "text": "Histograms display 1D continuous distributions\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) +\n  geom_histogram()"
  },
  {
    "objectID": "lectures/06-1dquant.html#do-not-rely-on-box-plots",
    "href": "lectures/06-1dquant.html#do-not-rely-on-box-plots",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Do NOT rely on box plots…",
    "text": "Do NOT rely on box plots…"
  },
  {
    "objectID": "lectures/06-1dquant.html#simulate-from-mixture-of-normal-distributions",
    "href": "lectures/06-1dquant.html#simulate-from-mixture-of-normal-distributions",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Simulate from mixture of Normal distributions",
    "text": "Simulate from mixture of Normal distributions\nWill sample 100 draws from \\(N(-1.5, 1)\\) and 100 draws from \\(N(1.5, 1)\\)"
  },
  {
    "objectID": "lectures/06-1dquant.html#can-we-trust-the-default",
    "href": "lectures/06-1dquant.html#can-we-trust-the-default",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Can we trust the default?",
    "text": "Can we trust the default?\n\nset.seed(2025)\nfake_data &lt;- \n  tibble(fake_x = c(rnorm(100, -1.5), rnorm(100, 1.5))) |&gt;\n  mutate(component = c(rep(\"left\", 100), rep(\"right\", 100)))\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram() +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins",
    "href": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What happens as we change the number of bins?",
    "text": "What happens as we change the number of bins?\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(bins = 15) +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-1",
    "href": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-1",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What happens as we change the number of bins?",
    "text": "What happens as we change the number of bins?\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(bins = 60) +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-2",
    "href": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-2",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What happens as we change the number of bins?",
    "text": "What happens as we change the number of bins?\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(bins = 5) +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-3",
    "href": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-3",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What happens as we change the number of bins?",
    "text": "What happens as we change the number of bins?\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(bins = 100) +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#variability-of-graphs---30-bins",
    "href": "lectures/06-1dquant.html#variability-of-graphs---30-bins",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Variability of graphs - 30 bins",
    "text": "Variability of graphs - 30 bins\n\nset.seed(2025)\nfake_data &lt;- \n  tibble(fake_x = c(rnorm(100, -1.5), rnorm(100, 1.5))) |&gt;\n  mutate(component = c(rep(\"left\", 100), rep(\"right\", 100)))\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram() +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-happens-with-a-different-sample",
    "href": "lectures/06-1dquant.html#what-happens-with-a-different-sample",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What happens with a different sample?",
    "text": "What happens with a different sample?\n\nset.seed(1985)\nfake_data2 &lt;- \n  tibble(fake_x = c(rnorm(100, -1.5), rnorm(100, 1.5))) |&gt;\n  mutate(component = c(rep(\"left\", 100), rep(\"right\", 100)))\n\nfake_data2 |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram() +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#variability-of-graphs---15-bins",
    "href": "lectures/06-1dquant.html#variability-of-graphs---15-bins",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Variability of graphs - 15 bins",
    "text": "Variability of graphs - 15 bins"
  },
  {
    "objectID": "lectures/06-1dquant.html#variability-of-graphs---a-few-bins",
    "href": "lectures/06-1dquant.html#variability-of-graphs---a-few-bins",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Variability of graphs - a few bins",
    "text": "Variability of graphs - a few bins"
  },
  {
    "objectID": "lectures/06-1dquant.html#variability-of-graphs---too-many-bins",
    "href": "lectures/06-1dquant.html#variability-of-graphs---too-many-bins",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Variability of graphs - too many bins",
    "text": "Variability of graphs - too many bins"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-about-displaying-conditional-distributions",
    "href": "lectures/06-1dquant.html#what-about-displaying-conditional-distributions",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What about displaying conditional distributions?",
    "text": "What about displaying conditional distributions?\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_histogram(aes(fill = species))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-about-displaying-conditional-distributions-1",
    "href": "lectures/06-1dquant.html#what-about-displaying-conditional-distributions-1",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What about displaying conditional distributions?",
    "text": "What about displaying conditional distributions?\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_histogram(aes(fill = species),\n                 position = \"identity\", alpha = 0.3)"
  },
  {
    "objectID": "lectures/06-1dquant.html#normalize-histogram-frequencies-with-density",
    "href": "lectures/06-1dquant.html#normalize-histogram-frequencies-with-density",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Normalize histogram frequencies with density",
    "text": "Normalize histogram frequencies with density\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_histogram(aes(y = after_stat(density), fill = species),\n                 position = \"identity\", alpha = 0.3)"
  },
  {
    "objectID": "lectures/06-1dquant.html#can-use-density-curves-instead",
    "href": "lectures/06-1dquant.html#can-use-density-curves-instead",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Can use density curves instead",
    "text": "Can use density curves instead\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(aes(color = species))"
  },
  {
    "objectID": "lectures/06-1dquant.html#we-should-not-fill-the-density-curves",
    "href": "lectures/06-1dquant.html#we-should-not-fill-the-density-curves",
    "title": "Visualizing 1D Quantitative Data",
    "section": "We should NOT fill the density curves",
    "text": "We should NOT fill the density curves\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(aes(fill = species), alpha = .3)"
  },
  {
    "objectID": "lectures/06-1dquant.html#recap-and-next-steps",
    "href": "lectures/06-1dquant.html#recap-and-next-steps",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nVisualize 1D quantitative data to inspect center, spread, and shape\nBoxplots are only a display of summary statistics (i.e., they suck)\nHistograms display shape of the distribution, but comes with tradeoffs\nDensity curves provide an easy way to visualize conditional distributions\n\n\n\n\n\nHW2 is due Wednesday and you have Lab 4 on Friday\nNext time: Density estimation\nRecommended reading: CW Chapter 7 Visualizing distributions: Histograms and density plots"
  },
  {
    "objectID": "lectures/17-pca.html#announcements-previously-and-today",
    "href": "lectures/17-pca.html#announcements-previously-and-today",
    "title": "Principal Component Analysis",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW6 is due Wednesday March 26th by 11:59 PM ET\nYou have Lab 8 this Friday\nWe do NOT have in-class lecture on Wednesday! I will post a recording\n\n\n\nCommon workflow:\n\nReduce the data to a few “useful” dimensions\nPlot those “useful” dimensions\n\nLast two classes:\n\nReduce the data by summarizing pairs of subjects with one distance.\nVisualize distances using multi-dimensional scaling or dendrograms.\n\nHow can we reduce the data without distances?\nPrincipal Component Analysis (PCA) is by far the most popular way"
  },
  {
    "objectID": "lectures/17-pca.html#dimension-reduction---searching-for-variance",
    "href": "lectures/17-pca.html#dimension-reduction---searching-for-variance",
    "title": "Principal Component Analysis",
    "section": "Dimension reduction - searching for variance",
    "text": "Dimension reduction - searching for variance\nGOAL: Focus on reducing dimensionality of feature space while retaining most of the information in a lower dimensional space\n\n\\(n \\times p\\) matrix \\(\\rightarrow\\) dimension reduction technique \\(\\rightarrow\\) \\(n \\times k\\) matrix\n\n\nSpecial case we just discussed: MDS\n\n\\(n \\times n\\) distance matrix \\(\\rightarrow\\) MDS \\(\\rightarrow\\) \\(n \\times k\\) matrix (usually \\(k = 2\\))\n\n\nReduce data to a distance matrix\nReduce distance matrix to \\(k = 2\\) dimensions"
  },
  {
    "objectID": "lectures/17-pca.html#principal-component-analysis-pca",
    "href": "lectures/17-pca.html#principal-component-analysis-pca",
    "title": "Principal Component Analysis",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\\[\n\\begin{pmatrix}\n& & \\text{really} & & \\\\\n& & \\text{wide} & & \\\\\n& & \\text{matrix} & &\n\\end{pmatrix}\n\\rightarrow \\text{matrix algebra stuff} \\rightarrow\n\\begin{pmatrix}\n\\text{much}  \\\\\n\\text{thinner}  \\\\\n\\text{matrix}\n\\end{pmatrix}\n\\]\n\nStart with \\(n \\times p\\) matrix of correlated variables \\(\\rightarrow\\) \\(n \\times k\\) matrix of uncorrelated variables\n\n\n\nEach of the \\(k\\) columns in the right-hand matrix are principal components, all uncorrelated with each other\nFirst column accounts for most variation in the data, second column for second-most variation, and so on\n\nIntuition: first few principal components account for most of the variation in the data"
  },
  {
    "objectID": "lectures/17-pca.html#what-are-principal-components",
    "href": "lectures/17-pca.html#what-are-principal-components",
    "title": "Principal Component Analysis",
    "section": "What are principal components?",
    "text": "What are principal components?\n\nAssume \\(\\boldsymbol{X}\\) is a \\(n \\times p\\) matrix that is centered and stardardized\nTotal variation \\(= p\\), since Var( \\(\\boldsymbol{x}_j\\) ) = 1 for all \\(j = 1, \\dots, p\\)\nPCA will give us \\(p\\) principal components that are \\(n\\)-length columns - call these \\(Z_1, \\dots, Z_p\\)\n\n\nFirst principal component (aka PC1):\n\\[Z_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + \\dots + \\phi_{p1} X_p\\]\n\n\n\n\\(\\phi_{j1}\\) are the weights indicating contributions of each variable \\(j \\in 1, \\dots, p\\)\nWeights are normalized \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\)\n\\(\\phi_{1} = (\\phi_{11}, \\phi_{21}, \\dots, \\phi_{p1})\\) is the loading vector for PC1\n\n\n\n\n\\(Z_1\\) is a linear combination of the \\(p\\) variables that has the largest variance"
  },
  {
    "objectID": "lectures/17-pca.html#what-are-principal-components-1",
    "href": "lectures/17-pca.html#what-are-principal-components-1",
    "title": "Principal Component Analysis",
    "section": "What are principal components?",
    "text": "What are principal components?\nSecond principal component:\n\\[Z_2 = \\phi_{12} X_1 + \\phi_{22} X_2 + \\dots + \\phi_{p2} X_p\\]\n\n\\(\\phi_{j2}\\) are the weights indicating the contributions of each variable \\(j \\in 1, \\dots, p\\)\nWeights are normalized \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\)\n\\(\\phi_{2} = (\\phi_{12}, \\phi_{22}, \\dots, \\phi_{p2})\\) is the loading vector for PC2\n\\(Z_2\\) is a linear combination of the \\(p\\) variables that has the largest variance\n\nSubject to constraint it is uncorrelated with \\(Z_1\\)"
  },
  {
    "objectID": "lectures/17-pca.html#what-are-principal-components-2",
    "href": "lectures/17-pca.html#what-are-principal-components-2",
    "title": "Principal Component Analysis",
    "section": "What are principal components?",
    "text": "What are principal components?\nWe repeat this process to create \\(p\\) principal components\n\nUncorrelated: Each (\\(Z_j, Z_{j'}\\)) is uncorrelated with each other\nOrdered Variance: Var( \\(Z_1\\) ) \\(&gt;\\) Var( \\(Z_2\\) ) \\(&gt; \\dots &gt;\\) Var( \\(Z_p\\) )\nTotal Variance: \\(\\sum_{j=1}^p \\text{Var}(Z_j) = p\\)\n\nIntuition: pick some \\(k &lt;&lt; p\\) such that if \\(\\sum_{j=1}^k \\text{Var}(Z_j) \\approx p\\), then just using \\(Z_1, \\dots, Z_k\\)"
  },
  {
    "objectID": "lectures/17-pca.html#visualizing-pca-in-two-dimensions",
    "href": "lectures/17-pca.html#visualizing-pca-in-two-dimensions",
    "title": "Principal Component Analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/17-pca.html#visualizing-pca-in-two-dimensions-1",
    "href": "lectures/17-pca.html#visualizing-pca-in-two-dimensions-1",
    "title": "Principal Component Analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/17-pca.html#visualizing-pca-in-two-dimensions-2",
    "href": "lectures/17-pca.html#visualizing-pca-in-two-dimensions-2",
    "title": "Principal Component Analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/17-pca.html#visualizing-pca-in-two-dimensions-3",
    "href": "lectures/17-pca.html#visualizing-pca-in-two-dimensions-3",
    "title": "Principal Component Analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/17-pca.html#visualizing-pca-in-two-dimensions-4",
    "href": "lectures/17-pca.html#visualizing-pca-in-two-dimensions-4",
    "title": "Principal Component Analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/17-pca.html#so-what-do-we-do-with-the-principal-components",
    "href": "lectures/17-pca.html#so-what-do-we-do-with-the-principal-components",
    "title": "Principal Component Analysis",
    "section": "So what do we do with the principal components?",
    "text": "So what do we do with the principal components?\nThe point: given a dataset with \\(p\\) variables, we can find \\(k\\) variables \\((k &lt;&lt; p)\\) that account for most of the variation in the data\n\nNote that the principal components are NOT easy to interpret - these are combinations of all variables\nPCA is similar to MDS with these main differences:\n\nMDS reduces a distance matrix while PCA reduces a data matrix\nPCA has a principled way to choose \\(k\\)\nCan visualize how the principal components are related to variables in data"
  },
  {
    "objectID": "lectures/17-pca.html#working-with-pca-on-starbucks-drinks",
    "href": "lectures/17-pca.html#working-with-pca-on-starbucks-drinks",
    "title": "Principal Component Analysis",
    "section": "Working with PCA on Starbucks drinks",
    "text": "Working with PCA on Starbucks drinks\nUse the prcomp() function (based on SVD) for PCA on centered and scaled data\n\nstarbucks_pca &lt;- prcomp(dplyr::select(starbucks, serv_size_m_l:caffeine_mg),\n                        center = TRUE, scale. = TRUE)\nsummary(starbucks_pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000"
  },
  {
    "objectID": "lectures/17-pca.html#computing-principal-components",
    "href": "lectures/17-pca.html#computing-principal-components",
    "title": "Principal Component Analysis",
    "section": "Computing Principal Components",
    "text": "Computing Principal Components\nExtract the matrix of principal components \\(\\boldsymbol{Z} = XV\\) (dimension of \\(\\boldsymbol{Z}\\) will match original data)\n\nstarbucks_pc_matrix &lt;- starbucks_pca$x\nhead(starbucks_pc_matrix)\n\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9        PC10         PC11\n[1,] -0.02812472 0.006489978 0.05145094 -0.06678083 -0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 -0.08080545 -0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 -0.09389227 -0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 -0.11582260 -0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 -0.03631676 -0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 -0.03497690 -0.002469611\n\n\nColumns are uncorrelated, such that Var( \\(Z_1\\) ) \\(&gt;\\) Var( \\(Z_2\\) ) \\(&gt; \\dots &gt;\\) Var( \\(Z_p\\) ) - can start with a scatterplot of \\(Z_1, Z_2\\)"
  },
  {
    "objectID": "lectures/17-pca.html#starbucks-drinks-pc1-and-pc2",
    "href": "lectures/17-pca.html#starbucks-drinks-pc1-and-pc2",
    "title": "Principal Component Analysis",
    "section": "Starbucks drinks: PC1 and PC2",
    "text": "Starbucks drinks: PC1 and PC2\n\nstarbucks &lt;- starbucks |&gt;\n  mutate(pc1 = starbucks_pc_matrix[,1], pc2 = starbucks_pc_matrix[,2])\nstarbucks |&gt;\n  ggplot(aes(x = pc1, y = pc2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"PC 1\", y = \"PC 2\")"
  },
  {
    "objectID": "lectures/17-pca.html#making-pcs-interpretable-with-biplots-factoextra",
    "href": "lectures/17-pca.html#making-pcs-interpretable-with-biplots-factoextra",
    "title": "Principal Component Analysis",
    "section": "Making PCs interpretable with biplots (factoextra)",
    "text": "Making PCs interpretable with biplots (factoextra)\n\nlibrary(factoextra)\n# Designate to only label the variables:\nfviz_pca_biplot(starbucks_pca, \n                label = \"var\",\n                # Change the alpha for observations which is represented by ind\n                alpha.ind = .25,\n                # Modify the alpha for variables (var):\n                alpha.var = .75,\n                col.var = \"darkblue\")"
  },
  {
    "objectID": "lectures/17-pca.html#making-pcs-interpretable-with-biplots-factoextra-output",
    "href": "lectures/17-pca.html#making-pcs-interpretable-with-biplots-factoextra-output",
    "title": "Principal Component Analysis",
    "section": "Making PCs interpretable with biplots (factoextra)",
    "text": "Making PCs interpretable with biplots (factoextra)"
  },
  {
    "objectID": "lectures/17-pca.html#how-many-principal-components-to-use",
    "href": "lectures/17-pca.html#how-many-principal-components-to-use",
    "title": "Principal Component Analysis",
    "section": "How many principal components to use?",
    "text": "How many principal components to use?\nIntuition: Additional principal components will add smaller and smaller variance\n\nKeep adding components until the added variance drops off\n\n\nsummary(starbucks_pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000"
  },
  {
    "objectID": "lectures/17-pca.html#create-scree-plot-aka-elbow-plot-to-choose",
    "href": "lectures/17-pca.html#create-scree-plot-aka-elbow-plot-to-choose",
    "title": "Principal Component Analysis",
    "section": "Create scree plot (aka “elbow plot”) to choose",
    "text": "Create scree plot (aka “elbow plot”) to choose\n\nfviz_eig(starbucks_pca, addlabels = TRUE) + \n  geom_hline(yintercept = 100 * (1 / ncol(starbucks_pca$x)), linetype = \"dashed\", color = \"darkred\")"
  },
  {
    "objectID": "lectures/17-pca.html#recap-and-next-steps",
    "href": "lectures/17-pca.html#recap-and-next-steps",
    "title": "Principal Component Analysis",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nWalked through PCA for dimension reduction\nPCA is a very common way to define “most important dimensions”\nEspecially useful to plot principal components with a biplot (e.g., with factoextra)\n\n\n\nHW6 is due Wednesday March 26th and you have lab on Friday!\nI will not have office hours on Wednesday\nWe do NOT have lecture this Wednesday - instead I will post a recording\n\n\n\n\nNext time: Visualizing trends\nRecommended reading: CW Chapter 12 Visualizing associations among two or more quantitative variables"
  },
  {
    "objectID": "lectures/17-pca.html#pca-singular-value-decomposition-svd",
    "href": "lectures/17-pca.html#pca-singular-value-decomposition-svd",
    "title": "Principal Component Analysis",
    "section": "PCA: singular value decomposition (SVD)",
    "text": "PCA: singular value decomposition (SVD)\n\\[\nX = U D V^T\n\\]\n\nMatrices \\(U\\) and \\(V\\) contain the left and right (respectively) singular vectors of scaled matrix \\(X\\)\n\\(D\\) is the diagonal matrix of the singular values\nSVD simplifies matrix-vector multiplication as rotate, scale, and rotate again\n\n\\(V\\) is called the loading matrix for \\(X\\) with \\(\\phi_{j}\\) as columns,\n\n\\(Z = X  V\\) is the PC matrix"
  },
  {
    "objectID": "lectures/17-pca.html#eigenvalue-decomposition-aka-spectral-decomposition",
    "href": "lectures/17-pca.html#eigenvalue-decomposition-aka-spectral-decomposition",
    "title": "Principal Component Analysis",
    "section": "Eigenvalue decomposition (aka spectral decomposition)",
    "text": "Eigenvalue decomposition (aka spectral decomposition)\n\\[\nX = U D V^T\n\\]\n\n\\(V\\) are the eigenvectors of \\(X^TX\\) (covariance matrix, \\(^T\\) means transpose)\n\\(U\\) are the eigenvectors of \\(XX^T\\)\nThe singular values (diagonal of \\(D\\)) are square roots of the eigenvalues of \\(X^TX\\) or \\(XX^T\\)\nMeaning that \\(Z = UD\\)"
  },
  {
    "objectID": "lectures/17-pca.html#eigenvalues-guide-dimension-reduction",
    "href": "lectures/17-pca.html#eigenvalues-guide-dimension-reduction",
    "title": "Principal Component Analysis",
    "section": "Eigenvalues guide dimension reduction",
    "text": "Eigenvalues guide dimension reduction\nWe want to choose \\(p^* &lt; p\\) such that we are explaining variation in the data\nEigenvalues \\(\\lambda_j\\) for \\(j \\in 1, \\dots, p\\) indicate the variance explained by each component\n\n\\(\\sum_j^p \\lambda_j = p\\), meaning \\(\\lambda_j \\geq 1\\) indicates \\(\\text{PC}j\\) contains at least one variable’s worth in variability\n\\(\\lambda_j / p\\) equals proportion of variance explained by \\(\\text{PC}j\\)\nArranged in descending order so that \\(\\lambda_1\\) is largest eigenvalue and corresponds to PC1\nCan compute the cumulative proportion of variance explained (CVE) with \\(p^*\\) components:\n\n\\[\\text{CVE}_{p^*} = \\frac{\\sum_j^{p*} \\lambda_j}{p}\\]"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#announcements-previously-and-today",
    "href": "lectures/13-nonlinear-pairs.html#announcements-previously-and-today",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW5 is due next Wednesday March 19th by 11:59 PM ET\nAs part of Homework 5, you’ll form groups for final projects\n\nTeams should be 3-4 students, you can pick your teammates or be randomized to a team\nGoal of the project: create and interpet hiqh-quality visualizations for a dataset of your choice\nProject requirements and rubrics are available on Canvas\nEDA reports are graded as as group, while presentations are graded individually\nHW5 is short so you have time to form teams and explore datasets\n\nYou do NOT have lab this week\nTODAY: How does LOESS (nonlinear regression) work? And maybe pairs plots"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#beyond-linear-regression",
    "href": "lectures/13-nonlinear-pairs.html#beyond-linear-regression",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Beyond Linear Regression",
    "text": "Beyond Linear Regression\nMany kinds of regression methods - we’ll focus on local linear regression for now.\nLet’s say: Still willing to assume Normality, but not linearity where \\(f(x)\\) is some unknown function\n\\[Y_i \\stackrel{iid}{\\sim} N(\\underbrace{f(X_i)}, \\sigma^2)\\]\nIntuition: Any nonlinear function is locally linear\nWe saw this in the extrapolation example\n\nLocal linear regressions fits a bunch of, well, local linear regressions, and then glues them together\nLocal linear regression is basically weighted linear regression, where only “local units” get weight"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#weighted-linear-regression",
    "href": "lectures/13-nonlinear-pairs.html#weighted-linear-regression",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Weighted Linear Regression",
    "text": "Weighted Linear Regression\nRemember that in typical linear regression, we solve the following:\n\\[\\arg \\min_{\\beta_0,\\beta_1} \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\\]\nIn weighted linear regression, we solve the following:\n\\[\\arg \\min_{\\beta_0,\\beta_1} \\sum_{i=1}^n w_i \\cdot (Y_i - \\beta_0 - \\beta_1 X_i)^2\\]\nLocal linear regression is exactly the same, except the weights depend on which \\(x\\) we want to estimate \\(f(x)\\)."
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#local-linear-regression-via-loess",
    "href": "lectures/13-nonlinear-pairs.html#local-linear-regression-via-loess",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Local linear regression via LOESS",
    "text": "Local linear regression via LOESS\n\\(Y_i \\overset{iid}{\\sim} N(f(x), \\sigma^2)\\), where \\(f(x)\\) is some unknown function\n\nIn local linear regression, we estimate \\(f(X_i)\\):\n\\[\\text{arg }\\underset{\\beta_0, \\beta_1}{\\text{min}} \\sum_i^n w_i(x) \\cdot \\big(Y_i - \\beta_0 - \\beta_1 X_i \\big)^2\\]\n\n\ngeom_smooth() uses tri-cubic weighting:\n\\[w_i(d_i) = \\begin{cases} (1 - |d_i|^3)^3, \\text{ if } i \\in \\text{neighborhood of  } x, \\\\\n0 \\text{ if } i \\notin \\text{neighborhood of  } x \\end{cases}\\]\n\n\\(d_i\\) is the distance between \\(x\\) and \\(X_i\\) scaled to be between 0 and 1\nspan: decides proportion of observations in neighborhood (default is 0.75)"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#animation-example",
    "href": "lectures/13-nonlinear-pairs.html#animation-example",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Animation example",
    "text": "Animation example"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#animation-example---changing-the-span",
    "href": "lectures/13-nonlinear-pairs.html#animation-example---changing-the-span",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Animation example - changing the span",
    "text": "Animation example - changing the span"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess",
    "href": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Displaying trend lines: LOESS",
    "text": "Displaying trend lines: LOESS\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth()\n\n\nFor \\(n &gt; 1000\\), mgcv::gam() is used with formula = y ~ s(x, bs = \"cs\") and method = \"REML\""
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess-1",
    "href": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess-1",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Displaying trend lines: LOESS",
    "text": "Displaying trend lines: LOESS\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(span = .1)"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess-2",
    "href": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess-2",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Displaying trend lines: LOESS",
    "text": "Displaying trend lines: LOESS\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(span = 1)"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#back-to-the-penguins",
    "href": "lectures/13-nonlinear-pairs.html#back-to-the-penguins",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Back to the penguins…",
    "text": "Back to the penguins…\nPretend I give you this penguins dataset and I ask you to make a plot for every pairwise comparison…\n\npenguins |&gt; slice(1:3)\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nWe can create a pairs plot to see all pairwise relationships in one plot\nPairs plot can include the various kinds of pairwise plots we’ve seen:\n\nTwo quantitative variables: scatterplot\nOne categorical, one quantitative: side-by-side violins, stacked histograms, overlaid densities\nTwo categorical: stacked bars, side-by-side bars, mosaic plots"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#create-pairs-plots-with-ggally",
    "href": "lectures/13-nonlinear-pairs.html#create-pairs-plots-with-ggally",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Create pairs plots with GGally",
    "text": "Create pairs plots with GGally\n\nlibrary(GGally)\npenguins |&gt; ggpairs(columns = 3:6)"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#create-pairs-plots-with-ggally-1",
    "href": "lectures/13-nonlinear-pairs.html#create-pairs-plots-with-ggally-1",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Create pairs plots with GGally",
    "text": "Create pairs plots with GGally\n\npenguins |&gt; ggpairs(columns = 3:6,\n                    mapping = aes(alpha = 0.5))"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#flexibility-in-customization",
    "href": "lectures/13-nonlinear-pairs.html#flexibility-in-customization",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Flexibility in customization",
    "text": "Flexibility in customization\n\npenguins |&gt; \n  ggpairs(columns = c(\"bill_length_mm\", \"body_mass_g\", \"island\"),\n          mapping = aes(alpha = 0.5, color = species), \n          lower = list(\n            continuous = \"smooth_lm\", \n            combo = \"facetdensitystrip\"\n          ),\n          upper = list(\n            continuous = \"cor\",\n            combo = \"facethist\"\n          )\n  )"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#flexibility-in-customization-output",
    "href": "lectures/13-nonlinear-pairs.html#flexibility-in-customization-output",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Flexibility in customization",
    "text": "Flexibility in customization"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#see-demo-for-more",
    "href": "lectures/13-nonlinear-pairs.html#see-demo-for-more",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "See demo for more!",
    "text": "See demo for more!"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#recap-and-next-steps",
    "href": "lectures/13-nonlinear-pairs.html#recap-and-next-steps",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nLOESS may seem like magic, but it’s just a bunch of little linear regressions glued together\nPairs plots: Nice way to see all pairwise relationships in a dataset\n\n\n\n\nHW5 is due Wednesday March 19th and you do NOT have lab this Friday!\nNext time: Contour Plots and Heat Maps"
  },
  {
    "objectID": "lectures/05-2dcat.html#announcements-previously-and-today",
    "href": "lectures/05-2dcat.html#announcements-previously-and-today",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW1 is due TONIGHT by 11:59 PM\nYou have Lab 3 again on Friday\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Wednesdays @ 11 AM\n\n\n\n\n\nDiscussed how similar looking graphics can have very different statistical results (thinking about power)\nDiscussed the challenges of multiple testing\n\n\n\n\n\nTODAY:\n\nVisuals for 2D categorical data\nHow do we visualize inference for 2D categorical data?"
  },
  {
    "objectID": "lectures/05-2dcat.html#d-categorical-basics",
    "href": "lectures/05-2dcat.html#d-categorical-basics",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "2D categorical basics",
    "text": "2D categorical basics"
  },
  {
    "objectID": "lectures/05-2dcat.html#d-categorical-basics-1",
    "href": "lectures/05-2dcat.html#d-categorical-basics-1",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "2D categorical basics",
    "text": "2D categorical basics\n\naddmargins(table(\"Species\" = penguins$species, \"Island\" = penguins$island))\n\n           Island\nSpecies     Biscoe Dream Torgersen Sum\n  Adelie        44    56        52 152\n  Chinstrap      0    68         0  68\n  Gentoo       124     0         0 124\n  Sum          168   124        52 344\n\n\n\nColumn and row sums: marginal distributions\nValues within rows: conditional distribution for Island given Species\nValues within columns: conditional distribution for Species given Island\nBottom right: total number of observations"
  },
  {
    "objectID": "lectures/05-2dcat.html#connecting-distributions-to-visualizations",
    "href": "lectures/05-2dcat.html#connecting-distributions-to-visualizations",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Connecting distributions to visualizations",
    "text": "Connecting distributions to visualizations\nFive distributions for two categorical variables \\(A\\) and \\(B\\):\n\nMarginals: \\(P(A)\\) and \\(P(B)\\)\nConditionals: \\(P(A | B)\\) and \\(P(B|A)\\)\nJoint: \\(P(A, B)\\)\n\nWe use bar charts to visualize marginal distributions for categorical variables…\n\nAnd we’ll use more bar charts to visualize conditional and joint distributions!"
  },
  {
    "objectID": "lectures/05-2dcat.html#stacked-bar-charts---a-bar-chart-of-spine-charts",
    "href": "lectures/05-2dcat.html#stacked-bar-charts---a-bar-chart-of-spine-charts",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Stacked bar charts - a bar chart of spine charts",
    "text": "Stacked bar charts - a bar chart of spine charts\n\npenguins |&gt;\n  ggplot(aes(x = species, fill = island)) +\n  geom_bar() + \n  theme_bw()\n\n\n\n\nEasy to see marginal of species, i.e., \\(P(\\) x \\()\\)\nCan see conditional of island | species, i.e., \\(P(\\) fill | x \\()\\)\nHarder to see conditional of species | island, i.e., \\(P(\\) x | fill \\()\\)"
  },
  {
    "objectID": "lectures/05-2dcat.html#side-by-side-bar-charts",
    "href": "lectures/05-2dcat.html#side-by-side-bar-charts",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Side-by-side bar charts",
    "text": "Side-by-side bar charts\n\npenguins |&gt;\n  ggplot(aes(x = species, fill = island)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw()\n\n\n\n\nEasy to see conditional of island | species, i.e., \\(P(\\) fill | x \\()\\)\nCan see conditional of species | island, i.e., \\(P(\\) x | fill \\()\\)"
  },
  {
    "objectID": "lectures/05-2dcat.html#side-by-side-bar-charts-1",
    "href": "lectures/05-2dcat.html#side-by-side-bar-charts-1",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Side-by-side bar charts",
    "text": "Side-by-side bar charts\n\npenguins |&gt;\n  ggplot(aes(x = species, fill = island)) + \n  geom_bar(position = position_dodge(preserve = \"single\")) +\n  theme_bw()\n\n\n\n\nEasy to see conditional of island | species, i.e., \\(P(\\) fill | x \\()\\)\nCan see conditional of species | island, i.e., \\(P(\\) x | fill \\()\\)"
  },
  {
    "objectID": "lectures/05-2dcat.html#complete-missing-values-to-preserve-location",
    "href": "lectures/05-2dcat.html#complete-missing-values-to-preserve-location",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Complete missing values to preserve location",
    "text": "Complete missing values to preserve location\n\npenguins |&gt;\n  count(species, island) |&gt;\n  complete(species = unique(species), island = unique(island), \n           fill = list(n = 0)) |&gt;\n  ggplot(aes(x = species, y = n, fill = island)) + \n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/05-2dcat.html#what-do-you-prefer",
    "href": "lectures/05-2dcat.html#what-do-you-prefer",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "What do you prefer?",
    "text": "What do you prefer?"
  },
  {
    "objectID": "lectures/05-2dcat.html#chi-squared-test-for-1d-categorical-data",
    "href": "lectures/05-2dcat.html#chi-squared-test-for-1d-categorical-data",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Chi-squared test for 1D categorical data:",
    "text": "Chi-squared test for 1D categorical data:\n\n\nNull hypothesis \\(H_0\\): \\(p_1 = p_2 = \\dots = p_K\\), compute the test statistic:\n\n\\[\n\\chi^2 = \\sum_{j=1}^K \\frac{(O_j - E_j)^2}{E_j}\n\\]\n\n\\(O_j\\): observed counts in category \\(j\\)\n\\(E_j\\): expected counts under \\(H_0\\), i.e., each category is equally to occur \\(n / K = p_1 = p_2 = \\dots = p_K\\)\n\n\n\n\nchisq.test(table(penguins$species))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(penguins$species)\nX-squared = 31.907, df = 2, p-value = 1.179e-07"
  },
  {
    "objectID": "lectures/05-2dcat.html#inference-for-2d-categorical-data",
    "href": "lectures/05-2dcat.html#inference-for-2d-categorical-data",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Inference for 2D categorical data",
    "text": "Inference for 2D categorical data\n\nAgain we use the chi-squared test:\n\nNull hypothesis \\(H_0\\): variables \\(A\\) and \\(B\\) are independent, compute the test statistic:\n\n\\[\\chi^2 = \\sum_{i}^{K_A} \\sum_{j}^{K_B} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\n\n\\(O_{ij}\\): observed counts in contingency table\n\\(E_{ij}\\): expected counts under \\(H_0\\)\n\n\\[\n\\begin{aligned}\nE_{ij} &= n \\cdot P(A = a_i, B = b_j) \\\\\n&= n \\cdot P(A = a_i) P(B = b_j) \\\\\n&= n \\cdot \\left( \\frac{n_{i \\cdot}}{n} \\right) \\left( \\frac{ n_{\\cdot j}}{n} \\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/05-2dcat.html#inference-for-2d-categorical-data-1",
    "href": "lectures/05-2dcat.html#inference-for-2d-categorical-data-1",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Inference for 2D categorical data",
    "text": "Inference for 2D categorical data\n\nAgain we use the chi-squared test:\n\nNull hypothesis \\(H_0\\): variables \\(A\\) and \\(B\\) are independent, compute the test statistic:\n\n\\[\\chi^2 = \\sum_{i}^{K_A} \\sum_{j}^{K_B} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\n\n\\(O_{ij}\\): observed counts in contingency table\n\\(E_{ij}\\): expected counts under \\(H_0\\)\n\n\n\nchisq.test(table(penguins$species, penguins$island))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(penguins$species, penguins$island)\nX-squared = 299.55, df = 4, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/05-2dcat.html#visualize-independence-test-with-mosaic-plots",
    "href": "lectures/05-2dcat.html#visualize-independence-test-with-mosaic-plots",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Visualize independence test with mosaic plots",
    "text": "Visualize independence test with mosaic plots\n\nTwo variables are independent if knowing the level of one tells us nothing about the other\n\ni.e. \\(P(A | B) = P(A)\\), and that \\(P(A, B) = P(A) \\times P(B)\\)\n\nCreate a mosaic plot using base R\n\n\nmosaicplot(table(penguins$species, penguins$island))"
  },
  {
    "objectID": "lectures/05-2dcat.html#shade-by-pearson-residuals",
    "href": "lectures/05-2dcat.html#shade-by-pearson-residuals",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Shade by Pearson residuals",
    "text": "Shade by Pearson residuals\n\n\nThe test statistic is:\n\n\\[\\chi^2 = \\sum_{i}^{K_A} \\sum_{j}^{K_B} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\n\nDefine the Pearson residuals as:\n\n\\[r_{ij} = \\frac{O_{ij} - E_{ij}}{\\sqrt{E_{ij}}}\\]\n\nSide-note: In general, Pearson residuals are \\(\\frac{\\text{residuals}}{\\sqrt{\\text{variance}}}\\)\n\n\n\n\n\n\\(r_{ij} \\approx 0 \\rightarrow\\) observed counts are close to expected counts\n\\(|r_{ij}| &gt; 2 \\rightarrow\\) “significant” at level \\(\\alpha = 0.05\\).\nVery positive \\(r_{ij} \\rightarrow\\) more than expected, while very negative \\(r_{ij} \\rightarrow\\) fewer than expected\nColor by Pearson residuals to tell us which combos are much bigger/smaller than expected."
  },
  {
    "objectID": "lectures/05-2dcat.html#titanic-dataset-example",
    "href": "lectures/05-2dcat.html#titanic-dataset-example",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Titanic Dataset Example",
    "text": "Titanic Dataset Example\n\ntitanic &lt;- read_csv(\"https://raw.githubusercontent.com/ryurko/DataViz-Class-Data/main/titanic.csv\")\n\nQuestion: Does survival (yes/no) depend on cabin (1st/2nd/3rd)?\n\ntable(\"Survived?\" = titanic$Survived, \"Class\" = titanic$Pclass)\n\n         Class\nSurvived?   1   2   3\n        0  64  90 270\n        1 120  83  85\n\n\n\n\nchisq.test(table(\"Survived?\" = titanic$Survived, \"Class\" = titanic$Pclass))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(`Survived?` = titanic$Survived, Class = titanic$Pclass)\nX-squared = 91.081, df = 2, p-value &lt; 2.2e-16\n\n\nConclusion: Class and survival are dependent - but how?"
  },
  {
    "objectID": "lectures/05-2dcat.html#guardian-1000-songs-to-hear-before-you-die",
    "href": "lectures/05-2dcat.html#guardian-1000-songs-to-hear-before-you-die",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Guardian: 1000 songs to hear before you die",
    "text": "Guardian: 1000 songs to hear before you die"
  },
  {
    "objectID": "lectures/05-2dcat.html#recap-and-next-steps",
    "href": "lectures/05-2dcat.html#recap-and-next-steps",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nFor 2D categorical data we create visualizations for marginal, conditional, and joint distributions\nCan create stacked and side-by-side bar charts to visualize 2D categorical data\nPerform 2D Chi-squared test to test if two categorical variables are associated with each other\nCreate mosaic plots to visualize 2D categorical data\nShade mosaic plot tiles by Pearson residuals to see what drives association between two categorical variables (if any)\n\n\n\n\n\nHW1 is due TONIGHT and you have Lab 3 on Friday!\nNext time: Visualizing 1D quantitative data\nRecommended reading: CW Chapter 11 Visualizing nested proportions"
  },
  {
    "objectID": "lectures/11-regression-inference.html#announcements-previously-and-today",
    "href": "lectures/11-regression-inference.html#announcements-previously-and-today",
    "title": "Inference with Linear Regression",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW4 is due TONIGHT by 11:59 PM and you have Lab 6 again on Friday!\nTake-home exam is next week Wednesday Feb 26th\nHere’s how the exam will work:\n\nI’ll post the exam Monday evening, and it’s due Wednesday by 11:59 PM EDT (Feb 26th)\nExam will cover material from HW 1-4 and Labs 1-6\nQuestions will be similar to homeworks but more open-ended, e.g, instead of “make a side-by-side violin plot…” I’ll ask “Make a plot that compares the conditional distributions…”\nThere will NOT be class on Wednesday Feb 26th\nConflict Feb 26th? Let me know ASAP! Day-of accommodations will NOT be made, late submissions will NOT be accepted\n\n\n\n\nScatterplots are the most common visual for 2D quantitative variables\n\nMany ways to incorporate additional dimensions in scatterplots, e.g., color and shape\n\nLinear regression is by far the most common model for describing the relationship between 2+ quantitative variables\n\nCan also: transform the outcome, transform the covariates, do nonparametric “smoothing”\n\nUse graphs to assess linear regression assumptions, i.e., residual diagnostic plots\n\n\n\n\nTODAY: More linear regression and inference with linear regression"
  },
  {
    "objectID": "lectures/11-regression-inference.html#displaying-trend-lines-linear-regression",
    "href": "lectures/11-regression-inference.html#displaying-trend-lines-linear-regression",
    "title": "Inference with Linear Regression",
    "section": "Displaying trend lines: linear regression",
    "text": "Displaying trend lines: linear regression\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "lectures/11-regression-inference.html#assessing-assumptions-of-linear-regression",
    "href": "lectures/11-regression-inference.html#assessing-assumptions-of-linear-regression",
    "title": "Inference with Linear Regression",
    "section": "Assessing assumptions of linear regression",
    "text": "Assessing assumptions of linear regression\nLinear regression assumes \\(Y_i \\overset{iid}{\\sim} N(\\beta_0 + \\beta_1 X_i, \\sigma^2)\\)\n\nIf this is true, then \\(Y_i - \\hat{Y}_i \\overset{iid}{\\sim} N(0, \\sigma^2)\\)\n\n\nPlot residuals against \\(\\hat{Y}_i\\), residuals vs fit plot\n\nUsed to assess linearity, any divergence from mean 0\nUsed to assess equal variance, i.e., if \\(\\sigma^2\\) is homogenous across predictions/fits \\(\\hat{Y}_i\\)\n\n\n\nMore difficult to assess the independence and fixed \\(X\\) assumptions\n\nMake these assumptions based on subject-matter knowledge"
  },
  {
    "objectID": "lectures/11-regression-inference.html#residual-vs-fit-plots",
    "href": "lectures/11-regression-inference.html#residual-vs-fit-plots",
    "title": "Inference with Linear Regression",
    "section": "Residual vs fit plots",
    "text": "Residual vs fit plots\n\nlin_reg &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\n\ntibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")"
  },
  {
    "objectID": "lectures/11-regression-inference.html#residual-vs-fit-plots-1",
    "href": "lectures/11-regression-inference.html#residual-vs-fit-plots-1",
    "title": "Inference with Linear Regression",
    "section": "Residual vs fit plots",
    "text": "Residual vs fit plots\n\ntibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth()"
  },
  {
    "objectID": "lectures/11-regression-inference.html#examples-of-residual-vs-fit-plots",
    "href": "lectures/11-regression-inference.html#examples-of-residual-vs-fit-plots",
    "title": "Inference with Linear Regression",
    "section": "Examples of Residual-vs-Fit Plots",
    "text": "Examples of Residual-vs-Fit Plots"
  },
  {
    "objectID": "lectures/11-regression-inference.html#more-fun-with-penguins",
    "href": "lectures/11-regression-inference.html#more-fun-with-penguins",
    "title": "Inference with Linear Regression",
    "section": "More fun with penguins…",
    "text": "More fun with penguins…\nDemo 03: Walk through an example of plotting/running different linear regression models\n\nOutcome: bill depth (in mm)\nCovariates: bill length (in mm) and species\n\n\nLinear regression models we will consider:\n\nbill_depth_mm ~ bill_length_mm\nbill_depth_mm ~ bill_length_mm + species\nbill_depth_mm ~ bill_length_mm + species + bill_length_mm \\(\\times\\) species"
  },
  {
    "objectID": "lectures/11-regression-inference.html#model-1-bill_depth_mm-bill_length_mm",
    "href": "lectures/11-regression-inference.html#model-1-bill_depth_mm-bill_length_mm",
    "title": "Inference with Linear Regression",
    "section": "Model 1: bill_depth_mm ~ bill_length_mm",
    "text": "Model 1: bill_depth_mm ~ bill_length_mm"
  },
  {
    "objectID": "lectures/11-regression-inference.html#model-1-bill_depth_mm-bill_length_mm-1",
    "href": "lectures/11-regression-inference.html#model-1-bill_depth_mm-bill_length_mm-1",
    "title": "Inference with Linear Regression",
    "section": "Model 1: bill_depth_mm ~ bill_length_mm",
    "text": "Model 1: bill_depth_mm ~ bill_length_mm\n\nsummary(lm(bill_depth_mm ~ bill_length_mm, data = penguins))\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1381 -1.4263  0.0164  1.3841  4.5255 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    20.88547    0.84388  24.749  &lt; 2e-16 ***\nbill_length_mm -0.08502    0.01907  -4.459 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.922 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.05525,   Adjusted R-squared:  0.05247 \nF-statistic: 19.88 on 1 and 340 DF,  p-value: 1.12e-05"
  },
  {
    "objectID": "lectures/11-regression-inference.html#how-are-the-intercept-and-slope-estimated",
    "href": "lectures/11-regression-inference.html#how-are-the-intercept-and-slope-estimated",
    "title": "Inference with Linear Regression",
    "section": "How are the intercept and slope estimated?",
    "text": "How are the intercept and slope estimated?\n\nWe have data \\((X_i, Y_i)\\). Want to estimate \\(\\beta_0\\) and \\(\\beta_1\\), where \\(\\mathbb{E}[Y | X] = \\beta_0 + \\beta_1 X\\)\nIf we had \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), then \\(\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\)\nThe estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are obtained by solving\n\\[\\arg \\min_{\\beta_0,\\beta_1} \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\\]\n\nRemember that \\(\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\), so the above is saying: “Give me the \\(\\hat{Y}_i\\) such that \\((Y_i - \\hat{Y}_i)^2\\) is minimized, on average”\n\nThe estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are:\n\\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}\\]\n\\[\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X}\\]"
  },
  {
    "objectID": "lectures/11-regression-inference.html#assessing-the-fit-of-linear-regression",
    "href": "lectures/11-regression-inference.html#assessing-the-fit-of-linear-regression",
    "title": "Inference with Linear Regression",
    "section": "Assessing the Fit of Linear Regression",
    "text": "Assessing the Fit of Linear Regression\n\nIntuitively, the more \\(X\\) and \\(Y\\) are correlated, the better the fit of the linear regression\nCorrelation is defined as\n\\[\\rho = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^n (X_i - \\bar{X})^2 \\cdot \\sum_{i=1}^n (Y_i - \\bar{Y})^2}} = \\frac{\\text{Cov}(X,Y)}{ \\sqrt{\\text{Var}(X) \\cdot \\text{Var}(Y)} }\\]\n\nCorrelation is just a standardized covariance, where \\(-1 \\leq \\rho \\leq 1\\).\nMore generally, \\(R^2\\) measures the fraction of variability in the outcome accounted by the covariates:\n\n\\[R^2 = 1 - \\frac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2} = 1 - \\frac{\\text{SS}_{\\text{residuals}}}{\\text{SS}_{\\text{total}}}\\]\nThe higher \\(R^2\\), the more the association. When linear regression has one covariate, \\(R = \\rho\\)"
  },
  {
    "objectID": "lectures/11-regression-inference.html#multiple-linear-regression",
    "href": "lectures/11-regression-inference.html#multiple-linear-regression",
    "title": "Inference with Linear Regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nLet’s say we have a bunch of covariates \\(X_1,X_2,\\dots,X_p\\)\nThe statistical model for multiple linear regression is\n\\[Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_1 X_{i1} + \\cdots + \\beta_k X_{ip}, \\sigma^2), \\hspace{0.1in} \\text{for all } i=1,\\dots,n\\]\n\nCovariates can be quadratic, cubic, etc. forms of other covariates, so this is quite flexible\nHow do we know when we’ve included the “right” covariates?\nThe higher \\(R^2\\), the more the association. So, maximize \\(R^2\\)?\n\n\n\nHowever, adding more covariates always increases \\(R^2\\). Better to look at “adjusted \\(R^2\\)”, which accounts for this\nAlso common: AIC and BIC (smaller is better)"
  },
  {
    "objectID": "lectures/11-regression-inference.html#special-case---categorical-variables",
    "href": "lectures/11-regression-inference.html#special-case---categorical-variables",
    "title": "Inference with Linear Regression",
    "section": "Special Case - Categorical Variables",
    "text": "Special Case - Categorical Variables\nCan include categorical variables in multiple linear regression, but need to code them as “dummy variables” (i.e., indicator variables)\nSay a categorical variable has \\(k \\geq 2\\) levels. Need to create \\((k-1)\\) indicator variables, equal to 1 for one category and 0 otherwise\nImportant: Categorical variable may be coded numerically (e.g., Agree = 1, Disagree = -1, Not Sure = 0)\n\nIf you put this variable straight into lm(), it will fit a very different model!"
  },
  {
    "objectID": "lectures/11-regression-inference.html#understanding-the-categorical-variables-example",
    "href": "lectures/11-regression-inference.html#understanding-the-categorical-variables-example",
    "title": "Inference with Linear Regression",
    "section": "Understanding the Categorical Variables Example",
    "text": "Understanding the Categorical Variables Example\nExample: Penguins species: Adelie, Chinstrap, Gentoo. There are \\(k = 3\\) levels.\nCreate an indicator for Chinstrap and Gentoo: \\(I_C\\) and \\(I_G\\).\n\nIf \\(I_C = I_G = 0\\), then the penguin must be Adelie\n\nThe statistical model would be \\(Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_C I_C + \\beta_G I_G, \\sigma^2)\\)\n\n\\(\\beta_0\\): \n\\(\\beta_0 + \\beta_C\\): \n\\(\\beta_0 + \\beta_G\\): \nSignificant \\(\\beta_C\\) \\(\\rightarrow\\) \nSignificant \\(\\beta_G\\) \\(\\rightarrow\\) \nHow to compare Chinstrap and Gentoo?"
  },
  {
    "objectID": "lectures/11-regression-inference.html#understanding-interactions-categorical-example",
    "href": "lectures/11-regression-inference.html#understanding-interactions-categorical-example",
    "title": "Inference with Linear Regression",
    "section": "Understanding Interactions (Categorical Example)",
    "text": "Understanding Interactions (Categorical Example)\n\nSay we also have a quantitative variable \\(X\\) (bill length). Consider two statistical models:\n\n\\(Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_X X + \\beta_C I_C + \\beta_G I_G, \\sigma^2)\\)\n\\(Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_X X + \\beta_C I_C + \\beta_G I_G + \\beta_{CX} I_C X + \\beta_{GX} I_G X, \\sigma^2)\\)\n\n\n\n\nFor Model 1…\n\nThe intercept for Adelie is \\(\\beta_0\\); for Chinstrap it is \\(\\beta_0 + \\beta_C\\); for Gentoo it is \\(\\beta_0 + \\beta_G\\)\nThe slope for all species is \\(\\beta_X\\).\n\n\n\n\n\nFor Model 2…\n\nThe intercept for Adelie is \\(\\beta_0\\); for Chinstrap it is \\(\\beta_0 + \\beta_C\\); for Gentoo it is \\(\\beta_0 + \\beta_G\\).\nThe slope for Adelie is \\(\\beta_X\\); for Chinstrap it is \\(\\beta_X + \\beta_{CX}\\); for Gentoo it is \\(\\beta_X + \\beta_{GX}\\)\n\n\n\n\n\nSignificant coefficient for categorical variables by themselves? Significantly different intercepts\nSignificant coefficient for interactions with categorical variables? Significantly different slopes"
  },
  {
    "objectID": "lectures/11-regression-inference.html#model-2-bill_depth_mm-bill_length_mm-species",
    "href": "lectures/11-regression-inference.html#model-2-bill_depth_mm-bill_length_mm-species",
    "title": "Inference with Linear Regression",
    "section": "Model 2: bill_depth_mm ~ bill_length_mm + species",
    "text": "Model 2: bill_depth_mm ~ bill_length_mm + species"
  },
  {
    "objectID": "lectures/11-regression-inference.html#model-3-bill_depth_mm-bill_length_mm-species-bill_length_mm-times-species",
    "href": "lectures/11-regression-inference.html#model-3-bill_depth_mm-bill_length_mm-species-bill_length_mm-times-species",
    "title": "Inference with Linear Regression",
    "section": "Model 3: bill_depth_mm ~ bill_length_mm + species + bill_length_mm \\(\\times\\) species",
    "text": "Model 3: bill_depth_mm ~ bill_length_mm + species + bill_length_mm \\(\\times\\) species"
  },
  {
    "objectID": "lectures/11-regression-inference.html#a-few-linear-regression-warnings",
    "href": "lectures/11-regression-inference.html#a-few-linear-regression-warnings",
    "title": "Inference with Linear Regression",
    "section": "A Few Linear Regression Warnings",
    "text": "A Few Linear Regression Warnings\n\nSimpson’s Paradox\n\nThere is a negative linear relationship between two variables but a positive linear relationship within every subpopulation\nIn these cases, subgroup analysis is especially important\n\n\n\n\nIs the intercept meaningful?\n\nThink about whether \\(X = 0\\) makes scientific sense for a particular variable before you interpret the intercept\n\n\n\n\n\nInterpolation versus Extrapolation\n\nInterpolation is defined as prediction within the range of a variable\nExtrapolation is defined as prediction outside the range of a variable\nGenerally speaking, interpolation is more reliable than extrapolation (less sensitive to model misspecification)"
  },
  {
    "objectID": "lectures/11-regression-inference.html#extrapolation-example",
    "href": "lectures/11-regression-inference.html#extrapolation-example",
    "title": "Inference with Linear Regression",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/11-regression-inference.html#extrapolation-example-1",
    "href": "lectures/11-regression-inference.html#extrapolation-example-1",
    "title": "Inference with Linear Regression",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/11-regression-inference.html#extrapolation-example-2",
    "href": "lectures/11-regression-inference.html#extrapolation-example-2",
    "title": "Inference with Linear Regression",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/11-regression-inference.html#recap-and-next-steps",
    "href": "lectures/11-regression-inference.html#recap-and-next-steps",
    "title": "Inference with Linear Regression",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nUse graphs to assess linear regression assumptions, i.e., residual diagnostic plots\nDiscussed interpretation of coefficients, the role of categorical variables, and interactions\nHighlighted common problems to consider: Simpson’s Paradox, intercept meaning, and extrapolation\n\n\n\n\nHW4 is due TONIGHT and you have Lab 6 on Friday\nGraphics critique due Feb 28th!\nNext time: Midsemester Review (take-home exam on Feb 26th)\nRecommended reading: CW Chapter 12 Visualizing associations among two or more quantitative variables"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#announcements-previously-and-today",
    "href": "lectures/04-power-multiple-testing.html#announcements-previously-and-today",
    "title": "Power and multiple testing",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW1 is due this Wednesday Jan 29 by 11:59 PM\nYou have Lab 3 again on Friday\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Wednesdays @ 11 AM\n\n\n\n\n\nMain estimators: \\(\\underbrace{\\hat{p}_1,\\dots,\\hat{p}_K}_{\\text{proportions}}\\) for \\(K\\)-many categories\nChi-square test is the main statistical test for 1D categorical data, tests \\(H_0: p_1 = \\cdots = p_K\\)\nCan also make confidence intervals for \\(\\hat{p}_1,\\dots,\\hat{p}_K\\) (just multiply CIs by \\(n\\))\n\n\n\n\n\nTODAY:\n\nInterpreting CIs on graphs is tricky and have to be careful with multiple testing"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#graphics-versus-statistical-inference",
    "href": "lectures/04-power-multiple-testing.html#graphics-versus-statistical-inference",
    "title": "Power and multiple testing",
    "section": "Graphics versus Statistical Inference",
    "text": "Graphics versus Statistical Inference\n\nReminder Anscombe’s Quartet: where statistical inference was the same but the graphics were very different\n\n\n\nThe opposite can be true! Graphics are the same, but statistical inference is very different…"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14",
    "href": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14",
    "title": "Power and multiple testing",
    "section": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)",
    "text": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-1",
    "href": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-1",
    "title": "Power and multiple testing",
    "section": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)",
    "text": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-2",
    "href": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-2",
    "title": "Power and multiple testing",
    "section": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)",
    "text": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-3",
    "href": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-3",
    "title": "Power and multiple testing",
    "section": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)",
    "text": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#power-under-this-scenario-2n4-n4-n4",
    "href": "lectures/04-power-multiple-testing.html#power-under-this-scenario-2n4-n4-n4",
    "title": "Power and multiple testing",
    "section": "Power under this scenario: (2n/4, n/4, n/4)",
    "text": "Power under this scenario: (2n/4, n/4, n/4)"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#how-do-we-combine-graphs-with-inference",
    "href": "lectures/04-power-multiple-testing.html#how-do-we-combine-graphs-with-inference",
    "title": "Power and multiple testing",
    "section": "How do we combine graphs with inference?",
    "text": "How do we combine graphs with inference?\n\nSimply add \\(p\\)-values (or other info) to graph via text\nAdd confidence intervals to the graph\n\n\nNeed to remember what each CI is for!\nOur CIs on previous slides are for each \\(\\hat{p}_j\\) marginally, NOT jointly\nHave to be careful with multiple testing…"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#cis-will-visually-capture-uncertainty-in-estimates",
    "href": "lectures/04-power-multiple-testing.html#cis-will-visually-capture-uncertainty-in-estimates",
    "title": "Power and multiple testing",
    "section": "CIs will visually capture uncertainty in estimates",
    "text": "CIs will visually capture uncertainty in estimates"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#rough-rules-for-comparing-cis-on-bar-charts",
    "href": "lectures/04-power-multiple-testing.html#rough-rules-for-comparing-cis-on-bar-charts",
    "title": "Power and multiple testing",
    "section": "Rough rules for comparing CIs on bar charts",
    "text": "Rough rules for comparing CIs on bar charts\n\n\nComparing overlap of two CIs is NOT exactly the same as directly testing for a significant difference…\n\nReally you want CI( \\(\\hat{p}_1 - \\hat{p}_2\\) ), not CI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) )\nCI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) ) not overlapping implies \\(0 \\notin\\) CI( \\(\\hat{p}_1 - \\hat{p}_2\\) )\nHowever CI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) ) overlapping DOES NOT imply \\(0 \\in\\) CI( \\(\\hat{p}_1 - \\hat{p}_2\\) )\n\n\nRoughly speaking:\n\nIf CIs don’t overlap \\(\\rightarrow\\) significant difference\nIf CIs overlap a little \\(\\rightarrow\\) ambiguous\nIf CIs overlap a lot \\(\\rightarrow\\) no significant difference\n\n\n\n\nBut if we’re comparing more than two CIs simultaneously, we need to account for multiple testing!\n\nWhen you look for all non-overlapping CIs: making \\(\\binom{K}{2} = \\frac{K!}{2!(K-2)!}\\) pairwise tests in your head!"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#corrections-for-multiple-testing",
    "href": "lectures/04-power-multiple-testing.html#corrections-for-multiple-testing",
    "title": "Power and multiple testing",
    "section": "Corrections for multiple testing",
    "text": "Corrections for multiple testing\n\n\nIn those bar plots, when we determine whether CIs overlap we make 3 comparisons:\n\nA vs B\nA vs C\nB vs C\n\n\nThis is a multiple testing issue\n\n\n\n\nIn short: we will make Type 1 errors (chance of false rejecting) more than 5% of the time!\nReminder: Type 1 error = Rejecting \\(H_0\\) when \\(H_0\\) is true\ne.g., CIs don’t overlap but actually \\(H_0: p_A = p_B\\) is true\nIf only interested in A vs B and nothing else, then just construct 95% CI for A vs B and control error rate at 5%\nHowever, if we construct several CIs, where A vs B is just one comparison we make, our Type 1 error rate &gt; 5%!"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#corrections-for-multiple-testing-1",
    "href": "lectures/04-power-multiple-testing.html#corrections-for-multiple-testing-1",
    "title": "Power and multiple testing",
    "section": "Corrections for multiple testing",
    "text": "Corrections for multiple testing\n\nVast literature on corrections for multiple testing (beyond the scope of this class… but in my thesis!)\nBut you should understand the following:\n\nCorrections for multiple testing inflate \\(p\\)-values (i.e., make them bigger)\nEquivalently, they inflate CIs (i.e., make them wider)\nPurpose of these corrections is to control Type 1 error rate \\(\\leq 5\\%\\)\n\n\n\n\nWe’ll focus on the Bonferroni correction, which inflates \\(p\\)-values the most but is easy to implement and very popular:\n\nWe usually reject null hypothesis when \\(p\\)-value \\(\\leq .05\\)\nBonferroni: if making \\(K\\) comparisons, reject only if \\(p\\)-value \\(\\leq .05/K\\)\nFor CIs: instead of plotting 95% CIs, we plot (1 - \\(0.05/K\\))% CIs\n\ne.g., for \\(K = 3\\) then plot 98.3% CIs"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#impact-of-bonferroni-correction-on-cis",
    "href": "lectures/04-power-multiple-testing.html#impact-of-bonferroni-correction-on-cis",
    "title": "Power and multiple testing",
    "section": "Impact of Bonferroni correction on CIs…",
    "text": "Impact of Bonferroni correction on CIs…"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#recap-and-next-steps",
    "href": "lectures/04-power-multiple-testing.html#recap-and-next-steps",
    "title": "Power and multiple testing",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nGraphs with the same trends can display very different statistical significance (largely due to sample size)\nCan visualize CIs for each \\(\\hat{p}_1\\), \\(\\dots\\), \\(\\hat{p}_K\\), but need to deal with multiple testing\n\n\n\n\n\nHW1 is due Wednesday and you have Lab 3 on Friday!\nNext time: Visualizations and inference for 2D categorical data\nRecommended reading: CW Chapter 11 Visualizing nested proportions"
  },
  {
    "objectID": "lectures/18-trends.html#announcements-previously-and-today",
    "href": "lectures/18-trends.html#announcements-previously-and-today",
    "title": "Visualizing Trends",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW6 is due Wednesday March 26th by 11:59 PM ET\nYou have Lab 8 this Friday\nWe do NOT have in-class lecture on Wednesday! I will post a recording\n\n\n\nCommon workflow:\n\nReduce the data to a few “useful” dimensions\nPlot those “useful” dimensions\n\nLast two classes:\n\nReduce the data by summarizing pairs of subjects with one distance.\nVisualize distances using multi-dimensional scaling or dendrograms.\n\nHow can we reduce the data without distances?\nPrincipal Component Analysis (PCA) is by far the most popular way"
  },
  {
    "objectID": "lectures/18-trends.html#longitudinal-data-and-time-series-structure",
    "href": "lectures/18-trends.html#longitudinal-data-and-time-series-structure",
    "title": "Visualizing Trends",
    "section": "Longitudinal data and time series structure",
    "text": "Longitudinal data and time series structure\n\nConsider a single observation measured across time\n\n\n\n\nVariable\n\\(T_1\\)\n\\(T_2\\)\n\\(\\dots\\)\n\\(T_J\\)\n\n\n\n\n\\(X_1\\)\n\\(x_{11}\\)\n\\(x_{12}\\)\n\\(\\dots\\)\n\\(x_{1J}\\)\n\n\n\\(X_2\\)\n\\(x_{21}\\)\n\\(x_{22}\\)\n\\(\\dots\\)\n\\(x_{2J}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\dots\\)\n\\(\\vdots\\)\n\n\n\\(X_P\\)\n\\(x_{P1}\\)\n\\(x_{P2}\\)\n\\(\\dots\\)\n\\(x_{PJ}\\)\n\n\n\n\nWith \\(N\\) observations we have \\(N\\) of these matrices\nTime may consist of regularly spaced intervals\n\nFor example, \\(T_1 = t\\), \\(T_2 = t + h\\), \\(T_3 = t + 2h\\), etc.\n\nIrregularly spaced intervals, then work with the raw \\(T_1,T_2,...\\)"
  },
  {
    "objectID": "lectures/18-trends.html#example-statistics-phds-by-year",
    "href": "lectures/18-trends.html#example-statistics-phds-by-year",
    "title": "Visualizing Trends",
    "section": "Example: Statistics PhDs by year",
    "text": "Example: Statistics PhDs by year\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  theme_light() +\n  labs(x = \"Year\", y = \"Number of PhDs\", title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#example-statistics-phds-by-year-1",
    "href": "lectures/18-trends.html#example-statistics-phds-by-year-1",
    "title": "Visualizing Trends",
    "section": "Example: Statistics PhDs by year",
    "text": "Example: Statistics PhDs by year\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year), \n                     labels = unique(stat_phd_year_summary$year)) + \n  theme_light() +\n  labs(x = \"Year\", y = \"Number of PhDs\", title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#add-lines-to-emphasize-order",
    "href": "lectures/18-trends.html#add-lines-to-emphasize-order",
    "title": "Visualizing Trends",
    "section": "Add lines to emphasize order",
    "text": "Add lines to emphasize order\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#drop-points-to-emphasize-trends",
    "href": "lectures/18-trends.html#drop-points-to-emphasize-trends",
    "title": "Visualizing Trends",
    "section": "Drop points to emphasize trends",
    "text": "Drop points to emphasize trends\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#can-fill-the-area-under-the-line",
    "href": "lectures/18-trends.html#can-fill-the-area-under-the-line",
    "title": "Visualizing Trends",
    "section": "Can fill the area under the line",
    "text": "Can fill the area under the line\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_area(fill = \"darkblue\", alpha = 0.5) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#several-time-series-do-not-only-use-points",
    "href": "lectures/18-trends.html#several-time-series-do-not-only-use-points",
    "title": "Visualizing Trends",
    "section": "Several time series? Do NOT only use points",
    "text": "Several time series? Do NOT only use points\n\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_point() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"bottom\", legend.text = element_text(size = 7)) +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\",\n       color = \"Field\")"
  },
  {
    "objectID": "lectures/18-trends.html#several-time-series-use-lines",
    "href": "lectures/18-trends.html#several-time-series-use-lines",
    "title": "Visualizing Trends",
    "section": "Several time series? Use lines!",
    "text": "Several time series? Use lines!\n\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#using-ggrepel-to-directly-label-lines",
    "href": "lectures/18-trends.html#using-ggrepel-to-directly-label-lines",
    "title": "Visualizing Trends",
    "section": "Using ggrepel to directly label lines",
    "text": "Using ggrepel to directly label lines\n\nstats_phds_2017 &lt;- stats_phds |&gt; filter(year == 2017)\n\nlibrary(ggrepel)\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  # Add the labels:\n  geom_text_repel(data = stats_phds_2017, aes(label = field),\n                  size = 3, \n                  # Drop the segment connection:\n                  segment.color = NA, \n                  # Move labels up or down based on overlap\n                  direction = \"y\",\n                  # Try to align the labels horizontally on the left hand side\n                  hjust = \"left\") +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year),\n                     # Update the limits so that there is some padding on the\n                     # x-axis but don't label the new maximum\n                     limits = c(min(stat_phd_year_summary$year),\n                                max(stat_phd_year_summary$year) + 3)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#using-ggrepel-to-directly-label-lines-output",
    "href": "lectures/18-trends.html#using-ggrepel-to-directly-label-lines-output",
    "title": "Visualizing Trends",
    "section": "Using ggrepel to directly label lines",
    "text": "Using ggrepel to directly label lines"
  },
  {
    "objectID": "lectures/18-trends.html#using-gghighlight-instead",
    "href": "lectures/18-trends.html#using-gghighlight-instead",
    "title": "Visualizing Trends",
    "section": "Using gghighlight instead",
    "text": "Using gghighlight instead\n\nlibrary(gghighlight)\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  gghighlight()  +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#using-gghighlight-instead-output",
    "href": "lectures/18-trends.html#using-gghighlight-instead-output",
    "title": "Visualizing Trends",
    "section": "Using gghighlight instead",
    "text": "Using gghighlight instead"
  },
  {
    "objectID": "lectures/18-trends.html#using-gghighlight-instead-1",
    "href": "lectures/18-trends.html#using-gghighlight-instead-1",
    "title": "Visualizing Trends",
    "section": "Using gghighlight instead",
    "text": "Using gghighlight instead\n\nlibrary(gghighlight)\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  gghighlight(line_label_type = \"sec_axis\")  +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#using-gghighlight-instead-1-output",
    "href": "lectures/18-trends.html#using-gghighlight-instead-1-output",
    "title": "Visualizing Trends",
    "section": "Using gghighlight instead",
    "text": "Using gghighlight instead"
  },
  {
    "objectID": "lectures/18-trends.html#how-do-we-plot-many-lines-not-like-this",
    "href": "lectures/18-trends.html#how-do-we-plot-many-lines-not-like-this",
    "title": "Visualizing Trends",
    "section": "How do we plot many lines? NOT LIKE THIS!",
    "text": "How do we plot many lines? NOT LIKE THIS!\n\nphd_field |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#how-do-we-plot-many-lines-not-like-this-output",
    "href": "lectures/18-trends.html#how-do-we-plot-many-lines-not-like-this-output",
    "title": "Visualizing Trends",
    "section": "How do we plot many lines? NOT LIKE THIS!",
    "text": "How do we plot many lines? NOT LIKE THIS!"
  },
  {
    "objectID": "lectures/18-trends.html#instead-we-highlight-specific-lines",
    "href": "lectures/18-trends.html#instead-we-highlight-specific-lines",
    "title": "Visualizing Trends",
    "section": "Instead we highlight specific lines",
    "text": "Instead we highlight specific lines\n\nphd_field |&gt;\n  filter(!(field %in% c(\"Biometrics and biostatistics\", \"Statistics (mathematics)\"))) |&gt;\n  ggplot() +\n  # Add the background lines - need to specify the group to be the field\n  geom_line(aes(x = year, y = n_phds, group = field),\n            color = \"gray\", size = .5, alpha = .5) +\n  # Now add the layer with the lines of interest:\n  geom_line(data = filter(phd_field,\n                          # Note this is just the opposite of the above since ! is removed\n                          field %in% c(\"Biometrics and biostatistics\", \n                                       \"Statistics (mathematics)\")),\n            aes(x = year, y = n_phds, color = field),\n            # Make the size larger\n            size = .75, alpha = 1) +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"bottom\", \n        # Drop the panel lines making the gray difficult to see\n        panel.grid = element_blank()) +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#instead-we-highlight-specific-lines-output",
    "href": "lectures/18-trends.html#instead-we-highlight-specific-lines-output",
    "title": "Visualizing Trends",
    "section": "Instead we highlight specific lines",
    "text": "Instead we highlight specific lines"
  },
  {
    "objectID": "lectures/18-trends.html#or-you-can-use-gghighlight-instead",
    "href": "lectures/18-trends.html#or-you-can-use-gghighlight-instead",
    "title": "Visualizing Trends",
    "section": "Or you can use gghighlight instead",
    "text": "Or you can use gghighlight instead\n\nphd_field |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  gghighlight(field %in% c(\"Biometrics and biostatistics\", \"Statistics (mathematics)\"),\n              line_label_type = \"sec_axis\") +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#or-you-can-use-gghighlight-instead-output",
    "href": "lectures/18-trends.html#or-you-can-use-gghighlight-instead-output",
    "title": "Visualizing Trends",
    "section": "Or you can use gghighlight instead",
    "text": "Or you can use gghighlight instead"
  },
  {
    "objectID": "lectures/18-trends.html#what-about-nightingales-rose-diagram",
    "href": "lectures/18-trends.html#what-about-nightingales-rose-diagram",
    "title": "Visualizing Trends",
    "section": "What about Nightingale’s rose diagram?",
    "text": "What about Nightingale’s rose diagram?"
  },
  {
    "objectID": "lectures/18-trends.html#what-about-nightingales-rose-diagram-1",
    "href": "lectures/18-trends.html#what-about-nightingales-rose-diagram-1",
    "title": "Visualizing Trends",
    "section": "What about Nightingale’s rose diagram?",
    "text": "What about Nightingale’s rose diagram?"
  },
  {
    "objectID": "lectures/18-trends.html#what-about-displaying-lines-instead",
    "href": "lectures/18-trends.html#what-about-displaying-lines-instead",
    "title": "Visualizing Trends",
    "section": "What about displaying lines instead?",
    "text": "What about displaying lines instead?"
  },
  {
    "objectID": "lectures/18-trends.html#recap-and-next-steps",
    "href": "lectures/18-trends.html#recap-and-next-steps",
    "title": "Visualizing Trends",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nDiscussed various aspects of visualizing trends\nWhen visualizing many lines, often useful to consider highlighting a small subset\n\n\n\nHW6 is due TONIGHT and you have lab on Friday!\nI will not have office hours today\n\n\n\n\nNext time: Time series, autocorrelation, and seasonal decomposition\nRecommended reading: CW CH 13 Visualizing time series and other functions of an independent variable, CW CH 14 Visualizing trends"
  },
  {
    "objectID": "demos.html",
    "href": "demos.html",
    "title": "Demos",
    "section": "",
    "text": "Demo\nDate\nTitle\nDemo file\n\n\n\n\n1\nJan 13\nInto the tidyverse\nHTML\n\n\n2\nFeb 17\nScatterplots and Linear Regression\nHTML\n\n\n3\nFeb 19\nMore Regression with Penguins\nHTML\n\n\n4\nMar 10\nNonlinear Regression and Pairs Plots\nHTML\n\n\n5\nMar 12\nContour Plots, Heat Maps, and Into High-Dimensional Data\nHTML\n\n\n6\nMar 17\nVisualizing Distances for High-Dimensional Data\nHTML\n\n\n7\nMar 19\nMore MDS and Creating Dendrograms\nHTML\n\n\n8\nMar 24\nPrincipal Component Analysis\nHTML\n\n\n9\nMar 26\nVisualizing Trends\nHTML\n\n\n10\nMar 31\nVisualizing time series data\nHTML\n\n\n11\nApr 2\nAnimations, modifying colors and themes\nHTML\n\n\n12\nApr 7\nVisualizations and inference for spatial data\nHTML\n\n\n13\nApr 9\nVisualizations and inference for areal data\nHTML\n\n\n14\nApr 14\nTidy Text Data and Sentiment Analysis\nHTML\n\n\n15\nApr 16\nTopic Models\nHTML",
    "crumbs": [
      "Demos"
    ]
  },
  {
    "objectID": "demos/03-regression.html",
    "href": "demos/03-regression.html",
    "title": "Demo 03: More Regression with Penguins",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs. Throughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")\n\nWe load the penguins data in the same way as before:\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "demos/03-regression.html#regression-with-penguins",
    "href": "demos/03-regression.html#regression-with-penguins",
    "title": "Demo 03: More Regression with Penguins",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs. Throughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")\n\nWe load the penguins data in the same way as before:\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "demos/03-regression.html#simple-linear-regression-based-only-on-bill-length",
    "href": "demos/03-regression.html#simple-linear-regression-based-only-on-bill-length",
    "title": "Demo 03: More Regression with Penguins",
    "section": "Simple Linear Regression (based only on bill length)",
    "text": "Simple Linear Regression (based only on bill length)\nFirst, we can run a simple linear regression (the first model) based only on bill length. We can display this line via geom_smooth():\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE)\n\n\n\n\n\n\n\n\nAnd display the regression model output using summary():\n\nsummary(lm(bill_depth_mm ~ bill_length_mm, data = penguins))\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1381 -1.4263  0.0164  1.3841  4.5255 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    20.88547    0.84388  24.749  &lt; 2e-16 ***\nbill_length_mm -0.08502    0.01907  -4.459 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.922 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.05525,   Adjusted R-squared:  0.05247 \nF-statistic: 19.88 on 1 and 340 DF,  p-value: 1.12e-05\n\n\nWe can write this regression model as:\n\\[\\text{depth} \\sim N(\\beta_0 + \\beta_L \\cdot \\text{length}, \\sigma^2)\\]\nNote that \\(\\beta_0\\) is the intercept and \\(\\beta_L\\) is the slope.\nThus, our estimates are:\n\n\\(\\hat{\\beta}_0 = 20.88547\\)\n\\(\\hat{\\beta}_L = 12.43\\)\n\\(\\hat{\\sigma}^2 = 1.922^2\\)"
  },
  {
    "objectID": "demos/03-regression.html#multiple-linear-regression-additive",
    "href": "demos/03-regression.html#multiple-linear-regression-additive",
    "title": "Demo 03: More Regression with Penguins",
    "section": "Multiple Linear Regression (Additive)",
    "text": "Multiple Linear Regression (Additive)\nWe can also run the second model, which is based on length and species, but with only additive effects. First, we’ll check the counts of the species variable to ensure that the species with the highest number of observations if the reference level (i.e., the first level for a factor variable):\n\ntable(penguins$species)\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n\nLooks like we’re lucky and that the Adelie species is the most popular and is already first due to alphabetical order. What function would we need to do to re-order the variable?\nNext, we’ll fit the regression that accounts for species without an interaction - so it’s just an additive effect:\n\ndepth_lm_species_add &lt;- lm(bill_depth_mm ~ bill_length_mm + species,\n                           data = penguins)\nsummary(depth_lm_species_add)\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm + species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4529 -0.6864 -0.0508  0.5519  3.5915 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      10.59218    0.68302  15.508  &lt; 2e-16 ***\nbill_length_mm    0.19989    0.01749  11.427  &lt; 2e-16 ***\nspeciesChinstrap -1.93319    0.22416  -8.624 2.55e-16 ***\nspeciesGentoo    -5.10602    0.19142 -26.674  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9533 on 338 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.769, Adjusted R-squared:  0.7669 \nF-statistic: 375.1 on 3 and 338 DF,  p-value: &lt; 2.2e-16\n\n\nThis tells us that Chinstrap is different from Adelie and Gentoo is different from Adelie, but it does NOT tell us Chinstrap is different from Gentoo. That would require another model with a reordered species variable. Exercise: Reorder species so that Gentoo is the reference level and compare to the results above.\nWe can manually extract intercepts and coefficients to use for plotting (read the code comments!):\n\n# Calculate species-specific intercepts in order:\nintercepts &lt;- # First for `Adelie` it's just the initial intercept\n  c(coef(depth_lm_species_add)[\"(Intercept)\"],\n    # Next for `Chinstrap` it's the intercept plus the `Chinstrap` term:\n    coef(depth_lm_species_add)[\"(Intercept)\"] + \n      coef(depth_lm_species_add)[\"speciesChinstrap\"],\n    # And finally for `Gentoo` it's again the intercept plus the `Gentoo` term\n    coef(depth_lm_species_add)[\"(Intercept)\"] + \n      coef(depth_lm_species_add)[\"speciesGentoo\"])\n\n# Create a small table to store the intercept, slopes, and species:\nlines_tbl &lt;- tibble(\"intercepts\" = intercepts,\n                    # Slopes are the same for each, thus use rep()\n                    \"slopes\" = rep(coef(depth_lm_species_add)[\"bill_length_mm\"],\n                                   3),\n                    # And the levels of species:\n                    \"species\" = levels(penguins$species))\n\nWe can now plot this model by specifying the regression lines with geom_abline() using the newly constructed lines_tbl as the data for this layer:\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(data = lines_tbl,\n              aes(intercept = intercepts, slope = slopes,\n                  color = species)) +\n  labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\", \n       title = \"Bill depth versus weight by species\")\n\n\n\n\n\n\n\n\nThis is a great example of Simpson’s Paradox! We originally observed a negative linear relationship between depth and length, but now observe a positive linear relationship within species!"
  },
  {
    "objectID": "demos/03-regression.html#multiple-linear-regression-interactive",
    "href": "demos/03-regression.html#multiple-linear-regression-interactive",
    "title": "Demo 03: More Regression with Penguins",
    "section": "Multiple Linear Regression (Interactive)",
    "text": "Multiple Linear Regression (Interactive)\nNext, we can run the third model, which is based on length and species, including interaction effects. This is the default type of model displayed when we map species to the color aesthetic for the geom_smooth() layer. In the plot below, we display across both layers, geom_point() and geom_smooth() by mapping species to color in the initial ggplot canvas construction:\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\", \n       title = \"Bill depth versus weight by species\")\n\n\n\n\n\n\n\n\nWhat about the summary of this model? Is the inclusion of interaction terms relevant? Note that by default, multiplying two variables in the lm() formula below includes both the additive AND interaction terms.\n\ndepth_lm_species_int &lt;- lm(bill_depth_mm ~ bill_length_mm * species,\n                           data = penguins)\nsummary(depth_lm_species_int)\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm * species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6574 -0.6675 -0.0524  0.5383  3.5032 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     11.40912    1.13812  10.025  &lt; 2e-16 ***\nbill_length_mm                   0.17883    0.02927   6.110 2.76e-09 ***\nspeciesChinstrap                -3.83998    2.05398  -1.870 0.062419 .  \nspeciesGentoo                   -6.15812    1.75451  -3.510 0.000509 ***\nbill_length_mm:speciesChinstrap  0.04338    0.04558   0.952 0.341895    \nbill_length_mm:speciesGentoo     0.02601    0.04054   0.642 0.521590    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9548 on 336 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7662 \nF-statistic: 224.5 on 5 and 336 DF,  p-value: &lt; 2.2e-16\n\n\nThe interaction terms do NOT appear to be necessary to include. This is justified by both the lack of significance and the slight drop in adjusted R-squared."
  },
  {
    "objectID": "demos/03-regression.html#what-about-the-intercept",
    "href": "demos/03-regression.html#what-about-the-intercept",
    "title": "Demo 03: More Regression with Penguins",
    "section": "What about the intercept?",
    "text": "What about the intercept?\nRemember the meaning of the intercept term… that is not reasonable in this setting because penguins will never have bills with length of 0mm! We should update the additive model (since we found the interaction terms to not be meaningful) to remove the intercept. This can be done by adding a 0 term to the lm() formula:\n\ndepth_lm_remove_b0 &lt;- lm(bill_depth_mm ~ 0 + bill_length_mm + species,\n                         data = penguins)\nsummary(depth_lm_remove_b0)\n\n\nCall:\nlm(formula = bill_depth_mm ~ 0 + bill_length_mm + species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4529 -0.6864 -0.0508  0.5519  3.5915 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \nbill_length_mm    0.19989    0.01749  11.427  &lt; 2e-16 ***\nspeciesAdelie    10.59218    0.68302  15.508  &lt; 2e-16 ***\nspeciesChinstrap  8.65899    0.86207  10.044  &lt; 2e-16 ***\nspeciesGentoo     5.48616    0.83547   6.567 1.94e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9533 on 338 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.997, Adjusted R-squared:  0.997 \nF-statistic: 2.795e+04 on 4 and 338 DF,  p-value: &lt; 2.2e-16\n\n\nWhat changed in the summary output? Why did that occur?\nWe can copy-and-paste our code from above to add these appropriate regression lines:\n\n# Calculate species-specific intercepts in order:\nnew_intercepts &lt;- # First for `Adelie` \n  c(coef(depth_lm_remove_b0)[\"speciesAdelie\"],\n    # Next for `Chinstrap` \n    coef(depth_lm_remove_b0)[\"speciesChinstrap\"],\n    # And finally for `Gentoo` \n    coef(depth_lm_remove_b0)[\"speciesGentoo\"])\n\n# Create a small table to store the intercept, slopes, and species:\nnew_lines_tbl &lt;- \n  tibble(\"intercepts\" = new_intercepts,\n         # Slopes are the same for each, thus use rep()\n         \"slopes\" = rep(coef(depth_lm_remove_b0)[\"bill_length_mm\"],\n                        3),\n         # And the levels of species:\n         \"species\" = levels(penguins$species))\n\nAgain, create the display:\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(data = new_lines_tbl,\n              aes(intercept = intercepts, slope = slopes,\n                  color = species)) +\n  labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\", \n       title = \"Bill depth versus weight by species\")\n\n\n\n\n\n\n\n\nWhy is this the same display as before? Here’s a great description of why we observe a higher R-squared with the intercept-term excluded from the model."
  },
  {
    "objectID": "demos/01-into-tidyverse.html",
    "href": "demos/01-into-tidyverse.html",
    "title": "Demo 01: Into the tidyverse",
    "section": "",
    "text": "(Broadly speaking) EDA = questions about data + wrangling + visualization\nR for Data Science: “EDA is a state of mind”, an iterative cycle:\n\ngenerate questions\nanswer via transformations and visualizations\n\nExample of questions?\n\nWhat type of variation do the variables display?\nWhat type of relationships exist between variables?\n\nGoal: develop understanding and become familiar with your data\n\nEDA is NOT a replacement for statistical inference and learning\nEDA is an important and necessary step to build intuition\n\nWe tackle the challenges of EDA with a data science workflow. An example of this according to Hadley Wickham in R for Data Science:\n\n\n\n\n\nAspects of data wrangling:\n\nimport: reading in data (e.g., read_csv())\ntidy: rows = observations, columns = variables (i.e. tabular data)\ntransform: filter observations, create new variables, summarize, etc."
  },
  {
    "objectID": "demos/01-into-tidyverse.html#what-is-exploratory-data-analysis-eda",
    "href": "demos/01-into-tidyverse.html#what-is-exploratory-data-analysis-eda",
    "title": "Demo 01: Into the tidyverse",
    "section": "",
    "text": "(Broadly speaking) EDA = questions about data + wrangling + visualization\nR for Data Science: “EDA is a state of mind”, an iterative cycle:\n\ngenerate questions\nanswer via transformations and visualizations\n\nExample of questions?\n\nWhat type of variation do the variables display?\nWhat type of relationships exist between variables?\n\nGoal: develop understanding and become familiar with your data\n\nEDA is NOT a replacement for statistical inference and learning\nEDA is an important and necessary step to build intuition\n\nWe tackle the challenges of EDA with a data science workflow. An example of this according to Hadley Wickham in R for Data Science:\n\n\n\n\n\nAspects of data wrangling:\n\nimport: reading in data (e.g., read_csv())\ntidy: rows = observations, columns = variables (i.e. tabular data)\ntransform: filter observations, create new variables, summarize, etc."
  },
  {
    "objectID": "demos/01-into-tidyverse.html#working-with-penguins",
    "href": "demos/01-into-tidyverse.html#working-with-penguins",
    "title": "Demo 01: Into the tidyverse",
    "section": "Working with penguins",
    "text": "Working with penguins\nIn R, there are many libraries or packages/groups of programs that are not permanently stored in R, so we have to load them when we want to use them. You can load an R package by typing library(package_name). (Sometimes we need to download/install the package first, as described in HW0.)\nThroughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")\n\nImport the penguins dataset by loading the palmerpenguins package using the library function and then access the data with the data() function:\n\nlibrary(palmerpenguins) \ndata(penguins)\n\nView some basic info about the penguins dataset:\n\n# displays same info as c(nrow(penguins), ncol(penguins))\ndim(penguins) \n\n[1] 344   8\n\nclass(penguins)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\ntbl (pronounced tibble) is the tidyverse way of storing tabular data, like a spreadsheet or data.frame\nI assure you that you’ll run into errors as you code in R; in fact, my attitude as a coder is that something is wrong if I never get any errors while working on a project. When you run into an error, your first reaction may be to panic and post a question to Piazza. However, checking help documentation in R can be a great way to figure out what’s going wrong. (For good or bad, I end up having to read help documentation almost every day of my life - because, well, I regularly make mistakes in R.)\nLook at the help documentation for penguins by typing help(penguins) in the Console. What are the names of the variables in this dataset? How many observations are in this dataset?\n\nhelp(penguins)\n\nYou should always look at your data before doing anything: view the first 6 (by default) rows with head()\n\nhead(penguins) # Try just typing penguins into your console, what happens?\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nIs our penguins dataset tidy?\n\nEach row = a single penguin\nEach column = different measurement about the penguins (can print out column names directly with colnames(penguins) or names(penguins))\n\nWe’ll now explore differences among the penguins using the tidyverse."
  },
  {
    "objectID": "demos/01-into-tidyverse.html#let-the-data-wrangling-begin",
    "href": "demos/01-into-tidyverse.html#let-the-data-wrangling-begin",
    "title": "Demo 01: Into the tidyverse",
    "section": "Let the data wrangling begin…",
    "text": "Let the data wrangling begin…\nFirst, load the tidyverse for exploring the data - and do NOT worry about the warning messages that will pop-up! Warning messages will tell you when other packages that are loaded may have functions replaced with the most recent package you’ve loaded. In general though, you should just be concerned when an error message pops up (errors are different than warnings!).\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWe’ll start by summarizing continuous (e.g., bill_length_mm, flipper_length_mm) and categorical (e.g., species, island) variables in different ways.\nWe can compute summary statistics for continuous variables with the summary() function:\n\nsummary(penguins$bill_length_mm)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  32.10   39.23   44.45   43.92   48.50   59.60       2 \n\n\nCompute counts of categorical variables with table() function:\n\ntable(\"island\" = penguins$island) # be careful it ignores NA values!\n\nisland\n   Biscoe     Dream Torgersen \n      168       124        52 \n\n\nHow do we remove the penguins with missing bill_length_mm values? Within the tidyverse, dplyr is a package with functions for data wrangling (because it’s within the tidyverse that means you do NOT have to load it separately with library(dplyr) after using library(tidyverse)!). It’s considered a “grammar of data manipulation”: dplyr functions are verbs, datasets are nouns.\nWe can filter() our dataset to choose observations meeting conditions:\n\nclean_penguins &lt;- filter(penguins, !is.na(bill_length_mm))\n# Use help(is.na) to see what it returns. And then observe \n# that the ! operator means to negate what comes after it.\n# This means !TRUE == FALSE (i.e., opposite of TRUE is equal to FALSE).\nnrow(penguins) - nrow(clean_penguins) # Difference in rows\n\n[1] 2\n\n\nIf we want to only consider a subset of columns in our data, we can select() variables of interest:\n\nsel_penguins &lt;- select(clean_penguins, species, island, bill_length_mm, flipper_length_mm)\nhead(sel_penguins, n = 3)\n\n# A tibble: 3 × 4\n  species island    bill_length_mm flipper_length_mm\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;             &lt;int&gt;\n1 Adelie  Torgersen           39.1               181\n2 Adelie  Torgersen           39.5               186\n3 Adelie  Torgersen           40.3               195\n\n\nWe can arrange() our dataset to sort observations by variables:\n\nbill_penguins &lt;- arrange(sel_penguins, desc(bill_length_mm)) # use desc() for descending order\nhead(bill_penguins, n = 3)\n\n# A tibble: 3 × 4\n  species   island bill_length_mm flipper_length_mm\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;             &lt;int&gt;\n1 Gentoo    Biscoe           59.6               230\n2 Chinstrap Dream            58                 181\n3 Gentoo    Biscoe           55.9               228\n\n\nWe can summarize() our dataset to one row based on functions of variables:\n\nsummarize(bill_penguins, max(bill_length_mm), median(flipper_length_mm))\n\n# A tibble: 1 × 2\n  `max(bill_length_mm)` `median(flipper_length_mm)`\n                  &lt;dbl&gt;                       &lt;dbl&gt;\n1                  59.6                         197\n\n\nWe can mutate() our dataset to create new variables:\n\nnew_penguins &lt;- mutate(bill_penguins, \n                       bill_flipper_ratio = bill_length_mm / flipper_length_mm,\n                       flipper_bill_ratio = flipper_length_mm / bill_length_mm)\nhead(new_penguins, n = 1)\n\n# A tibble: 1 × 6\n  species island bill_length_mm flipper_length_mm bill_flipper_ratio\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;             &lt;int&gt;              &lt;dbl&gt;\n1 Gentoo  Biscoe           59.6               230              0.259\n# ℹ 1 more variable: flipper_bill_ratio &lt;dbl&gt;\n\n\nHow do we perform several of these actions?\n\nhead(arrange(select(mutate(filter(penguins, !is.na(flipper_length_mm)), bill_flipper_ratio = bill_length_mm / flipper_length_mm), species, island, bill_flipper_ratio), desc(bill_flipper_ratio)), n = 1)\n\n# A tibble: 1 × 3\n  species   island bill_flipper_ratio\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Chinstrap Dream               0.320\n\n\nThat’s awfully annoying to do, and also difficult to read…"
  },
  {
    "objectID": "demos/01-into-tidyverse.html#enter-the-pipeline",
    "href": "demos/01-into-tidyverse.html#enter-the-pipeline",
    "title": "Demo 01: Into the tidyverse",
    "section": "Enter the pipeline",
    "text": "Enter the pipeline\nThe |&gt; (pipe) operator is used in the to chain commands together. Note: you can also use the tidyverse pipe %&gt;% (from magrittr), but |&gt; is the built-in pipe that is native to new versions of R without loading the tidyverse.\n|&gt; directs the data analyis pipeline: output of one function pipes into input of the next function\n\npenguins |&gt;\n  filter(!is.na(flipper_length_mm)) |&gt;\n  mutate(bill_flipper_ratio = bill_length_mm / flipper_length_mm) |&gt;\n  select(species, island, bill_flipper_ratio) |&gt;\n  arrange(desc(bill_flipper_ratio)) |&gt;\n  head(n = 5)\n\n# A tibble: 5 × 3\n  species   island bill_flipper_ratio\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Chinstrap Dream               0.320\n2 Chinstrap Dream               0.275\n3 Chinstrap Dream               0.270\n4 Chinstrap Dream               0.270\n5 Chinstrap Dream               0.268"
  },
  {
    "objectID": "demos/01-into-tidyverse.html#more-pipeline-actions",
    "href": "demos/01-into-tidyverse.html#more-pipeline-actions",
    "title": "Demo 01: Into the tidyverse",
    "section": "More pipeline actions!",
    "text": "More pipeline actions!\nInstead of head(), we can slice() our dataset to choose the observations based on the position\n\npenguins |&gt;\n  filter(!is.na(flipper_length_mm)) |&gt;\n  mutate(bill_flipper_ratio = bill_length_mm / flipper_length_mm) |&gt;\n  select(species, island, bill_flipper_ratio) |&gt;\n  arrange(desc(bill_flipper_ratio)) |&gt;\n  slice(c(1, 2, 10, 100))\n\n# A tibble: 4 × 3\n  species   island bill_flipper_ratio\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Chinstrap Dream               0.320\n2 Chinstrap Dream               0.275\n3 Chinstrap Dream               0.264\n4 Gentoo    Biscoe              0.227"
  },
  {
    "objectID": "demos/01-into-tidyverse.html#grouped-operations",
    "href": "demos/01-into-tidyverse.html#grouped-operations",
    "title": "Demo 01: Into the tidyverse",
    "section": "Grouped operations",
    "text": "Grouped operations\nWe group_by() to split our dataset into groups based on a variable’s values\n\npenguins |&gt;\n  filter(!is.na(flipper_length_mm)) |&gt;\n  group_by(island) |&gt;\n  summarize(n_penguins = n(), #counts number of rows in group\n            ave_flipper_length = mean(flipper_length_mm), \n            sum_bill_depth = sum(bill_depth_mm),\n            .groups = \"drop\") |&gt; # all levels of grouping dropping\n  arrange(desc(n_penguins)) |&gt;\n  slice(1:5)\n\n# A tibble: 3 × 4\n  island    n_penguins ave_flipper_length sum_bill_depth\n  &lt;fct&gt;          &lt;int&gt;              &lt;dbl&gt;          &lt;dbl&gt;\n1 Biscoe           167               210.          2651.\n2 Dream            124               193.          2275.\n3 Torgersen         51               191.           940.\n\n\n\ngroup_by() is only useful in a pipeline (e.g. with summarize()), and pay attention to its behavior\nspecify the .groups field to decide if observations remain grouped or not after summarizing (you can also use ungroup() for this as well)"
  },
  {
    "objectID": "demos/01-into-tidyverse.html#putting-it-all-together",
    "href": "demos/01-into-tidyverse.html#putting-it-all-together",
    "title": "Demo 01: Into the tidyverse",
    "section": "Putting it all together…",
    "text": "Putting it all together…\nAs your own exercise, create a tidy dataset where each row == an island with the following variables:\n\nnumber of penguins,\nnumber of unique species on the island (see help(unique)),\naverage body_mass_g,\nvariance (see help(var)) of bill_depth_mm\n\nPrior to making those variables, make sure to filter missings and also only consider female penguins. Then arrange the islands in order of the average body_mass_g:\n\n# INSERT YOUR CODE HERE"
  },
  {
    "objectID": "demos/09-trends.html",
    "href": "demos/09-trends.html",
    "title": "Demo 09: Visualizing trends",
    "section": "",
    "text": "In this demo, we’ll first work with a dataset on the number of PhD degrees awarded in the US from TidyTuesday.\n\n# Read in the tidytuesday data\nlibrary(tidyverse)\nphd_field &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv\")\nphd_field\n\n# A tibble: 3,370 × 5\n   broad_field   major_field                                 field   year n_phds\n   &lt;chr&gt;         &lt;chr&gt;                                       &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Life sciences Agricultural sciences and natural resources Agric…  2008    111\n 2 Life sciences Agricultural sciences and natural resources Agric…  2008     28\n 3 Life sciences Agricultural sciences and natural resources Agric…  2008      3\n 4 Life sciences Agricultural sciences and natural resources Agron…  2008     68\n 5 Life sciences Agricultural sciences and natural resources Anima…  2008     41\n 6 Life sciences Agricultural sciences and natural resources Anima…  2008     18\n 7 Life sciences Agricultural sciences and natural resources Anima…  2008     77\n 8 Life sciences Agricultural sciences and natural resources Envir…  2008    182\n 9 Life sciences Agricultural sciences and natural resources Fishi…  2008     52\n10 Life sciences Agricultural sciences and natural resources Food …  2008     96\n# ℹ 3,360 more rows\n\n\nLet’s start by grabbing the rows corresponding to Statistics PhDs. While there are a number of ways to do this, we can grab field containing “statistics” (including biostatistics) with the str_detect() function.\n\nstats_phds &lt;- phd_field |&gt;\n  filter(str_detect(tolower(field), \"statistics\"))\n\nWhat are the different fields that were captured?\n\ntable(stats_phds$field)\n\n\n                       Biometrics and biostatistics \n                                                 10 \n           Educational statistics, research methods \n                                                 10 \nManagement information systems, business statistics \n                                                 10 \n                Mathematics and statistics, general \n                                                 10 \n                  Mathematics and statistics, other \n                                                 10 \n                           Statistics (mathematics) \n                                                 10 \n                       Statistics (social sciences) \n                                                 10 \n\n\nTo start, let’s just summarize the number of PhDs by year:\n\nstat_phd_year_summary &lt;- stats_phds |&gt;\n  group_by(year) |&gt;\n  summarize(n_phds = sum(n_phds))\n\nNow, we’ll make the typical scatterplot display with n_phds on the y-axis and year on the x-axis:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nWe should fix our x-axis here and make the breaks more informative. In this case, I’ll change it so each year is labeled (that may not be appropriate for every visual but it works out here).\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  # Modify the x-axis to make the axis breaks at the unique years and show their\n  # respective labels\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nTo emphasize the ordering of the year along the x-axis, we’ll add a line connecting the points to emphasize the order:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nWe can drop the points, leaving only the connecting lines to emphasize trends:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nAnother common way to display trends is by filling in the area under the line. However, this is only appropriate when the y-axis starts at 0! It’s also redundant use of ink so just be careful when deciding whether or not to fill the area. We can fill the area under the line with the geom_area() aesthetic - but note that it changes the y-axis by default to start at 0:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  # Fill the area under the line\n  geom_area(fill = \"darkblue\", alpha = 0.5) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nYou can also make this plot using the ggridges package."
  },
  {
    "objectID": "demos/09-trends.html#phds-awarded-by-field-over-time",
    "href": "demos/09-trends.html#phds-awarded-by-field-over-time",
    "title": "Demo 09: Visualizing trends",
    "section": "",
    "text": "In this demo, we’ll first work with a dataset on the number of PhD degrees awarded in the US from TidyTuesday.\n\n# Read in the tidytuesday data\nlibrary(tidyverse)\nphd_field &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv\")\nphd_field\n\n# A tibble: 3,370 × 5\n   broad_field   major_field                                 field   year n_phds\n   &lt;chr&gt;         &lt;chr&gt;                                       &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Life sciences Agricultural sciences and natural resources Agric…  2008    111\n 2 Life sciences Agricultural sciences and natural resources Agric…  2008     28\n 3 Life sciences Agricultural sciences and natural resources Agric…  2008      3\n 4 Life sciences Agricultural sciences and natural resources Agron…  2008     68\n 5 Life sciences Agricultural sciences and natural resources Anima…  2008     41\n 6 Life sciences Agricultural sciences and natural resources Anima…  2008     18\n 7 Life sciences Agricultural sciences and natural resources Anima…  2008     77\n 8 Life sciences Agricultural sciences and natural resources Envir…  2008    182\n 9 Life sciences Agricultural sciences and natural resources Fishi…  2008     52\n10 Life sciences Agricultural sciences and natural resources Food …  2008     96\n# ℹ 3,360 more rows\n\n\nLet’s start by grabbing the rows corresponding to Statistics PhDs. While there are a number of ways to do this, we can grab field containing “statistics” (including biostatistics) with the str_detect() function.\n\nstats_phds &lt;- phd_field |&gt;\n  filter(str_detect(tolower(field), \"statistics\"))\n\nWhat are the different fields that were captured?\n\ntable(stats_phds$field)\n\n\n                       Biometrics and biostatistics \n                                                 10 \n           Educational statistics, research methods \n                                                 10 \nManagement information systems, business statistics \n                                                 10 \n                Mathematics and statistics, general \n                                                 10 \n                  Mathematics and statistics, other \n                                                 10 \n                           Statistics (mathematics) \n                                                 10 \n                       Statistics (social sciences) \n                                                 10 \n\n\nTo start, let’s just summarize the number of PhDs by year:\n\nstat_phd_year_summary &lt;- stats_phds |&gt;\n  group_by(year) |&gt;\n  summarize(n_phds = sum(n_phds))\n\nNow, we’ll make the typical scatterplot display with n_phds on the y-axis and year on the x-axis:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nWe should fix our x-axis here and make the breaks more informative. In this case, I’ll change it so each year is labeled (that may not be appropriate for every visual but it works out here).\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  # Modify the x-axis to make the axis breaks at the unique years and show their\n  # respective labels\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nTo emphasize the ordering of the year along the x-axis, we’ll add a line connecting the points to emphasize the order:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nWe can drop the points, leaving only the connecting lines to emphasize trends:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nAnother common way to display trends is by filling in the area under the line. However, this is only appropriate when the y-axis starts at 0! It’s also redundant use of ink so just be careful when deciding whether or not to fill the area. We can fill the area under the line with the geom_area() aesthetic - but note that it changes the y-axis by default to start at 0:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  # Fill the area under the line\n  geom_area(fill = \"darkblue\", alpha = 0.5) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nYou can also make this plot using the ggridges package."
  },
  {
    "objectID": "demos/09-trends.html#plotting-and-labeling-several-lines",
    "href": "demos/09-trends.html#plotting-and-labeling-several-lines",
    "title": "Demo 09: Visualizing trends",
    "section": "Plotting and labeling several lines",
    "text": "Plotting and labeling several lines\nWe’ll now switch to displaying the different Statistics fields separately with the stats_phds dataset. First, we should NOT display multiple time series with just points as follows:\n\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_point() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  theme(legend.position = \"bottom\",\n        # Adjust the size of the legend's text\n        legend.text = element_text(size = 5),\n        legend.title = element_text(size = 6)) +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\",\n       color = \"Field\")\n\n\n\n\n\n\n\n\nIt’s much simpler to just display the lines to compare the trends:\n\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  theme(legend.position = \"bottom\",\n        # Adjust the size of the legend's text\n        legend.text = element_text(size = 5),\n        legend.title = element_text(size = 6)) +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\",\n       color = \"Field\")\n\n\n\n\n\n\n\n\nThe legend is pretty cluttered though, instead we can directly label the displayed lines using the ggrepel package. We first need to create a dataset with just the final values (which in this case corresponds to year == 2017), and then add labels for these values. To make the labels visible, we need to increase our x-axis limits. Note that this is a “hack”, but you will rely on hacks to customize visuals in the future… The following code chunk demonstrates how to do this:\n\nstats_phds_2017 &lt;- stats_phds |&gt;\n  filter(year == 2017)\n\n# Access the ggrepel package:\n# install.packages(\"ggrepel\")\nlibrary(ggrepel)\n\nWarning: package 'ggrepel' was built under R version 4.2.3\n\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  # Add the labels:\n  geom_text_repel(data = stats_phds_2017,\n                  aes(label = field),\n                  size = 2, \n                  # Drop the segment connection:\n                  segment.color = NA, \n                  # Move labels up or down based on overlap\n                  direction = \"y\",\n                  # Try to align the labels horizontally on the left hand side\n                  hjust = \"left\") +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year),\n                     # Update the limits so that there is some padding on the\n                     # x-axis but don't label the new maximum\n                     limits = c(min(stat_phd_year_summary$year),\n                                max(stat_phd_year_summary$year) + 3)) +\n  theme_bw() +\n  # Drop the legend\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\",\n       color = \"Field\")\n\n\n\n\n\n\n\n\nAn alternative approach is to use the gghighlight package:\n\nlibrary(gghighlight)\n\nWarning: package 'gghighlight' was built under R version 4.2.3\n\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  gghighlight()  +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\nlabel_key: field\n\n\n\n\n\n\n\n\n\nNext, let’s switch to back to the original dataset phd_field. What happens if we plot a line for every field attempting to use the color aesthetic to separate them?\n\nphd_field |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\",\n       color = \"Field\")\n\nWarning: Removed 270 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThe plot above is obviously a disaster… When we are dealing with potentially way too many categories, we can instead highlight lines of interest while setting the background lines to gray, so we can still see background trends. We need to use the group aesthetic to split the gray lines from each other. Plus, we should adjust the alpha due to the overlap. The following code chunk demonstrates how to do this for highlighting the “Statistics (mathematics)” and “Biometrics and biostatistics” lines. We essentially create separate plot layers by filtering on the field variable:\n\n# First display the background lines using the full dataset with those two fields \n# filtered out:\nphd_field |&gt;\n  # The following line says: NOT (field in c(\"Biometrics and biostatistics\", \"Statistics (mathematics)\"))\n  filter(!(field %in% c(\"Biometrics and biostatistics\", \n                        \"Statistics (mathematics)\"))) |&gt;\n  ggplot() +\n  # Add the background lines - need to specify the group to be the field\n  geom_line(aes(x = year, y = n_phds, group = field),\n            color = \"gray\", size = .5, alpha = .5) +\n  # Now add the layer with the lines of interest:\n  geom_line(data = filter(phd_field,\n                          # Note this is just the opposite of the above since ! is removed\n                          field %in% c(\"Biometrics and biostatistics\", \n                                       \"Statistics (mathematics)\")),\n            aes(x = year, y = n_phds, color = field),\n            # Make the size larger\n            size = .75, alpha = 1) +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  theme(legend.position = \"bottom\", \n        # Drop the panel lines making the gray difficult to see\n        panel.grid = element_blank()) +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\",\n       color = \"Field\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 270 rows containing missing values or values outside the scale range\n(`geom_line()`)."
  },
  {
    "objectID": "demos/09-trends.html#florence-nightingales-rose-diagrams",
    "href": "demos/09-trends.html#florence-nightingales-rose-diagrams",
    "title": "Demo 09: Visualizing trends",
    "section": "Florence Nightingale’s rose diagrams",
    "text": "Florence Nightingale’s rose diagrams\nAnother way to visualize time series data is to display it in a cycle pattern, using polar coordinates, as done by Florence Nightingale’s famous rose diagram. We can recreate the rose diagram by accessing the data in the HistData package. We’ll first load and print out the first so many rows of the data below:\n\nlibrary(HistData)\nhead(Nightingale)\n\n        Date Month Year  Army Disease Wounds Other Disease.rate Wounds.rate\n1 1854-04-01   Apr 1854  8571       1      0     5          1.4         0.0\n2 1854-05-01   May 1854 23333      12      0     9          6.2         0.0\n3 1854-06-01   Jun 1854 28333      11      0     6          4.7         0.0\n4 1854-07-01   Jul 1854 28722     359      0    23        150.0         0.0\n5 1854-08-01   Aug 1854 30246     828      1    30        328.5         0.4\n6 1854-09-01   Sep 1854 30290     788     81    70        312.2        32.1\n  Other.rate\n1        7.0\n2        4.6\n3        2.5\n4        9.6\n5       11.9\n6       27.7\n\n\nTo recreate the plot, we’ll need to first make a longer version of the dataset with the Disease, Wounds, and Other columns separated into three rows. To do that, we’ll use the pivot_longer() function after just selecting the columns of interest for our plot:\n\ncrimean_war_data &lt;- Nightingale |&gt;\n  dplyr::select(Date, Month, Year, Disease, Wounds, Other) |&gt;\n  # Now pivot those columns to take up separate rows:\n  pivot_longer(Disease:Other,\n               names_to = \"cause\", values_to = \"count\")\n\nNext, we’ll make a label column matching Nightingale’s plot based on the Date column. We’ll talk about dates more below, but we can condition on being above or below certain dates in a natural way:\n\ncrimean_war_data &lt;- crimean_war_data |&gt;\n  mutate(time_period = ifelse(Date &lt;= as.Date(\"1855-03-01\"),\n                              \"April 1854 to March 1855\", \n                              \"April 1855 to March 1856\"))\n\nAnd finally we can go ahead and display the rose diagram facetted by the time period (using similar colors to Nightingale):\n\ncrimean_war_data |&gt; \n  ggplot(aes(x = Month, y = count)) + \n  geom_col(aes(fill = cause), width = 1, \n           position = \"identity\", alpha = 0.5) + \n  coord_polar() + \n  facet_wrap(~ time_period, ncol = 2) +\n  scale_fill_manual(values = c(\"skyblue3\", \"grey30\", \"firebrick\")) +\n  scale_y_sqrt() +\n  theme_void() +\n  # All of this below is to just customize the theme in a way that we are\n  # close to resembling the original plot (ie lets make it look old!)\n  theme(axis.text.x = element_text(size = 9),\n        strip.text = element_text(size = 11),\n        legend.position = \"bottom\",\n        plot.background = element_rect(fill = alpha(\"cornsilk\", 0.5)),\n        plot.margin = unit(c(10, 10, 10, 10), \"pt\"),\n        plot.title = element_text(vjust = 5)) +\n  labs(title = \"Diagram of the Causes of Mortality in the Army in the East\")\n\n\n\n\n\n\n\n\nThis looks pretty close to the original diagram, except the order of the months does not match the original. We can of course change that by reordering the factor variable:\n\ncrimean_war_data |&gt; \n  # Manually relevel it to match the original plot\n  mutate(Month = fct_relevel(Month, \n                             \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\",\n                             \"Dec\", \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\")) |&gt;\n  ggplot(aes(x = Month, y = count)) + \n  geom_col(aes(fill = cause), width = 1, \n           position = \"identity\", alpha = 0.5) + \n  coord_polar() + \n  facet_wrap(~ time_period, ncol = 2) +\n  scale_fill_manual(values = c(\"skyblue3\", \"grey30\", \"firebrick\")) +\n  scale_y_sqrt() +\n  theme_void() +\n  # All of this below is to just customize the theme in a way that we are\n  # close to resembling the original plot (ie lets make it look old!)\n  theme(axis.text.x = element_text(size = 9),\n        strip.text = element_text(size = 11),\n        legend.position = \"bottom\",\n        plot.background = element_rect(fill = alpha(\"cornsilk\", 0.5)),\n        plot.margin = unit(c(10, 10, 10, 10), \"pt\"),\n        plot.title = element_text(vjust = 5)) +\n  labs(title = \"Diagram of the Causes of Mortality in the Army in the East\")\n\n\n\n\n\n\n\n\nHow does this compare to just a simple line graph?\n\ncrimean_war_data |&gt; \n  ggplot(aes(x = Date, y = count, color = cause)) + \n  geom_line() +\n  # Add a reference line at the cutoff point\n  geom_vline(xintercept = as.Date(\"1855-03-01\"), linetype = \"dashed\",\n             color = \"gray\") +\n  scale_color_manual(values = c(\"skyblue3\", \"grey30\", \"firebrick\")) +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Diagram of the Causes of Mortality in the Army in the East\",\n       y = \"Counts\", x = \"Date\")\n\n\n\n\n\n\n\n\nWe can customize the x-axis further using scale_x_date():\n\ncrimean_war_data |&gt; \n  ggplot(aes(x = Date, y = count, color = cause)) + \n  geom_line() +\n  # Add a reference line at the cutoff point\n  geom_vline(xintercept = as.Date(\"1855-03-01\"), linetype = \"dashed\",\n             color = \"gray\") +\n  scale_color_manual(values = c(\"skyblue3\", \"grey30\", \"firebrick\")) +\n  # Format to use abbreviate month %b with year %Y\n  scale_x_date(date_labels = \"%b %Y\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Diagram of the Causes of Mortality in the Army in the East\",\n       y = \"Counts\", x = \"Date\")\n\n\n\n\n\n\n\n\nWhich one do you prefer? Maybe filling the area under the lines would be better here…"
  },
  {
    "objectID": "demos/07-clustering.html",
    "href": "demos/07-clustering.html",
    "title": "Demo 07: More MDS and Creating Dendrograms",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs."
  },
  {
    "objectID": "demos/07-clustering.html#mds-meets-mcu",
    "href": "demos/07-clustering.html#mds-meets-mcu",
    "title": "Demo 07: More MDS and Creating Dendrograms",
    "section": "MDS meets MCU",
    "text": "MDS meets MCU\nWe will work with a dataset about the performance of MCU movies. The dataset was accessed from here with information such as the box office performance and reviews.\nHere is the code to read in the data into R:\n\nlibrary(tidyverse)\nmcu_movies &lt;- read_csv(\"https://raw.githubusercontent.com/ryurko/DataViz-Class-Data/main/mcu_movies.csv\")\n\nFor this demo, we’ll focus just on the columns that contain quantitative variables about the movies’ performance, but we’ll also exclude the year variable:\n\n# Select only the continuous variables excluding the year\nmcu_quant &lt;- mcu_movies |&gt; \n  dplyr::select(-c(film, category, year))\n\nThere are 15 measurements about each movie:\n\nhead(mcu_quant)\n\n# A tibble: 6 × 15\n  worldwide_gross_m percent_budget_recovered critics_percent_score\n              &lt;dbl&gt;                    &lt;dbl&gt;                 &lt;dbl&gt;\n1               518                      398                    83\n2               623                      479                    87\n3              1395                      382                    76\n4              2797                      699                    94\n5              2048                      683                    85\n6              1336                      668                    96\n# ℹ 12 more variables: audience_percent_score &lt;dbl&gt;,\n#   audience_vs_critics_percent_deviance &lt;dbl&gt;, budget &lt;dbl&gt;,\n#   domestic_gross_m &lt;dbl&gt;, international_gross_m &lt;dbl&gt;,\n#   opening_weekend_m &lt;dbl&gt;, second_weekend_m &lt;dbl&gt;,\n#   x1st_vs_2nd_weekend_drop_off &lt;dbl&gt;,\n#   percent_gross_from_opening_weekend &lt;dbl&gt;,\n#   percent_gross_from_domestic &lt;dbl&gt;, …\n\n\nWe’re interested in the questions: which of these movies are most similar, and which are most different?\nWe’re going to follow our workflow from the previous demo and first compute the distance matrix for the movies (based on Euclidean distance), followed by performing multi-dimensional scaling (MDS) to see which movies are “close” and which are “far apart”.\nFirst, scale the data and then compute the distance matrix:\n\n# This is just one way to scale the data - without centering the columns\nmcu_quant &lt;- apply(mcu_quant, MARGIN = 2,\n                   FUN = function(x) x / sd(x))\nmcu_dist &lt;- dist(mcu_quant)\n\nStaring at a distance matrix and trying to find the most/least similar pairs of observations is not practical for large datasets. This is why an approach like MDS can be really useful: We can quickly see which movies are “close” and which are “far apart” by plotting the first two coordinates (using k = 2). In the code chunk below, we run MDS to get the two new coordinates, then add these coordinates as columns to the original dataset for plotting purposes:\n\n# Run MDS\nmcu_mds &lt;- cmdscale(d = dist(mcu_quant), k = 2)\n\n# Add to original dataset\nmcu_movies &lt;- mcu_movies |&gt;\n  mutate(mds1 = mcu_mds[,1], \n         mds2 = mcu_mds[,2])\n\n# Create plot:\nmcu_movies |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  geom_point(alpha = .5) +\n  labs(x = \"MDS Coordinate 1\", y = \"MDS Coordinate 2\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nSince this dataset is relatively small, I can instead plot the film titles directly using geom_text() where I just need to map the film variable to the label aesthetic of the plot:\n\nmcu_movies |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  # Use text labels instead of points:\n  geom_text(aes(label = film),\n            alpha = .75) +\n  labs(x = \"MDS Coordinate 1\", y = \"MDS Coordinate 2\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nNow I can see where the movies fall in this projection. If you know anything about the MCU, you can see that some of the biggest movies are along the right-hand side: Black Panther, The Avengers, Spider-Man: No Way Home, Avengers: Infinity War, and Avengers Endgame. We see the various other movies throughout, including the definitive worst MCU movie on the bottom-left corner, as well as a compact group of several movies together."
  },
  {
    "objectID": "demos/07-clustering.html#visualizing-distance-structure-with-hierarchical-clustering-and-dendrograms",
    "href": "demos/07-clustering.html#visualizing-distance-structure-with-hierarchical-clustering-and-dendrograms",
    "title": "Demo 07: More MDS and Creating Dendrograms",
    "section": "Visualizing distance structure with hierarchical clustering and dendrograms",
    "text": "Visualizing distance structure with hierarchical clustering and dendrograms\nWhile this MDS plot is useful for visualizing how the movies relate to each other. However, it can be difficult to imagine how we could use the MDS plot to identify clusters of movies (unless the points themselves were already clearly clustered, which they are not in the above plot). This is where dendrograms can be a great visual tool for understanding the clustering of observations in your dataset.\nDendrograms are tree-like structures used for visualizing distances. Dendrograms have the following axes:\n\ny-axis: distance (or more generally speaking: dissimilarity) at which a pair of observations are linked\nx-axis: rough grouping of observations (the exact ordering is not necessarily meaningful, other than the fact that pairs of observations near each other are being assigned to the same cluster)\n\nBelow, we implement hierarchical clustering with complete linkage and single linkage (complete linkage is more commonly used), and then plot the results on a dendrogram. We use the hclust function in R to perform hierarchical clustering (the default method is complete linkage) given a distance matrix (or in more general terms, a dissimilarity matrix).\nI’ll first start with complete linkage:\n\nhc_complete &lt;- hclust(mcu_dist, method = \"complete\")\n\nplot(hc_complete, ylab = \"Pairwise Distance\", \n     main = \"Complete Linkage\", xlab = \"MCU Movies\")\n\n\n\n\n\n\n\n\nFrom looking at this dendrogram, we can broadly see a group of movies on the right-hand side that are separate from most of the other movies. However, the movies are labeled by their row numbers which is useless for us! We can update the leaf labels by modifying the labels input for the hclust plot object. For example, I can instead label the movies with the actual film titles:\n\nplot(hc_complete, ylab = \"Pairwise Distance\", \n     labels = mcu_movies$film,\n     main = \"Complete Linkage\", xlab = \"MCU Movies\")\n\n\n\n\n\n\n\n\nThis is much more useful! I can now see that the group of movies on the right-hand side correspond to the biggest one I previously mentioned, as well as Captain Marvel. From this figure, we can now see which pairs of movies were most similar such as Ant-Man and its sequel, along with Thor and its first sequel (both of which are not great…).\nNOTE: This is a small enough dataset that labeling the leaves is worthwhile, but for larger datasets the labels can become problematic and unreadable. Instead, you can also turn off the leaf labels by just setting label to FALSE:\n\nplot(hc_complete, ylab = \"Pairwise Distance\", \n     labels = FALSE,\n     main = \"Complete Linkage\", xlab = \"MCU Movies\")\n\n\n\n\n\n\n\n\nWhile the above was with complete linkage, the following demonstrates the results with single linkage:\n\nhc_single &lt;- hclust(mcu_dist, method = \"single\")\nplot(hc_single, ylab = \"Pairwise Distance\",\n     labels = mcu_movies$film,\n     main = \"Single Linkage\", xlab = \"MCU Movies\")\n\n\n\n\n\n\n\n\nIn this case, we can clearly see a very different looking dendrogram driven by the difference in how we compute distances between clusters. Single linkage results in this chaining effect: where poorly separate but distinct clusters are merged together.\nAlternatively, because base R plotting is pretty annoying at times, we can instead use the ggdendro package to create our dendrogram. This offers more customization and even allows you to extract a dataset constructed by the dendrogram for use - but we won’t focus on that for this demo. Instead the following code uses the ggdendro package to create two dendrograms and then plots them side-by-side using the patchwork package (which I have previously used in various solutions):\n\n# You'll need to run the following lines if you do NOT have ggdendro and patchwork \n# packages installed already:\n# install.packages(\"ggdendro\")\n# install.packages(\"patchwork\")\nlibrary(ggdendro)\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.2.3\n\nhc_single_ggdendro &lt;- ggdendrogram(hc_single, theme_dendro = FALSE) +\n  labs(y = \"Pairwise Distance\", title = \"Single Linkage\") + \n  theme_bw() +\n  # Remove the x-axis title\n  theme(axis.title.x = element_blank())\n\nhc_complete_ggdendro &lt;- ggdendrogram(hc_complete, theme_dendro = FALSE) +\n  labs(y = \"Pairwise Distance\", title = \"Complete Linkage\") + \n  theme_bw() +\n  # Remove the x-axis title\n  theme(axis.title.x = element_blank())\n\nhc_single_ggdendro + hc_complete_ggdendro\n\n\n\n\n\n\n\n\nIf we want to add the movie titles to the ggdendro version of the dendrogram, we unfortunately we need to go back to the beginning of this process! We need to add rownames to our initial dataset that we used for computing the distance matrix. These names will then automatically carry over and serve as the observation labels in the dendrograms above:\n\n# Add the film titles as the row names for mcu_quant\nrownames(mcu_quant) &lt;- mcu_movies$film\n# Recompute the distance matrix\nmcu_dist &lt;- dist(mcu_quant)\n\n# And repeat the dendrogram process\nhc_complete &lt;- hclust(mcu_dist, method = \"complete\")\nggdendrogram(hc_complete, theme_dendro = FALSE) +\n  labs(y = \"Pairwise Distance\", title = \"Complete Linkage\") + \n  theme_bw() +\n  # Remove the x-axis title\n  theme(axis.title.x = element_blank())\n\n\n\n\n\n\n\n\nBut now we can’t read any of the labels! Conveniently, since ggdendro returns a ggplot object, we can just use coord_flip() to make this easier to read:\n\nhc_complete_ggdendro &lt;- ggdendrogram(hc_complete, theme_dendro = FALSE) +\n  labs(y = \"Cluster Dissimilarity (based on complete linkage)\", \n       title = \"Which MCU movies are similar to each other?\") + \n  coord_flip() +\n  theme_bw() +\n  # Remove the y-axis title (changed from x to y since we flipped it!)\n  theme(axis.title.y = element_blank())\n\n# Display this:\nhc_complete_ggdendro\n\n\n\n\n\n\n\n\nIt can also be helpful to put an MDS plot and a dendrogram plot side-by-side (again using patchwork). The MDS plot gives us a better idea of what the dendrogram is doing behind the scenes (think about why).\n\nmcu_mds_plot &lt;- mcu_movies |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  geom_text(aes(label = film),\n            alpha = .75) +\n  labs(x = \"MDS Coordinate 1\", y = \"MDS Coordinate 2\") +\n  theme_bw()\n\nmcu_mds_plot + hc_complete_ggdendro\n\n\n\n\n\n\n\n\nUsing our dendrogram, we can assign MCU movies to clusters by cutting the tree via the cutree function. In terms of code, there are two ways to do this: (1) we pick the height of the tree to cut at or (2) we tell it how many clusters we want and it finds the corresponding height to use.\nLet’s start with cutting based on the height. From looking at the dendrogram above, I decide to break the movies into clusters based on a threshold of 10, i.e., if the complete linkage distance is \\(\\leq 10\\) then the clusters are merged together while anything above that threshold is in a different cluster.\nIf I call this cutree function at height h = 10 , you’ll see how it returns arbitrary labels to each movie:\n\nmcu_clusters &lt;- cutree(hc_complete, h = 10)\nmcu_clusters\n\n                          Ant-Man                Ant-Man & The Wasp \n                                1                                 1 \n          Avengers: Age of Ultron                Avengers: End Game \n                                1                                 2 \n           Avengers: Infinity War                     Black Panther \n                                2                                 2 \n                  Black Panther 2                       Black Widow \n                                1                                 1 \n                  Captain America        Captain America: Civil War \n                                1                                 1 \n  Captain America: Winter Soldier                    Captain Marvel \n                                1                                 2 \n                       Dr Strange Dr Strange: Multiverse of Madness \n                                1                                 1 \n                         Eternals           Guardians of the Galaxy \n                                1                                 1 \n        Guardians of the Galaxy 2                   Incredible Hulk \n                                1                                 1 \n                         Iron Man                        Iron Man 2 \n                                1                                 1 \n                       Iron Man 3                         Shang-Chi \n                                1                                 1 \n        Spider-Man: Far from Home            Spider-Man: Homecoming \n                                1                                 1 \n          Spider-Man: No Way Home                      The Avengers \n                                2                                 2 \n                 Thor: Dark World              Thor: Love & Thunder \n                                1                                 1 \n                   Thor: Ragnarok                              Thor \n                                1                                 1 \n\n\nThis returns a vector of cluster assignments for every movie (where the names of the vector are the film titles). NOTE: The numbers are arbitrary labels without meaninful order! You could replace all the 1s with Zs and all of the 2s with As, and the meaning would be the same. All that matters is which observations have the same labels. I can now add these labels to be another column for my plot above to display what the clusters are via color while denoting the height at which I cut the dendrogram:\n\ncluster_mcu_mds_plot &lt;- mcu_movies |&gt;\n  mutate(cluster = as.factor(mcu_clusters)) |&gt;\n  ggplot(aes(x = mds1, y = mds2,\n             color = cluster)) +\n  geom_text(aes(label = film),\n            alpha = .75) +\n  labs(x = \"MDS Coordinate 1\", y = \"MDS Coordinate 2\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n# Update dendogram with cut:\ncut_dendro &lt;- hc_complete_ggdendro +\n  # This is a horizontal line since its considered before the flip:\n  geom_hline(yintercept = 10, linetype = \"dashed\", \n             color = \"darkred\")\n\ncluster_mcu_mds_plot + cut_dendro"
  },
  {
    "objectID": "demos/07-clustering.html#other-dendrogram-visualization-tools",
    "href": "demos/07-clustering.html#other-dendrogram-visualization-tools",
    "title": "Demo 07: More MDS and Creating Dendrograms",
    "section": "Other dendrogram visualization tools",
    "text": "Other dendrogram visualization tools\n\ndendextend\nWe’ll learn how to make prettier versions of this in a later lab. In particular, we’ll use the dendextend package (which you’ll have to install). This allows you to prespecify \\(k\\) the clusters displayed via color that you want to add to your dendrogram:\n\n# Install dendextend if you do not have it already!\n# install.packages(\"dendextend\")\nlibrary(dendextend)\nhc_dendrogram &lt;- as.dendrogram(hc_complete)\nhc_dendrogram &lt;- set(hc_dendrogram,\n                     \"branches_k_color\", k = 4)\nplot(hc_dendrogram, ylab = \"Pairwise Distance\")\n\n\n\n\n\n\n\n\nCheck this page out for a simple tutorial of customizing dendrograms in R using ggdendro and dendextend.\n\n\nfactoextra\nAnother package with dendrogram visualizations, that we’ll use later on for PCA, is the factoextra package. This package contains many extremely useful functions for creating visualizations without too many steps. We can use the fviz_dend() function to display the dendrogram for an hclust object:\n\n# Install factoextra if you do not have it already!\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nfviz_dend(hc_complete)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\nℹ The deprecated feature was likely used in the factoextra package.\n  Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\n\n\n\n\n\n\n\n\nWe can customize this in pretty easy ways, such as changing the font size via the cex argument and displaying clusters via color:\n\nfviz_dend(hc_complete, cex = 0.25, k = 3, color_labels_by_k = TRUE) \n\n\n\n\n\n\n\n\nCheck out the function documentation for more customization options: https://rpkgs.datanovia.com/factoextra/reference/fviz_dend.html"
  },
  {
    "objectID": "demos/04-nonlinear-pairs.html",
    "href": "demos/04-nonlinear-pairs.html",
    "title": "Demo 04: Nonlinear Regression and Pairs Plots",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs. Throughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")"
  },
  {
    "objectID": "demos/04-nonlinear-pairs.html#adjusting-the-span",
    "href": "demos/04-nonlinear-pairs.html#adjusting-the-span",
    "title": "Demo 04: Nonlinear Regression and Pairs Plots",
    "section": "Adjusting the span",
    "text": "Adjusting the span\nWhen using method = \"loess\", we can control the proportion of observations that are used when estimating the local regression (i.e., the size of the neighborhood around the observation of interest) with the span argument. For span &lt; 1, then the “neighborhood” includes proportion span of all possible points. By default, method = \"loess\" using the tri-cubic weighting, such that the weight is proportional to (1 - (dist / maxdist)^3)^3 (where maxdist refers to the maximum distance from the observations in the considered neighborhood). The default setting is span = 0.75, meaning that 75% of the dataset’s observations are used when fitting the local linear regression with weights. We can change span directly in geom_smooth():\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(span = .2)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nUpdate to use all observations instead:\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(span = 1)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/13-areal-data.html",
    "href": "demos/13-areal-data.html",
    "title": "Demo 13: Visualizations for areal data",
    "section": "",
    "text": "What is a polygon? I know this is getting back to elementary school stuff, but in short, a polygon is a shape consisting of a finite number of edges to form an enclosed space. Geographic borders making up regions like countries, states, counties, etc., can be envisioned as very complex polygons.\n\n\nIn ggplot(), polygons are just another geometry, making it really easy to add geographic shapes (e.g. corresponding to countries, states, counties, etc.) to maps. The following code makes a county-level map of the US by utilizing the geom_polygon() function. This function just needs a set of latitude and longitude coordinates and the group that each of these coordinates belongs to. Each group corresponds to a polygon, and the individual latitude and longitude coordinates (many of which will belong to the same group) can be “connected together” to make a polygon. For example, for the state of Michigan, there are two groups, because it consists of two different polygons.\n\nlibrary(tidyverse)\nlibrary(ggmap)\nus_data &lt;- map_data(\"state\")\ncounty_data &lt;- map_data(\"county\")\n\n#For reference, this is what us_data looks like:\nhead(us_data)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\nus_county_map &lt;- ggplot() + \n  #this creates all of the counties\n  geom_polygon(aes(long, lat, group = group), fill = \"darkblue\", size = 4, \n               data = county_data) + \n  #this draws outlines for the states\n  geom_polygon(aes(long, lat, group = group), color = 'white', \n               fill = NA, data = us_data) + \n  theme_bw() + theme(axis.text = element_blank(), \n                     axis.title = element_blank())\nus_county_map\n\n\n\n\n\n\n\n\nIn what follows, we’ll demonstrate how you can plot state-specific data (i.e., areal data, where each subject is a state). The workflow generalizes to other regions (e.g., maybe you want to plot country-specific data, county-specific data, etc.)\nThe workflow will be as follows:\n\nGet state boundaries (i.e., the latitude and longitude information at the state level).\nGet state-specific data. For example, we will use the state.x77 dataset in the datasets package, which contains information about each of the 50 United States in the 1970s. (Despite its name, none of the data is from 1977. See help(state.x77) for more details.)\nMatch the data from the the second bulletpoint to the data from the first bulletpoint (i.e., connect the state-specific data with the state boundary data).\nPlot the data.\n\nFirst, let’s get the state boundaries. (We actually did this earlier when making our US map above, but we’ll do it again here for demonstration.)\n\n#  Get state borders from ggmap package and maps\nlibrary(maps)\n\nWarning: package 'maps' was built under R version 4.2.3\n\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n#?map_data \nstate_borders &lt;- map_data(\"state\") \nhead(state_borders)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\nNow we’ll load the state-specific dataset, state.x77:\n\nlibrary(datasets)\n#Note that the state.x77 dataset looks like this:\nhead(state.x77)\n\n           Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAlabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska            365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia      21198   5114        1.1    71.71   10.3    62.6    20 156361\nColorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\n\n\nWe’ll need to do a minor amount of data manipulation to match the state.x77 dataset to the state_borders dataset. First, note that state.x77 technically doesn’t have a column with the state names; instead, its rownames correspond to the state names. So, first we’ll have to create a column with the state names. Second, the names within state_borders are all lowercase (see above), so we’ll need to take that into account with matching the two datasets as well.\nThe following code first grabs the rownames of the state.x77 table, then converts state.x77 to a tibble named state_data, and then adds that column with the state names and makes them lower case:\n\nstate_names &lt;- rownames(state.x77)\nstate_data &lt;- as_tibble(state.x77)\nstate_data &lt;- state_data |&gt;\n  mutate(state = state_names) |&gt;\n  mutate(state = tolower(state))\n\nNow we can match our two datasets using left_join():\n\n#  join state_data data to state_borders\nstate_borders &lt;- state_borders |&gt;\n  left_join(state_data, by = c(\"region\" = \"state\"))\n#Note that we now have the information from state_data inside\n#our state_borders dataset:\nhead(state_borders)\n\n       long      lat group order  region subregion Population Income Illiteracy\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;       3615   3624        2.1\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;       3615   3624        2.1\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;       3615   3624        2.1\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;       3615   3624        2.1\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;       3615   3624        2.1\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;       3615   3624        2.1\n  Life Exp Murder HS Grad Frost  Area\n1    69.05   15.1    41.3    20 50708\n2    69.05   15.1    41.3    20 50708\n3    69.05   15.1    41.3    20 50708\n4    69.05   15.1    41.3    20 50708\n5    69.05   15.1    41.3    20 50708\n6    69.05   15.1    41.3    20 50708\n\n\nFinally, we can make a plot of the state-specific data (in this case we focus on the illteracy rate by state in 1970). To do this, we just specify a fill within the geom_polygon() function. It’s also helpful to use the scale_fill_gradient2() function to denote what the colors should be for this fill (below, we set the midpoint within this function equal to the median of the variable we are plotting, which is common practice).\nImportant: If you do this (set the midpoint equal to the median), remember that you are forcing half the regions in your map to have “high value” colors and half the regions in your map to have “low value” colors. Furthermore, when looking at maps like the ones below, you should always keep the scale in mind. For example, there seems to be a distinction between the southern and northern US: They tend to be different colors. Ultimately, though, this difference corresponds to a 1-2% difference in illiteracy rates; we would need to (ironically) read up on illiteracy rates to better understand if this is a scientifically meaningful difference.\n\n#  Make the plot!  \n# Before running the following code, you need to have the `mapproj` package\n# install.packages(\"mapproj\")\n#  (Change the fill variable below as you see fit)\n#  (Change the color gradient as you see fit)\nggplot(state_borders) + \n  geom_polygon(aes(x = long, y = lat, group = group,\n                   fill = Illiteracy), color = \"black\") + \n  scale_fill_gradient2(low = \"darkgreen\", mid = \"lightgrey\", \n                       high = \"darkorchid4\", midpoint = 0.95) +\n  theme_void() +\n  coord_map(\"polyconic\") + \n  labs(\n    title = \"Spatial Distribution of Illiteracy Percent by State\",\n    subtitle = \"Percent of Population in State\",\n    caption = \"U.S. Department of Commerce, Bureau of the Census (1977)\",\n    fill = \"Illiteracy %\"\n  ) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nIn the above code, note that we have the line coord_map(\"polyconic\"). This specifies a certain kind of map projection for our plot. We can consider other projections. For example, an old-school projection we discussed in lecture was the Mercator projection. The below map is the same map as above, but with the Mercator projection.\n\nggplot(state_borders) + \n  geom_polygon(aes(x = long, y = lat, group = group,\n                   fill = Illiteracy), color = \"black\") + \n  scale_fill_gradient2(low = \"darkgreen\", mid = \"lightgrey\", \n                       high = \"darkorchid4\", midpoint = 0.95) +\n  theme_void() +\n  coord_map(\"mercator\") + \n  labs(\n    title = \"Spatial Distribution of Illiteracy Percent by State\",\n    subtitle = \"Percent of Population in State\",\n    caption = \"U.S. Department of Commerce, Bureau of the Census (1977)\",\n    fill = \"Illiteracy %\"\n  ) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nNote that the coord_map() function is specifying the coordinates of our map.\nUnfortunately, some popular projections (like the Robinson projection and the Winkel Tripel projection) are not readily available. It’s a little bit annoying to get these projections working in R, but it can be done: See this tutorial. There isn’t one “right” projection, but some are better than others."
  },
  {
    "objectID": "demos/13-areal-data.html#plotting-spatial-objects-with-geom_polygon",
    "href": "demos/13-areal-data.html#plotting-spatial-objects-with-geom_polygon",
    "title": "Demo 13: Visualizations for areal data",
    "section": "",
    "text": "In ggplot(), polygons are just another geometry, making it really easy to add geographic shapes (e.g. corresponding to countries, states, counties, etc.) to maps. The following code makes a county-level map of the US by utilizing the geom_polygon() function. This function just needs a set of latitude and longitude coordinates and the group that each of these coordinates belongs to. Each group corresponds to a polygon, and the individual latitude and longitude coordinates (many of which will belong to the same group) can be “connected together” to make a polygon. For example, for the state of Michigan, there are two groups, because it consists of two different polygons.\n\nlibrary(tidyverse)\nlibrary(ggmap)\nus_data &lt;- map_data(\"state\")\ncounty_data &lt;- map_data(\"county\")\n\n#For reference, this is what us_data looks like:\nhead(us_data)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\nus_county_map &lt;- ggplot() + \n  #this creates all of the counties\n  geom_polygon(aes(long, lat, group = group), fill = \"darkblue\", size = 4, \n               data = county_data) + \n  #this draws outlines for the states\n  geom_polygon(aes(long, lat, group = group), color = 'white', \n               fill = NA, data = us_data) + \n  theme_bw() + theme(axis.text = element_blank(), \n                     axis.title = element_blank())\nus_county_map\n\n\n\n\n\n\n\n\nIn what follows, we’ll demonstrate how you can plot state-specific data (i.e., areal data, where each subject is a state). The workflow generalizes to other regions (e.g., maybe you want to plot country-specific data, county-specific data, etc.)\nThe workflow will be as follows:\n\nGet state boundaries (i.e., the latitude and longitude information at the state level).\nGet state-specific data. For example, we will use the state.x77 dataset in the datasets package, which contains information about each of the 50 United States in the 1970s. (Despite its name, none of the data is from 1977. See help(state.x77) for more details.)\nMatch the data from the the second bulletpoint to the data from the first bulletpoint (i.e., connect the state-specific data with the state boundary data).\nPlot the data.\n\nFirst, let’s get the state boundaries. (We actually did this earlier when making our US map above, but we’ll do it again here for demonstration.)\n\n#  Get state borders from ggmap package and maps\nlibrary(maps)\n\nWarning: package 'maps' was built under R version 4.2.3\n\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n#?map_data \nstate_borders &lt;- map_data(\"state\") \nhead(state_borders)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\nNow we’ll load the state-specific dataset, state.x77:\n\nlibrary(datasets)\n#Note that the state.x77 dataset looks like this:\nhead(state.x77)\n\n           Population Income Illiteracy Life Exp Murder HS Grad Frost   Area\nAlabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska            365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia      21198   5114        1.1    71.71   10.3    62.6    20 156361\nColorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\n\n\nWe’ll need to do a minor amount of data manipulation to match the state.x77 dataset to the state_borders dataset. First, note that state.x77 technically doesn’t have a column with the state names; instead, its rownames correspond to the state names. So, first we’ll have to create a column with the state names. Second, the names within state_borders are all lowercase (see above), so we’ll need to take that into account with matching the two datasets as well.\nThe following code first grabs the rownames of the state.x77 table, then converts state.x77 to a tibble named state_data, and then adds that column with the state names and makes them lower case:\n\nstate_names &lt;- rownames(state.x77)\nstate_data &lt;- as_tibble(state.x77)\nstate_data &lt;- state_data |&gt;\n  mutate(state = state_names) |&gt;\n  mutate(state = tolower(state))\n\nNow we can match our two datasets using left_join():\n\n#  join state_data data to state_borders\nstate_borders &lt;- state_borders |&gt;\n  left_join(state_data, by = c(\"region\" = \"state\"))\n#Note that we now have the information from state_data inside\n#our state_borders dataset:\nhead(state_borders)\n\n       long      lat group order  region subregion Population Income Illiteracy\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;       3615   3624        2.1\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;       3615   3624        2.1\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;       3615   3624        2.1\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;       3615   3624        2.1\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;       3615   3624        2.1\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;       3615   3624        2.1\n  Life Exp Murder HS Grad Frost  Area\n1    69.05   15.1    41.3    20 50708\n2    69.05   15.1    41.3    20 50708\n3    69.05   15.1    41.3    20 50708\n4    69.05   15.1    41.3    20 50708\n5    69.05   15.1    41.3    20 50708\n6    69.05   15.1    41.3    20 50708\n\n\nFinally, we can make a plot of the state-specific data (in this case we focus on the illteracy rate by state in 1970). To do this, we just specify a fill within the geom_polygon() function. It’s also helpful to use the scale_fill_gradient2() function to denote what the colors should be for this fill (below, we set the midpoint within this function equal to the median of the variable we are plotting, which is common practice).\nImportant: If you do this (set the midpoint equal to the median), remember that you are forcing half the regions in your map to have “high value” colors and half the regions in your map to have “low value” colors. Furthermore, when looking at maps like the ones below, you should always keep the scale in mind. For example, there seems to be a distinction between the southern and northern US: They tend to be different colors. Ultimately, though, this difference corresponds to a 1-2% difference in illiteracy rates; we would need to (ironically) read up on illiteracy rates to better understand if this is a scientifically meaningful difference.\n\n#  Make the plot!  \n# Before running the following code, you need to have the `mapproj` package\n# install.packages(\"mapproj\")\n#  (Change the fill variable below as you see fit)\n#  (Change the color gradient as you see fit)\nggplot(state_borders) + \n  geom_polygon(aes(x = long, y = lat, group = group,\n                   fill = Illiteracy), color = \"black\") + \n  scale_fill_gradient2(low = \"darkgreen\", mid = \"lightgrey\", \n                       high = \"darkorchid4\", midpoint = 0.95) +\n  theme_void() +\n  coord_map(\"polyconic\") + \n  labs(\n    title = \"Spatial Distribution of Illiteracy Percent by State\",\n    subtitle = \"Percent of Population in State\",\n    caption = \"U.S. Department of Commerce, Bureau of the Census (1977)\",\n    fill = \"Illiteracy %\"\n  ) + \n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "demos/13-areal-data.html#use-coord_map-to-specify-your-map-projection",
    "href": "demos/13-areal-data.html#use-coord_map-to-specify-your-map-projection",
    "title": "Demo 13: Visualizations for areal data",
    "section": "",
    "text": "In the above code, note that we have the line coord_map(\"polyconic\"). This specifies a certain kind of map projection for our plot. We can consider other projections. For example, an old-school projection we discussed in lecture was the Mercator projection. The below map is the same map as above, but with the Mercator projection.\n\nggplot(state_borders) + \n  geom_polygon(aes(x = long, y = lat, group = group,\n                   fill = Illiteracy), color = \"black\") + \n  scale_fill_gradient2(low = \"darkgreen\", mid = \"lightgrey\", \n                       high = \"darkorchid4\", midpoint = 0.95) +\n  theme_void() +\n  coord_map(\"mercator\") + \n  labs(\n    title = \"Spatial Distribution of Illiteracy Percent by State\",\n    subtitle = \"Percent of Population in State\",\n    caption = \"U.S. Department of Commerce, Bureau of the Census (1977)\",\n    fill = \"Illiteracy %\"\n  ) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nNote that the coord_map() function is specifying the coordinates of our map.\nUnfortunately, some popular projections (like the Robinson projection and the Winkel Tripel projection) are not readily available. It’s a little bit annoying to get these projections working in R, but it can be done: See this tutorial. There isn’t one “right” projection, but some are better than others."
  },
  {
    "objectID": "demos/13-areal-data.html#dendrogram-of-states",
    "href": "demos/13-areal-data.html#dendrogram-of-states",
    "title": "Demo 13: Visualizations for areal data",
    "section": "Dendrogram of States",
    "text": "Dendrogram of States\nIf it seems like outcomes tend to cluster geographically (as in the graph above), then automated clustering techniques like hierarchical clustering should be able to pick up geographic clusters. To put it another way: If techniques like hierarchical clustering identify that certain geographic areas have similar outcomes, then there is a strong case to be made that indeed outcomes are clustering geographically.\nTo verify areal graph results with dendrograms, you can follow this workflow:\n\nMake an areal graph of outcomes (as we did above for illiteracy rates).\nUsing your eyes, identify geographic regions that appear to have similar outcomes. For example, in the graph above, we can broadly identify two clusters: The south and the north.\nRun hierarchical clustering on your data. When doing this, use the outcome (in this case, illiteracy rates) to measure the “distance” between geographic subjects (in this case, states).\nMake a dendrogram, and color the leaves of the dendrogram by the geographic regions you identified in the second bulletpoint. If the geographic regions tend to cluster together according to the dendrogram, this suggests that, indeed, outcomes are clustering geographically.\n\nBelow is the code to implement this workflow for the illiteracy example. We already completed the first two bulletpoints above, so now we just need to do the last two bulletpoints (below).\n\n# Remember that we have to scale our data when creating dendrograms\nillit_scaled &lt;- state_data$Illiteracy / sd(state_data$Illiteracy)\n# distance matrix for our dataset\nillit_dist &lt;- dist(illit_scaled)\n# run hierarchical clustering\nillit_hc &lt;- hclust(illit_dist, method = \"complete\")\n# convert to a dendrogram type object\nillit_dend &lt;- as.dendrogram(illit_hc)\n\n#We'll need the following library to make the dendrogram\n#more graphically pleasing:\nlibrary(dendextend)\n\n\n---------------------\nWelcome to dendextend version 1.17.1\nType citation('dendextend') for how to cite the package.\n\nType browseVignettes(package = 'dendextend') for the package vignette.\nThe github page is: https://github.com/talgalili/dendextend/\n\nSuggestions and bug-reports can be submitted at: https://github.com/talgalili/dendextend/issues\nYou may ask questions at stackoverflow, use the r and dendextend tags: \n     https://stackoverflow.com/questions/tagged/dendextend\n\n    To suppress this message use:  suppressPackageStartupMessages(library(dendextend))\n---------------------\n\n\n\nAttaching package: 'dendextend'\n\n\nThe following object is masked from 'package:stats':\n\n    cutree\n\n# first, let's change the labels according to the state abbreviations\n# (which is available in the datasets library, which we loaded earlier)\nillit_dend &lt;- set(illit_dend, \"labels\", state.abb, order_value = T)\n\n#We will also color the labels by the region of the state.\ntable(state.region) # this comes from the datasets package hence why we didn't load it\n\nstate.region\n    Northeast         South North Central          West \n            9            16            12            13 \n\nstat_region_colors &lt;- ifelse(state.region == \"Northeast\", \"darkgreen\",\n                             ifelse(state.region == \"South\", \"purple\",\n                                    ifelse(state.region == \"North Central\", \"orange\",\n                                           \"blue\")))\n#Set the leaf labels according to the above colors:\nillit_dend &lt;- set(illit_dend, \"labels_colors\", stat_region_colors, order_value = T)\n# plot the dendrogram\nplot(illit_dend)\n\n\n\n\n\n\n\n\nAs we can see, many of the southern states tend to cluster together. Meanwhile, the other clusters are fairly heterogeneous in terms of regions of the US. Thus, it appears that southern states tend to have more similar illiteracy rates (in this case, higher rates) than other parts of the US."
  },
  {
    "objectID": "demos/13-areal-data.html#visual-randomization-tests",
    "href": "demos/13-areal-data.html#visual-randomization-tests",
    "title": "Demo 13: Visualizations for areal data",
    "section": "Visual Randomization Tests",
    "text": "Visual Randomization Tests\nIn the above graph, we want to assess if illiteracy rates tend to depend on geography. So, consider the null hypothesis that illiteracy rates do not depend on geography. If the null hypothesis is true, what are the chances that our map just “happened” to look like the map above?\nTo answer this question, we will use a visual randomization test. The workflow for a visual randomization test is as follows:\n\nMake an areal graph of outcomes (as we did above for illiteracy rates).\nShuffle the outcomes randomly a few times (e.g., 8 times). Make a new areal graph for each of these shuffles. (In short, you’re taking the colors of your original graph and just shuffling them around the different geographic regions on the map.)\nPlot your original map along with all of the “shuffled” graphs, and show your graphs to someone else. Tell them that one graph is the “real graph” and the rest are “random graphs.” Can they tell which graph is the real graph? If so, then the graph we have is “significantly non-random” in terms of geography.\n\nBelow is the code to implement this workflow for the illiteracy example. We already completed the first bulletpoint, so now we just need to do the last two bulletpoints (below).\n\n# It'll be helpful to write a function that automatically makes a ggplot for us.\n# This is literally the same code we used to generate the original areal map above, but for any dataset called \"state_data\".\nget_state_map_illit &lt;- function(state_data){\n  plot &lt;- ggplot(state_data) + \n    geom_polygon(aes(x = long, y = lat, group = group,\n                     fill = Illiteracy), color = \"black\") +\n    scale_fill_gradient2(low = \"darkgreen\", mid = \"lightgrey\", \n                         high = \"darkorchid4\", midpoint = 0.95) +\n    theme_void() +\n    coord_map(\"polyconic\")\n  return(plot)\n}\n\n# Now we're going to permute (i.e., \"shuffle\") the outcomes a few times. \n# number of randomizations/permutations/shuffles:\nn_shuffles &lt;- 9\n\n# It's helpful to store ggplot objects in lists in R.\n# We haven't talked much about lists in this class, but they\n# are quite flexible and easy to use.\n# For example, we're going to create an object called plot_list.\n# plot_list[[1]] refers to the first object in the list,\n# plot_list[[2]] refers to the second object in the list,\n# and so on.\nplot_list &lt;- list(length = n_shuffles)\n# Will use a for loop to do this\nfor(i in 1:n_shuffles){\n  #create a \"randomized\" dataset\n  state_borders_rand &lt;- state_borders\n  #shuffle the outcomes\n  state_borders_rand$Illiteracy &lt;- sample(state_borders_rand$Illiteracy)\n  #create the plot and store it\n  plot_list[[i]] = get_state_map_illit(state_borders_rand)\n}\n# Could have also do the following for those that don't like for loops... (even\n# though this is still a for loop but calling compiled code underneath)\n# plot_list &lt;- lapply(1:nshuffles, \n#                     function(i) {\n#                       state_borders_rand &lt;- state_borders\n#                       # shuffle the outcomes\n#                       state_borders_rand$Illiteracy &lt;- sample(state_borders_rand$Illiteracy)\n#                       # Return the plot \n#                       get_state_map_illit(state_borders_rand)\n#                     })\n\n\n# pick a random entry of plot_list to be the \"real\" plot\nplot_list[[sample(1:n_shuffles, size = 1)]] = get_state_map_illit(state_borders)\n\n# Plot all the plots together using the cowplot package:\n# install.packages(\"cowplot\")\nlibrary(cowplot)\n\nWarning: package 'cowplot' was built under R version 4.2.3\n\n\n\nAttaching package: 'cowplot'\n\n\nThe following object is masked from 'package:ggmap':\n\n    theme_nothing\n\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nplot_grid(plotlist = plot_list, ncol = 3)\n\n\n\n\n\n\n\n# library(gridExtra)\n# grid.arrange(plot_list[[1]], plot_list[[2]], plot_list[[3]],\n#              plot_list[[4]], plot_list[[5]], plot_list[[6]],\n#              plot_list[[7]], plot_list[[8]], plot_list[[9]],\n#              nrow = 3)\n\nBecause it’s really annoying to have all of the legends displayed together, we can use the cowplot package to display a single legend below each of these maps. First, we grab a legend using the get_legend() function:\n\n# Grab the legend for just the first plot, since they are all the same\nmap_legend &lt;- get_legend(plot_list[[1]])\n\nWarning in get_plot_component(plot, \"guide-box\"): Multiple components found;\nreturning the first one. To return all, use `return_all = TRUE`.\n\n\nNext, we are going to update our plot_list so that the legends are removed from each of them. We can do this quickly using the lapply() function:\n\nlegend_free_plot_list &lt;- \n  lapply(1:length(plot_list),\n         function(i) plot_list[[i]] + theme(legend.position = \"none\"))\n\nAnd finally, we will now use multiple plot_grid function calls to display the shuffled maps next to the legend:\n\nplot_grid(\n  plot_grid(plotlist = legend_free_plot_list, ncol = 3),\n  map_legend, ncol = 2,\n  # Adjust so the maps are much larger:\n  rel_widths = c(4, 1)\n)\n\n\n\n\n\n\n\n\nIf you can spot which plot is the real plot (without having seen it previously!!), then illiteracy rates are significantly non-random across geography."
  },
  {
    "objectID": "demos/10-time-series.html",
    "href": "demos/10-time-series.html",
    "title": "Demo 10: Visualizing time series data",
    "section": "",
    "text": "A time series measures a single variable over many points in time. Time intervals may be regularly or irregularly spaced, but we’ll only consider regularly-spaced data today. We’ll focus on visualizing a single variable over time, since there are already so many choices and information to work with.\nFor this demo, we’re going to work with a dataset that’s actually already loaded when you start R: it’s defined under co2, and known as the “Mauna Loa Atmospheric CO2 Concentration” dataset. This dataset contains 468 monthly measurements of CO2 concentration from 1959 to 1997.\nWhen you start R, if you type in co2, this is what you’ll see:\n\nco2\n\n        Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct\n1959 315.42 316.31 316.50 317.56 318.13 318.00 316.39 314.65 313.68 313.18\n1960 316.27 316.81 317.42 318.87 319.87 319.43 318.01 315.74 314.00 313.68\n1961 316.73 317.54 318.38 319.31 320.42 319.61 318.42 316.63 314.83 315.16\n1962 317.78 318.40 319.53 320.42 320.85 320.45 319.45 317.25 316.11 315.27\n1963 318.58 318.92 319.70 321.22 322.08 321.31 319.58 317.61 316.05 315.83\n1964 319.41 320.07 320.74 321.40 322.06 321.73 320.27 318.54 316.54 316.71\n1965 319.27 320.28 320.73 321.97 322.00 321.71 321.05 318.71 317.66 317.14\n1966 320.46 321.43 322.23 323.54 323.91 323.59 322.24 320.20 318.48 317.94\n1967 322.17 322.34 322.88 324.25 324.83 323.93 322.38 320.76 319.10 319.24\n1968 322.40 322.99 323.73 324.86 325.40 325.20 323.98 321.95 320.18 320.09\n1969 323.83 324.26 325.47 326.50 327.21 326.54 325.72 323.50 322.22 321.62\n1970 324.89 325.82 326.77 327.97 327.91 327.50 326.18 324.53 322.93 322.90\n1971 326.01 326.51 327.01 327.62 328.76 328.40 327.20 325.27 323.20 323.40\n1972 326.60 327.47 327.58 329.56 329.90 328.92 327.88 326.16 324.68 325.04\n1973 328.37 329.40 330.14 331.33 332.31 331.90 330.70 329.15 327.35 327.02\n1974 329.18 330.55 331.32 332.48 332.92 332.08 331.01 329.23 327.27 327.21\n1975 330.23 331.25 331.87 333.14 333.80 333.43 331.73 329.90 328.40 328.17\n1976 331.58 332.39 333.33 334.41 334.71 334.17 332.89 330.77 329.14 328.78\n1977 332.75 333.24 334.53 335.90 336.57 336.10 334.76 332.59 331.42 330.98\n1978 334.80 335.22 336.47 337.59 337.84 337.72 336.37 334.51 332.60 332.38\n1979 336.05 336.59 337.79 338.71 339.30 339.12 337.56 335.92 333.75 333.70\n1980 337.84 338.19 339.91 340.60 341.29 341.00 339.39 337.43 335.72 335.84\n1981 339.06 340.30 341.21 342.33 342.74 342.08 340.32 338.26 336.52 336.68\n1982 340.57 341.44 342.53 343.39 343.96 343.18 341.88 339.65 337.81 337.69\n1983 341.20 342.35 342.93 344.77 345.58 345.14 343.81 342.21 339.69 339.82\n1984 343.52 344.33 345.11 346.88 347.25 346.62 345.22 343.11 340.90 341.18\n1985 344.79 345.82 347.25 348.17 348.74 348.07 346.38 344.51 342.92 342.62\n1986 346.11 346.78 347.68 349.37 350.03 349.37 347.76 345.73 344.68 343.99\n1987 347.84 348.29 349.23 350.80 351.66 351.07 349.33 347.92 346.27 346.18\n1988 350.25 351.54 352.05 353.41 354.04 353.62 352.22 350.27 348.55 348.72\n1989 352.60 352.92 353.53 355.26 355.52 354.97 353.75 351.52 349.64 349.83\n1990 353.50 354.55 355.23 356.04 357.00 356.07 354.67 352.76 350.82 351.04\n1991 354.59 355.63 357.03 358.48 359.22 358.12 356.06 353.92 352.05 352.11\n1992 355.88 356.63 357.72 359.07 359.58 359.17 356.94 354.92 352.94 353.23\n1993 356.63 357.10 358.32 359.41 360.23 359.55 357.53 355.48 353.67 353.95\n1994 358.34 358.89 359.95 361.25 361.67 360.94 359.55 357.49 355.84 356.00\n1995 359.98 361.03 361.66 363.48 363.82 363.30 361.94 359.50 358.11 357.80\n1996 362.09 363.29 364.06 364.76 365.45 365.01 363.70 361.54 359.51 359.65\n1997 363.23 364.06 364.61 366.40 366.84 365.68 364.52 362.57 360.24 360.83\n        Nov    Dec\n1959 314.66 315.43\n1960 314.84 316.03\n1961 315.94 316.85\n1962 316.53 317.53\n1963 316.91 318.20\n1964 317.53 318.55\n1965 318.70 319.25\n1966 319.63 320.87\n1967 320.56 321.80\n1968 321.16 322.74\n1969 322.69 323.95\n1970 323.85 324.96\n1971 324.63 325.85\n1972 326.34 327.39\n1973 327.99 328.48\n1974 328.29 329.41\n1975 329.32 330.59\n1976 330.14 331.52\n1977 332.24 333.68\n1978 333.75 334.78\n1979 335.12 336.56\n1980 336.93 338.04\n1981 338.19 339.44\n1982 339.09 340.32\n1983 340.98 342.82\n1984 342.80 344.04\n1985 344.06 345.38\n1986 345.48 346.72\n1987 347.64 348.78\n1988 349.91 351.18\n1989 351.14 352.37\n1990 352.69 354.07\n1991 353.64 354.89\n1992 354.09 355.33\n1993 355.30 356.78\n1994 357.59 359.05\n1995 359.61 360.74\n1996 360.80 362.38\n1997 362.49 364.34\n\n\nThese numbers are expressed in parts per million (ppm) - if you’ve taken a chemistry class (which I haven’t), I assure you know more about this than I do, but ppm is a very common measure for pollutants and contaminants.\nThe co2 object is actually defined with a class we haven’t seen yet, ts (time series):\n\nclass(co2)\n\n[1] \"ts\"\n\n\nAs a result, it contains extra attributes about the times associated with each value, and base R graphs can plot this automatically (we don’t need to specify the time range manually):\n\nplot(co2)\n\n\n\n\n\n\n\n\nThis is typically called a line plot. Here’s how you would plot the same data in the gg style (note that you need the package ggfortify to do this) without constructing the typical long-table format we’re used to:\n\nlibrary(ggfortify)\n\nWarning: package 'ggfortify' was built under R version 4.2.3\n\n\nLoading required package: ggplot2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nautoplot(co2)\n\n\n\n\n\n\n\n\nIn time series, we are interested in checking for trends (does the variable tend to increase or decrease over time?), seasonality (are there tendencies that regularly occur? If so, at what intervals do they occur?), general variability (i.e., variation beyond average trends and seasonality), and outliers (unusual spikes or valleys). Which of these do we see in the co2 data?\nIn order to show you more general-purpose plotting methods, we’ll treat co2 as numeric instead of ts from now on. This is actually the form that many time series data take (they’re usually not in the ts format), so this will also show you some of the formatting/structuring issues you’ll have to work with in the wild.\nIn the code below, we create a dataset with 1:468 as the obs_i variable (x-axis variable) and the CO2 concentration as the co2_val variable (y-axis) variable). Then, we use our usual ggplot to plot the data:\n\nlibrary(tidyverse)\nco2_tbl &lt;- tibble(co2_val = as.numeric(co2)) |&gt;\n  mutate(obs_i = 1:n())\nco2_tbl |&gt;\n  ggplot(aes(x = obs_i, y = co2_val)) + \n  geom_line() + \n  labs(x = \"Time index\", y = \"CO2 (ppm)\")\n\n\n\n\n\n\n\n\nNote the x-axis: This will not be helpful/informative to readers. It’s a common mistake for people to just put something like “index” on the x-axis, but they don’t even tell you what they are indexing. Be sure to not make this mistake! To properly structure time series data and label it correctly, you’ll need to know how to work with Date type objects in R (which we will discuss next…).\n\n\nThe following code redefines creates a new column in our dataset to be the monthly dates 1/1/1959, 2/1/1959, … , 12/1/1997 using the as.Date() function (given the description of the time range in help(co2)):\n\nco2_tbl &lt;- co2_tbl |&gt;\n  # We can use the seq() function with dates which is pretty useful!\n  mutate(obs_date = seq(as.Date(\"1959/1/1\"), by = \"month\",\n                        length.out = n()))\nco2_tbl |&gt;\n  ggplot(aes(x = obs_date, y = co2_val)) + \n  geom_line() + \n  labs(x = \"Year\", y = \"CO2 (ppm)\")\n\n\n\n\n\n\n\n\nUnfortunately, the default format for dates in as.Date() is Year/Month/Day. If you prefer another format, such as the common Month/Day/Year (as I used above in the previous paragraph), you need to include the format argument within as.Date(), as such (note that the Y is capitalized - yes, as.Date() is that picky):\n\nco2_tbl &lt;- co2_tbl |&gt;\n  mutate(obs_date = seq(as.Date(\"1/1/1959\", format = \"%m/%d/%Y\"), \n                        by = \"month\",\n                        length.out = n()))\nco2_tbl |&gt;\n  ggplot(aes(x = obs_date, y = co2_val)) + \n  geom_line() + \n  labs(x = \"Year\", y = \"CO2 (ppm)\")\n\n\n\n\n\n\n\n\nNote that all we needed to do was convert the obs_date variable to a Date class, and then we could use ggplot as is. This is because ggplot knows to use a special date scale for the x-axis when x has class Date. As a result, we can easily play with the breaks on the date axis using scale_x_date(). For example:\n\nFor a subset of the data, maybe we only want ticks every 4 months, using date_breaks.\nWe can specify the format of the date with date_labels. (See Details section of ?strftime for the formatting options. Here, we choose abbreviated month %b and full year %Y.)\n\n\nco2_tbl[1:26,] |&gt;\n  ggplot(aes(x = obs_date, y = co2_val)) + \n  geom_line() + \n  scale_x_date(date_breaks = \"4 months\", date_labels = \"%b %Y\") +\n  labs(x = \"Year\", y = \"CO2 (ppm)\") +\n  # Modify the x-axis text \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "demos/10-time-series.html#formatting-date-data-in-r",
    "href": "demos/10-time-series.html#formatting-date-data-in-r",
    "title": "Demo 10: Visualizing time series data",
    "section": "",
    "text": "The following code redefines creates a new column in our dataset to be the monthly dates 1/1/1959, 2/1/1959, … , 12/1/1997 using the as.Date() function (given the description of the time range in help(co2)):\n\nco2_tbl &lt;- co2_tbl |&gt;\n  # We can use the seq() function with dates which is pretty useful!\n  mutate(obs_date = seq(as.Date(\"1959/1/1\"), by = \"month\",\n                        length.out = n()))\nco2_tbl |&gt;\n  ggplot(aes(x = obs_date, y = co2_val)) + \n  geom_line() + \n  labs(x = \"Year\", y = \"CO2 (ppm)\")\n\n\n\n\n\n\n\n\nUnfortunately, the default format for dates in as.Date() is Year/Month/Day. If you prefer another format, such as the common Month/Day/Year (as I used above in the previous paragraph), you need to include the format argument within as.Date(), as such (note that the Y is capitalized - yes, as.Date() is that picky):\n\nco2_tbl &lt;- co2_tbl |&gt;\n  mutate(obs_date = seq(as.Date(\"1/1/1959\", format = \"%m/%d/%Y\"), \n                        by = \"month\",\n                        length.out = n()))\nco2_tbl |&gt;\n  ggplot(aes(x = obs_date, y = co2_val)) + \n  geom_line() + \n  labs(x = \"Year\", y = \"CO2 (ppm)\")\n\n\n\n\n\n\n\n\nNote that all we needed to do was convert the obs_date variable to a Date class, and then we could use ggplot as is. This is because ggplot knows to use a special date scale for the x-axis when x has class Date. As a result, we can easily play with the breaks on the date axis using scale_x_date(). For example:\n\nFor a subset of the data, maybe we only want ticks every 4 months, using date_breaks.\nWe can specify the format of the date with date_labels. (See Details section of ?strftime for the formatting options. Here, we choose abbreviated month %b and full year %Y.)\n\n\nco2_tbl[1:26,] |&gt;\n  ggplot(aes(x = obs_date, y = co2_val)) + \n  geom_line() + \n  scale_x_date(date_breaks = \"4 months\", date_labels = \"%b %Y\") +\n  labs(x = \"Year\", y = \"CO2 (ppm)\") +\n  # Modify the x-axis text \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "demos/12-spatial-data.html",
    "href": "demos/12-spatial-data.html",
    "title": "Demo 12: Visualizations and inference for spatial data",
    "section": "",
    "text": "Before moving on to the code below, you’ll first need to create a free Stadia Maps account in order to access the map styles to use with ggmap. You can find the directions about the process here. After you click on the “Create an Account” button, follow the steps by entering in your relevant information. Then select Mobile/Native App version to generate an API key. You’ll then find your API key in the displayed Stadia-hosted URLs, displayed at the end of the URLs after api_key=. Once you have the API key saved somewhere, after you install the ggmap package you can then run the following code to save your API key once and then never have to run this step again:\n\n# You first need to install the ggmap R package\n# install.packages(\"ggmap\")\nggmap:register_stadiamaps(\"YOUR-API-KEY-HERE\",\n                          # write = TRUE saves this once\n                          write = TRUE)"
  },
  {
    "objectID": "demos/12-spatial-data.html#example-plotting-airline-flight-data-across-the-us",
    "href": "demos/12-spatial-data.html#example-plotting-airline-flight-data-across-the-us",
    "title": "Demo 12: Visualizations and inference for spatial data",
    "section": "Example: Plotting Airline Flight Data across the US",
    "text": "Example: Plotting Airline Flight Data across the US\nHere, we’ll work with airline data from this GitHub repository. You can find out more about this dataset here.\nBefore we begin, note that this is just one example of how you can add interesting information to maps with ggmap. As long as you have latitude and longitude information, you should be able to add data to maps. For more interesting examples and for an in-depth description of ggmap, see the short paper by David Kahle and Hadley Wickham here. (This paper may be helpful for teams that will be working with spatial data for the group project.)"
  },
  {
    "objectID": "demos/12-spatial-data.html#load-flight-data-from-github",
    "href": "demos/12-spatial-data.html#load-flight-data-from-github",
    "title": "Demo 12: Visualizations and inference for spatial data",
    "section": "Load flight data from GitHub",
    "text": "Load flight data from GitHub\nHere we will use a large dataset on GitHub about airline flights across the world. After loading in the raw data, we’ll do some data manipulation to create useful variables for this dataset.\nFirst, we’ll load a dataset on airports across the world:\n\n#  Load and format airports data\nairports &lt;- read_csv(\"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\",\n                     col_names = c(\"ID\", \"name\", \"city\", \"country\", \"IATA_FAA\", \n                                   \"ICAO\", \"lat\", \"lon\", \"altitude\", \"timezone\", \"DST\"))\n# Here's what the data look like:\nairports\n\n# A tibble: 7,698 × 14\n      ID name   city  country IATA_FAA ICAO    lat   lon altitude timezone DST  \n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;\n 1     1 Gorok… Goro… Papua … GKA      AYGA  -6.08 145.      5282 10       U    \n 2     2 Madan… Mada… Papua … MAG      AYMD  -5.21 146.        20 10       U    \n 3     3 Mount… Moun… Papua … HGU      AYMH  -5.83 144.      5388 10       U    \n 4     4 Nadza… Nadz… Papua … LAE      AYNZ  -6.57 147.       239 10       U    \n 5     5 Port … Port… Papua … POM      AYPY  -9.44 147.       146 10       U    \n 6     6 Wewak… Wewak Papua … WWK      AYWK  -3.58 144.        19 10       U    \n 7     7 Narsa… Nars… Greenl… UAK      BGBW  61.2  -45.4      112 -3       E    \n 8     8 Godth… Godt… Greenl… GOH      BGGH  64.2  -51.7      283 -3       E    \n 9     9 Kange… Sond… Greenl… SFJ      BGSF  67.0  -50.7      165 -3       E    \n10    10 Thule… Thule Greenl… THU      BGTL  76.5  -68.7      251 -4       E    \n# ℹ 7,688 more rows\n# ℹ 3 more variables: X12 &lt;chr&gt;, X13 &lt;chr&gt;, X14 &lt;chr&gt;\n\n\nThen, we’ll load a dataset on airline flight routes across the world:\n\n#  Load and format routes data\nroutes &lt;- read_csv(\"https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat\",\n                   col_names = c(\"airline\", \"airlineID\", \"sourceAirport\", \n                                 \"sourceAirportID\", \"destinationAirport\", \n                                 \"destinationAirportID\", \"codeshare\", \"stops\",\n                                 \"equipment\"))\n# Here's what the data look like:\nroutes\n\n# A tibble: 67,663 × 9\n   airline airlineID sourceAirport sourceAirportID destinationAirport\n   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;           &lt;chr&gt;             \n 1 2B      410       AER           2965            KZN               \n 2 2B      410       ASF           2966            KZN               \n 3 2B      410       ASF           2966            MRV               \n 4 2B      410       CEK           2968            KZN               \n 5 2B      410       CEK           2968            OVB               \n 6 2B      410       DME           4029            KZN               \n 7 2B      410       DME           4029            NBC               \n 8 2B      410       DME           4029            TGK               \n 9 2B      410       DME           4029            UUA               \n10 2B      410       EGO           6156            KGD               \n# ℹ 67,653 more rows\n# ℹ 4 more variables: destinationAirportID &lt;chr&gt;, codeshare &lt;chr&gt;, stops &lt;dbl&gt;,\n#   equipment &lt;chr&gt;"
  },
  {
    "objectID": "demos/12-spatial-data.html#manipulating-the-data-to-get-some-custom-tibbles",
    "href": "demos/12-spatial-data.html#manipulating-the-data-to-get-some-custom-tibbles",
    "title": "Demo 12: Visualizations and inference for spatial data",
    "section": "Manipulating the data to get some custom tibbles",
    "text": "Manipulating the data to get some custom tibbles\nHere, we’ll do some data manipulation to obtain the number of arrivals/departures per airport.\nThe following code does two tasks that very commonly come up with spatial data:\n\nCount the number of events at a particular location.\nMerge two datasets by a common variable (e.g., an ID or location).\n\nWe’ve seen instances of the first task before (e.g., counting the number of subjects belonging to a certain group). We haven’t done the second task before, but it regularly comes up in spatial data: Often, you’ll find multiple datasets that contain different variables about the same spatial location, and you’ll want to merge them together. We’ll do this using the left_join() function within the dplyr package that’s in the tidyverse.\n\n#  Manipulate the routes data to create two new tibbles:\n#  one for arrivals, one for departures.  \n\n#  Each counts the number of flights arriving/departing from each airport.\ndepartures &lt;- routes |&gt; \n  group_by(sourceAirportID) |&gt;\n  summarize(n_depart = n()) |&gt;\n  # Convert the ID to integer since it's a character (need to have matching\n  # data types for columns you will join on)\n  mutate(sourceAirportID = as.integer(sourceAirportID))\n\narrivals &lt;- routes |&gt; \n  group_by(destinationAirportID) |&gt; \n  summarize(n_arrive = n()) |&gt; \n  mutate(destinationAirportID = as.integer(destinationAirportID))\n\n# Join both of these summaries to the airports data bsed on the ID column to\n# match their respective ID columns\n\n# First join the departures:\nairports &lt;- airports |&gt;\n  # We join the departures so that ID matches sourceAirportID\n  left_join(departures, by = c(\"ID\" = \"sourceAirportID\"))\n\n# And now join arrivals:\nairports &lt;- airports |&gt;\n  # Same process - but different ID\n  left_join(arrivals, by = c(\"ID\" = \"destinationAirportID\"))\n\nThus, each row corresponds to a single airport with columns about the airport along with info about the number of flights (departures and arrivals)."
  },
  {
    "objectID": "demos/12-spatial-data.html#mapping-the-data-we-created",
    "href": "demos/12-spatial-data.html#mapping-the-data-we-created",
    "title": "Demo 12: Visualizations and inference for spatial data",
    "section": "Mapping the data we created",
    "text": "Mapping the data we created\nFirst, we’ll use ggmap to create a base map of the data. Although the dataset is for the whole world, we’ll focus on the US for simplicity.\n\n# First, we'll draw a \"box\" around the US\n# (in terms of latitude and longitude)\nUS &lt;- c(left = -125, bottom = 10, right = -67, top = 49)\nmap &lt;- get_stadiamap(US, zoom = 5, maptype = \"stamen_toner_lite\")\n#  Visualize the basic map\nggmap(map)\n\n\n\n\n\n\n\n\nNow we’ll demonstrate how to add points to the map using geom_point(). Thus, this is equivalent to creating a scatterplot, but within the space of a map.\n\n#  Add points to the map of airports\nggmap(map) + \n  geom_point(data = airports, aes(x = lon, y = lat), alpha = .25)\n\n\n\n\n\n\n\n\nWe can also add contour lines using stat_density2d(), just like we did with scatterplots. In the code below, we also use the bins argument, which we haven’t seen before in stat_density2d, but intuitively it is the same as when we used bins for histograms: More bins corresponds to more granularity in the data. I’ve found that not specifying bins can give very odd results for spatial data, so I recommend setting bins to a somewhat low value such that you can easily see high- and low-density areas on a map.\n\nggmap(map) +\n  stat_density2d(data = airports,\n                 aes(x = lon, y = lat, fill = after_stat(level)),\n                 alpha = 0.2, geom = \"polygon\", bins = 4) +\n  geom_point(data = airports, aes(x = lon, y = lat), alpha = .25)\n\n\n\n\n\n\n\n\nNow, think about what the above map is displaying. It is density of airports (which correspond to the rows of our dataset), but it is not displaying the density of flights (arrivals or departures). How can we display the density of flights on this map?\nA common option is to make the size of the points proportional to a third variable (in this case departures):\n\n#  Add points to the map of airports\n#  Each point will be located at the lat/long of the airport\n#  The size of the points is proportional to the square root of the number of departures at that airport\nmap_point_depart &lt;- ggmap(map) +\n  geom_point(data = airports, \n             aes(x = lon, y = lat, size = sqrt(n_depart)), alpha = .25)\n\n# In this case, it can be helpful to create your own legend for the plot, so \n# that you can set which sizes of the correspond to which values in the data:\nmap_point_depart &lt;- map_point_depart + \n  scale_size_area(breaks = sqrt(c(1, 5, 10, 50, 100, 500)), \n                  labels = c(1, 5, 10, 50, 100, 500), \n                  name = \"# departures\")\nmap_point_depart\n\n\n\n\n\n\n\n\nYou could also change the color of the points. However, as we’ve seen previously, just setting color = variable for a continuous variable often gives unsatisfactory results (because the default colors aren’t great). This is especially true for spatial data, where there is often a wide range of responses across geography.\nIn what follows, I demonstrate two ways that you can manually change the colors on a map; one uses scale_color_gradient2(), and one uses scale_color_distiller. The second one is more straightforward, but the first one allows for more flexibility.\n\n# What happens if we just set sqrt(n_depart) equal to color\nggmap(map) +\n  geom_point(data = airports, \n             aes(x = lon, y = lat, color = sqrt(n_depart)), alpha = .5)\n\n\n\n\n\n\n\n# Using scale_color_gradient2 to manually set the color scale\nggmap(map) +\n  geom_point(data = airports, \n             aes(x = lon, y = lat, color = sqrt(n_depart)), alpha = .5) +\n  # Note that you can also set a \"mid\" color, which is what the color will be \n  # at the midpoint (which you also set). Using limits (or breaks, \n  # which I don't do here) is very useful when your variable has a wide range \n  # and/or is very skewed (which is the case for the n_depart variable).\n  scale_color_gradient2(low = \"blue\", mid = \"purple\", high = \"red\", \n                        midpoint = 12.5, limits = sqrt(c(1, 500)))\n\n\n\n\n\n\n\n# Using scale_color_distiller to manually set the color scale\nggmap(map) +\n  geom_point(data = airports, \n             aes(x = lon, y = lat, color = sqrt(n_depart)), alpha = .5) +\n  # Note that there are many different palettes - see help documentation\n  # for scale_color_distiller\n  scale_color_distiller(palette = \"Spectral\") \n\n\n\n\n\n\n\n\nWe could of course map one variable to size and another to color, e.g., n_depart to size and n_arrive to color. You should NOT map one variable to both aesthetics - that is a redundant display of information.\n\nggmap(map) +\n  geom_point(data = airports, \n             aes(x = lon, y = lat, \n                 size = sqrt(n_depart), color = sqrt(n_arrive)), \n             alpha = .5) +\n  scale_size_area(breaks = sqrt(c(1, 5, 10, 50, 100, 500)), \n                  labels = c(1, 5, 10, 50, 100, 500), \n                  name = \"# departures\") +\n  scale_color_distiller(palette = \"Spectral\") +\n  labs(color = \"sqrt(# arrivals)\")\n\n\n\n\n\n\n\n\nFinally, we can (as usual) change the shape of the points according to another variable. The following marks airports that have an altitude higher than 1500 feet (which I don’t think would normally be of scientific interest, but this nonetheless shows you how to change shape in this way):\n\nggmap(map) +\n  geom_point(data = airports, \n             aes(x = lon, y = lat, \n                 size = sqrt(n_depart), color = sqrt(n_arrive),\n                 shape = altitude &gt; 1500), \n             alpha = .5) +\n  scale_size_area(breaks = sqrt(c(1, 5, 10, 50, 100, 500)), \n                  labels = c(1, 5, 10, 50, 100, 500), \n                  name = \"# departures\") +\n  scale_color_distiller(palette = \"Spectral\") +\n  labs(color = \"sqrt(# arrivals)\")\n\n\n\n\n\n\n\n\nThe above plots are nice ways to show the distribution of the number of flights. However, earlier we used stat_density2d to plot the density of airports. How can we use this function to plot the density of flights?\nThis requires a bit of hacking. The stat_density2d will plot the density of points across rows in your dataset. Thus, if we want to look at the density of departures, we can duplicate rows that have multiple departures For example, if an airport has n_depart = 5, we can create 5 rows in a new dataset for that airport. Then, we can use stat_density_2d on this “duplicated” dataset.\n\n# First remove the airports with missing n_depart\nclean_airports &lt;- airports |&gt;\n  filter(!is.na(n_depart)) # ! is the negate operator in R, so this only keeps\n  # rows where n_depart is NOT missing  \n\nairports_duplicated &lt;- clean_airports[rep(row.names(clean_airports), \n                                          clean_airports$n_depart),]\n\nggmap(map) + stat_density2d(data = airports_duplicated,\n                            aes(x = lon, y = lat, fill = after_stat(level)), \n                                alpha = 0.2, geom = \"polygon\")\n\n\n\n\n\n\n\n\nUnsurprisingly, we see there are higher concentrations of flights at major cities. Arguably, it’s much easier to deduce this from the above density plot than from the color plots. But both types of plots (density plots and color plots) serve a purpose, and preference for one or the other will depend on the application and the questions of interest."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Graphics and Visualization 36-315",
    "section": "",
    "text": "This is the companion website for Statistical Graphics and Visualization 36-315. While all of the assignments will be posted on Canvas, this website provides an alternative way to access lecture materials and additional demo files (see the see side-bar).\nLectures are on Mondays and Wednesdays from 12 - 12:50 PM, located in DH A302.\nOffice hours schedule:\n\n\n\n\n\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\nProf Yurko\nWednesdays and Thursdays @ 2 PM\nBaker Hall 132D\n\n\nAnna Rosengart\nMondays @ 2 PM\nZoom (see Canvas)\n\n\nPerry Lin\nWednesdays @ 11 AM\nZoom (see Canvas)\n\n\n\nThere are no required textbooks for this course, but the following are useful free resources that I will sometimes refer to as recommended reading:\n\nR for Data Science\nData Visualization: A Practical Introduction\nFundamentals of Data Visualization\nggplot2: Elegant Graphics for Data Analysis\n\nAnd the following are interesting data visualization websites:\n\nFlowingData\nHistory of Data Visualization\nFriends Don’t Let Friends Make Bad Graphs",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "demos/08-pca.html",
    "href": "demos/08-pca.html",
    "title": "Demo 08: Principal Component Analysis",
    "section": "",
    "text": "Throughout this demo we will again use the dataset about Starbucks drinks available in the #TidyTuesday project.\nYou can read in and manipulate various columns in the dataset with the following code:\n\nlibrary(tidyverse)\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g),\n         fiber_g = as.numeric(fiber_g))\n\nWe will apply principal component analysis (PCA) to the quantitative variables in this dataset:\n\n# Select the variables of interest:\nstarbucks_quant_data &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg)\ndim(starbucks_quant_data)\n\n[1] 1147   11\n\n\nAs seen above, there are 11 quantitative variables in the dataset, and it’s difficult to visualize 11 quantitative variables simultaneously. Maybe we can “get away” with just plotting two dimensions that make up the majority of the variation among these 11 variables (i.e., the first two principal components).\nTo conduct PCA, you must center and standardize your variables. We can either do that manually with the scale() function:\n\nscaled_starbucks_quant_data &lt;- scale(starbucks_quant_data)\n\nOr we can tell R do that for us before performing PCA using the prcomp() function:\n\n# perform PCA\nstarbucks_pca &lt;- prcomp(starbucks_quant_data, \n                        # Center and scale variables:\n                        center = TRUE, scale. = TRUE)\n# This is equivalent to the following commented out code:\n# starbucks_pca &lt;- prcomp(scaled_starbucks_quant_data, \n#                         center = FALSE, scale. = FALSE)\n# View the summary\nsummary(starbucks_pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000\n\n\nThere are 11 variables in this dataset, and thus there are 11 principal components. However, we can see that the first principal component accounts for over half of the variation in the dataset(!), while the second accounts for about 15% of the variation. As we can see, the variation accounted by each component adds up to the total variation in the data (i.e., the “cumulative proportion” equals 100% in the PC11 column). Also, in the first row, we can see that \\(\\text{Var}(Z_1) &gt; \\text{Var}(Z_2) &gt; \\cdots &gt; \\text{Var}(Z_{11})\\), as expected given what we talked about in lecture.\nWe haven’t actually computed the principal components \\(Z_1,\\dots,Z_{11}\\) yet. In brief, PCA provides a \\(p \\times p\\) “rotation matrix,” and the matrix \\(\\boldsymbol{Z} = (Z_1,\\dots,Z_{11})\\) is equal to the original data matrix \\(X\\) times the rotation matrix. The prcomp() function returns us the result of this matrix multiplication: the matrix of the principal component scores \\(\\boldsymbol{Z} = (Z_1,\\dots,Z_{11})\\) which can be accessed in the following way:\n\nstarbucks_pc_matrix &lt;- starbucks_pca$x\nhead(starbucks_pc_matrix)\n\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9        PC10         PC11\n[1,] -0.02812472 0.006489978 0.05145094 -0.06678083 -0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 -0.08080545 -0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 -0.09389227 -0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 -0.11582260 -0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 -0.03631676 -0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 -0.03497690 -0.002469611\n\n\nWe could have manually computed this using the returned rotation matrix and the original data (but centered and scaled). You perform matrix multiplication in R using the %*% operator:\n\nmanual_starbucks_pc_matrix &lt;- \n  as.matrix(scaled_starbucks_quant_data) %*% starbucks_pca$rotation\nhead(manual_starbucks_pc_matrix)\n\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9        PC10         PC11\n[1,] -0.02812472 0.006489978 0.05145094 -0.06678083 -0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 -0.08080545 -0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 -0.09389227 -0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 -0.11582260 -0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 -0.03631676 -0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 -0.03497690 -0.002469611\n\n\nAs you can see from just the first so many rows, these matrices match. If we view the dimensionality of this matrix (just the one returned to us by prcomp), we can seee that it matches the dimensionality of the original dataset:\n\ndim(starbucks_pc_matrix)\n\n[1] 1147   11\n\n\nIndeed, it is literally an 11-dimensional rotation of our dataset. However, the first column of this matrix accounts for over half of the variation in the data and the second column accounts for over 15% of the variation, so maybe we can “get away” with plotting just those first two dimensions.\nTo recreate what the summary output of prcomp function gave us above, the following line of code computes the standard deviation of each \\(Z\\) (the numbers match what’s given in the first row of numbers above):\n\napply(starbucks_pc_matrix, MARGIN = 2, FUN = sd)\n\n       PC1        PC2        PC3        PC4        PC5        PC6        PC7 \n2.47478380 1.30742010 1.05712064 0.97918632 0.67836258 0.56399067 0.44130936 \n       PC8        PC9       PC10       PC11 \n0.28122634 0.16874262 0.08701525 0.04048139 \n\n\nThis corresponds to the singular values, i.e., \\(\\sqrt{\\lambda_j}\\). We can then compute the proportion of variance explained by each component (also displayed in the summary output) by squaring these values and dividing by the number of columns:\n\n# Note that I can just replace the sd function above with the var function\napply(starbucks_pc_matrix, MARGIN = 2, FUN = var) / \n  ncol(starbucks_pc_matrix)\n\n         PC1          PC2          PC3          PC4          PC5          PC6 \n0.5567777142 0.1553952108 0.1015912775 0.0871641674 0.0418341630 0.0289168610 \n         PC7          PC8          PC9         PC10         PC11 \n0.0177049043 0.0071898413 0.0025885519 0.0006883321 0.0001489766 \n\n\nThe plot below displays the first two PCs \\(Z_1\\) and \\(Z_2\\):\n\n# First add these columns to the original dataset:\nstarbucks &lt;- starbucks |&gt;\n  mutate(pc1 = starbucks_pc_matrix[,1], \n         pc2 = starbucks_pc_matrix[,2])\nstarbucks |&gt;\n  ggplot(aes(x = pc1, y = pc2)) +\n  geom_point(alpha = 0.25) +\n  labs(x = \"PC 1\", y = \"PC 2\")\n\n\n\n\n\n\n\n\nThis matches what we saw returned by MDS!"
  },
  {
    "objectID": "demos/08-pca.html#principal-components-of-starbucks",
    "href": "demos/08-pca.html#principal-components-of-starbucks",
    "title": "Demo 08: Principal Component Analysis",
    "section": "",
    "text": "Throughout this demo we will again use the dataset about Starbucks drinks available in the #TidyTuesday project.\nYou can read in and manipulate various columns in the dataset with the following code:\n\nlibrary(tidyverse)\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g),\n         fiber_g = as.numeric(fiber_g))\n\nWe will apply principal component analysis (PCA) to the quantitative variables in this dataset:\n\n# Select the variables of interest:\nstarbucks_quant_data &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg)\ndim(starbucks_quant_data)\n\n[1] 1147   11\n\n\nAs seen above, there are 11 quantitative variables in the dataset, and it’s difficult to visualize 11 quantitative variables simultaneously. Maybe we can “get away” with just plotting two dimensions that make up the majority of the variation among these 11 variables (i.e., the first two principal components).\nTo conduct PCA, you must center and standardize your variables. We can either do that manually with the scale() function:\n\nscaled_starbucks_quant_data &lt;- scale(starbucks_quant_data)\n\nOr we can tell R do that for us before performing PCA using the prcomp() function:\n\n# perform PCA\nstarbucks_pca &lt;- prcomp(starbucks_quant_data, \n                        # Center and scale variables:\n                        center = TRUE, scale. = TRUE)\n# This is equivalent to the following commented out code:\n# starbucks_pca &lt;- prcomp(scaled_starbucks_quant_data, \n#                         center = FALSE, scale. = FALSE)\n# View the summary\nsummary(starbucks_pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000\n\n\nThere are 11 variables in this dataset, and thus there are 11 principal components. However, we can see that the first principal component accounts for over half of the variation in the dataset(!), while the second accounts for about 15% of the variation. As we can see, the variation accounted by each component adds up to the total variation in the data (i.e., the “cumulative proportion” equals 100% in the PC11 column). Also, in the first row, we can see that \\(\\text{Var}(Z_1) &gt; \\text{Var}(Z_2) &gt; \\cdots &gt; \\text{Var}(Z_{11})\\), as expected given what we talked about in lecture.\nWe haven’t actually computed the principal components \\(Z_1,\\dots,Z_{11}\\) yet. In brief, PCA provides a \\(p \\times p\\) “rotation matrix,” and the matrix \\(\\boldsymbol{Z} = (Z_1,\\dots,Z_{11})\\) is equal to the original data matrix \\(X\\) times the rotation matrix. The prcomp() function returns us the result of this matrix multiplication: the matrix of the principal component scores \\(\\boldsymbol{Z} = (Z_1,\\dots,Z_{11})\\) which can be accessed in the following way:\n\nstarbucks_pc_matrix &lt;- starbucks_pca$x\nhead(starbucks_pc_matrix)\n\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9        PC10         PC11\n[1,] -0.02812472 0.006489978 0.05145094 -0.06678083 -0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 -0.08080545 -0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 -0.09389227 -0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 -0.11582260 -0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 -0.03631676 -0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 -0.03497690 -0.002469611\n\n\nWe could have manually computed this using the returned rotation matrix and the original data (but centered and scaled). You perform matrix multiplication in R using the %*% operator:\n\nmanual_starbucks_pc_matrix &lt;- \n  as.matrix(scaled_starbucks_quant_data) %*% starbucks_pca$rotation\nhead(manual_starbucks_pc_matrix)\n\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9        PC10         PC11\n[1,] -0.02812472 0.006489978 0.05145094 -0.06678083 -0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 -0.08080545 -0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 -0.09389227 -0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 -0.11582260 -0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 -0.03631676 -0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 -0.03497690 -0.002469611\n\n\nAs you can see from just the first so many rows, these matrices match. If we view the dimensionality of this matrix (just the one returned to us by prcomp), we can seee that it matches the dimensionality of the original dataset:\n\ndim(starbucks_pc_matrix)\n\n[1] 1147   11\n\n\nIndeed, it is literally an 11-dimensional rotation of our dataset. However, the first column of this matrix accounts for over half of the variation in the data and the second column accounts for over 15% of the variation, so maybe we can “get away” with plotting just those first two dimensions.\nTo recreate what the summary output of prcomp function gave us above, the following line of code computes the standard deviation of each \\(Z\\) (the numbers match what’s given in the first row of numbers above):\n\napply(starbucks_pc_matrix, MARGIN = 2, FUN = sd)\n\n       PC1        PC2        PC3        PC4        PC5        PC6        PC7 \n2.47478380 1.30742010 1.05712064 0.97918632 0.67836258 0.56399067 0.44130936 \n       PC8        PC9       PC10       PC11 \n0.28122634 0.16874262 0.08701525 0.04048139 \n\n\nThis corresponds to the singular values, i.e., \\(\\sqrt{\\lambda_j}\\). We can then compute the proportion of variance explained by each component (also displayed in the summary output) by squaring these values and dividing by the number of columns:\n\n# Note that I can just replace the sd function above with the var function\napply(starbucks_pc_matrix, MARGIN = 2, FUN = var) / \n  ncol(starbucks_pc_matrix)\n\n         PC1          PC2          PC3          PC4          PC5          PC6 \n0.5567777142 0.1553952108 0.1015912775 0.0871641674 0.0418341630 0.0289168610 \n         PC7          PC8          PC9         PC10         PC11 \n0.0177049043 0.0071898413 0.0025885519 0.0006883321 0.0001489766 \n\n\nThe plot below displays the first two PCs \\(Z_1\\) and \\(Z_2\\):\n\n# First add these columns to the original dataset:\nstarbucks &lt;- starbucks |&gt;\n  mutate(pc1 = starbucks_pc_matrix[,1], \n         pc2 = starbucks_pc_matrix[,2])\nstarbucks |&gt;\n  ggplot(aes(x = pc1, y = pc2)) +\n  geom_point(alpha = 0.25) +\n  labs(x = \"PC 1\", y = \"PC 2\")\n\n\n\n\n\n\n\n\nThis matches what we saw returned by MDS!"
  },
  {
    "objectID": "demos/08-pca.html#one-biplot-to-rule-them-all",
    "href": "demos/08-pca.html#one-biplot-to-rule-them-all",
    "title": "Demo 08: Principal Component Analysis",
    "section": "One Biplot to Rule Them All",
    "text": "One Biplot to Rule Them All\nHowever, the components by themselves aren’t very interpretable - how do they relate to original variables? At this point, it’s important to remember that the principal components are linear combinations of the original variables. So, there is a (deterministic) linear relationship between the original variables and the principal components that we are plotting here.\nUsing the popular R package factoextra, we can plot these linear relationships on top of the scatterplot. We can do so using what’s called a biplot, which is essentially just a fancy expression for “scatterplots with arrows on top”. After installing the factoextra package, we can create the biplot using the fviz_pca_biplot() function on the prcomp output directly (but with the observation labels turned off!):\n\n# install.packages(\"factoextra\")\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n# Designate to only label the variables:\nfviz_pca_biplot(starbucks_pca, label = \"var\", \n                # Change the alpha for the observations - \n                # which is represented by ind\n                alpha.ind = .25,\n                # Modify the alpha for the variables (var):\n                alpha.var = .75,\n                # Modify the color of the variables\n                col.var = \"darkblue\")\n\n\n\n\n\n\n\n\nThe above plot tells us a lot of information:\n\nThe direction of a particular arrow is indicative of “as this variable increases….” For example, the far left arrow for caffeine_mg suggests that, as caffeine_mg increases, \\(Z_1\\) and \\(Z_2\\) tend to decrease (in other words, within the definition of \\(Z_1\\) and \\(Z_2\\), the coefficient for caffeine_mg is negative; this is verified below). You can contrast this with serv_size_m_l which is pointing to the upper right, indicating that as serv_size_m_l increases then both \\(Z_1\\) and \\(Z_2\\) tend to increase.\nThe angle of the different vectors is also indicative of the correlation between different variables. If two vectors are at a right angle (90 degrees), that suggests that they are uncorrelated, e.g., serv_size_m_l and saturated_fag_g. If two vectors are in similar directions (i.e., their angle is less than 90 degrees), that suggests that they are positively correlated, e.g., sugar_g and total_carbs_g. If two vectors are in different directions (i.e., their angle is greater than 90 degrees), that suggests that they are negatively correlated, e.g., caffeine_mg and calories.\nThe length of the lines also indicate how strongly related the principal components are with the individual variables. For example, serv_size_m_l has a fairly long line because it has a large positive coefficient for \\(Z_1\\) in the rotation matrix (see below). Meanwhile, caffeine_mg has a relatively short arrow because its coefficients are relatively small.\n\nFor reference, the below code shows the rotation matrix we used to create the \\(Z\\)s. You’ll see that the directions of the vectors in the above plot are the first two columns of this matrix.\n\nstarbucks_pca$rotation\n\n                        PC1         PC2         PC3          PC4         PC5\nserv_size_m_l    0.20078297  0.44103545  0.35053466 -0.117331692 -0.71633828\ncalories         0.39488151  0.10314156 -0.01048587  0.055814030  0.11487225\ntotal_fat_g      0.35254969 -0.31687231  0.06598414  0.046196797  0.07677253\nsaturated_fat_g  0.33929914 -0.30565133  0.05310592 -0.003731227  0.16145662\ntrans_fat_g      0.29974182 -0.39855899  0.01855869 -0.092804122 -0.35695525\ncholesterol_mg   0.33049434 -0.37077805  0.01219867 -0.105617624 -0.18815364\nsodium_mg        0.33573598  0.24647412 -0.09107538 -0.083512068  0.34969486\ntotal_carbs_g    0.34858318  0.34483762 -0.09623296  0.002842153  0.12386718\nfiber_g          0.11351848  0.04137855  0.17814948  0.956078124 -0.04719036\nsugar_g          0.34234584  0.35100839 -0.13314389 -0.109371714  0.12108189\ncaffeine_mg     -0.03085327 -0.01056235  0.89572768 -0.167846419  0.35265479\n                        PC6         PC7         PC8         PC9         PC10\nserv_size_m_l    0.30806678  0.13668394  0.04039275  0.01194522 -0.001076764\ncalories        -0.01331210 -0.18521073  0.09836135 -0.45551398  0.744248239\ntotal_fat_g      0.37698224 -0.03833030  0.03871096 -0.58859673 -0.518643989\nsaturated_fat_g  0.57285718 -0.06553378 -0.26369346  0.56257742  0.209355859\ntrans_fat_g     -0.50043224  0.15197176 -0.58086994 -0.05398876  0.032105721\ncholesterol_mg  -0.26449384 -0.04594580  0.74615325  0.27703165 -0.032124871\nsodium_mg       -0.06228905  0.82317144  0.06292570  0.04230447 -0.037304757\ntotal_carbs_g   -0.17619489 -0.34490217 -0.08588651  0.12501079 -0.148886253\nfiber_g         -0.11365528  0.06192955  0.01207815  0.10654914 -0.061378250\nsugar_g         -0.16729497 -0.33345131 -0.10758116  0.14408095 -0.321644156\ncaffeine_mg     -0.19600402 -0.06671121 -0.02122274  0.01530108 -0.020691492\n                         PC11\nserv_size_m_l    0.0053899973\ncalories        -0.1070327163\ntotal_fat_g      0.0489644534\nsaturated_fat_g -0.0152794817\ntrans_fat_g      0.0069417249\ncholesterol_mg  -0.0004710159\nsodium_mg        0.0185545403\ntotal_carbs_g    0.7347049650\nfiber_g         -0.0730283725\nsugar_g         -0.6635335478\ncaffeine_mg     -0.0094861578\n\n\nIn the above example, we plotted the first two principal components; thus, implicitly, we have chosen \\(k = 2\\), the only reason being that it is easy to visualize. However, how many principal components should we actually be using?"
  },
  {
    "objectID": "demos/08-pca.html#creating-and-interpreting-scree-plots",
    "href": "demos/08-pca.html#creating-and-interpreting-scree-plots",
    "title": "Demo 08: Principal Component Analysis",
    "section": "Creating and Interpreting Scree Plots",
    "text": "Creating and Interpreting Scree Plots\nThere is a common visual used to answer this question, but first let’s build some intuition. We already know that \\(Z_1\\) accounts for the most variation in our data, \\(Z_2\\) accounts for the next most, and so on. Thus, each time we add a new principal component dimension, we capture a “higher proportion of the information in the data,” but that increase in proportion decreases for each new dimension we add. (You may have to read those last two sentences a few times to get what I mean.) Thus, in practice, it is recommended to keep adding principal components until the marginal gain “levels off,” i.e., decreases to the point that it isn’t too beneficial to add another dimension to the data.\nThis trade-off between dimensions and marginal gain in information is often inspected visually using a scree plot, or what is more commonly known as an elbow plot. In an elbow plot, the x-axis has the numbers \\(1,2,\\dots,p\\) (i.e., the dimensions in the data), and the y-axis has the proportion of variation that the particular principal component \\(Z_j\\) accounts for. We can construct the scree plot using the fviz_screeplot() function from `factoextra\n\nfviz_eig(starbucks_pca, addlabels = TRUE) # Add the labels \n\n\n\n\n\n\n\n\nThe graphical rule-of-thumb is to then look for the “elbow,” i.e., where the proportion of variation starts to become flat. Unfortunately there is not a definitive “this is the elbow for sure” rule, and it is up to your judgment. Another useful rule-of-thumb is to consider drawing a horizontal line at 1 divided by the number of variables in your original dataset. Why do you think that is a useful rule? We easily do this because factoextra generates ggplot objects, so we can add another geometric layer corresponding to our reference:\n\nfviz_eig(starbucks_pca, addlabels = TRUE) +\n  # Have to multiply by 100 to get on percent scale\n  geom_hline(yintercept = 100 * (1 / ncol(starbucks_quant_data)),\n             linetype = \"dashed\", color = \"darkred\")\n\n\n\n\n\n\n\n\nBased on this plot, I think there’s a strong argument to stop at \\(k = 3\\) (but maybe go up to \\(k = 5\\) for another substantial drop in the elbow).\nLet’s say we decide \\(k = 3\\). This means that we should use the first three principal components in our graphics and other analyses in order for a “satisfactory” amount of the variation in the data to be captured. Our above visual only plots the first two principal components, and so we are “hiding” some data information that we are better off plotting in some way if possible (specifically, we are hiding about 30% of the information, i.e., the total amount of information captured by principal components 3, 4, …, 11, and about a third of this remaining information is captured by that third component that we are not plotting). This means that, theoretically, we should plot three quantitative variables, and we’ve discussed a bit about how to do this - you could use the size of points, transparency, or even a 3D scatterplot if you wanted to - but we are not going to explore that further here. Alternatively, you could just make three scatterplots (one for each pair of principal components).\nIf you’re having issues with the factoextra package then you can easily remake the scree plot manually. All we need to do is grab the proportion of variance explained by each component, turn it into a table, and then display it in some way. We already computed these values earlier in the demo, but we also just grab the singular values directly provided to us by R:\n\n# Manual creation of elbow plot, start by computing the eigenvalues and dividing by\n# the total variance in the data:\ntibble(prop_var = (starbucks_pca$sdev)^2 / ncol(starbucks_quant_data)) |&gt;\n  # Add a column for the PC index:\n  mutate(pc_index = 1:n()) |&gt;\n  # Now make the plot!\n  ggplot(aes(x = pc_index, y = prop_var)) +\n  geom_bar(stat = \"identity\", fill = \"blue\", alpha = 0.75) +\n  geom_point(color = \"black\", size = 2) +\n  geom_line() +\n  # Add the horizontal reference line:\n  geom_hline(yintercept = (1 / ncol(starbucks_quant_data)),\n             linetype = \"dashed\", color = \"darkred\") +\n  # And label:\n  labs(x = \"Dimensions\", y = \"Proportion of explained variance\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nMaking a biplot from scratch is much more difficult… Instead, you can try the ggfortify package (which is useful for making model diagnostic plots). The following code demonstrates how to do this (after you installed ggfortify):\n\n# install.packages(\"ggfortify\")\nlibrary(ggfortify)\n\nWarning: package 'ggfortify' was built under R version 4.2.3\n\nautoplot(starbucks_pca, \n         data = starbucks_quant_data,\n         alpha = 0.25,\n         loadings = TRUE, loadings.colour = 'darkblue',\n         loadings.label.colour = 'darkblue',\n         loadings.label = TRUE, loadings.label.size = 3,\n         loadings.label.repel = TRUE) +\n  theme_bw()"
  },
  {
    "objectID": "demos/14-text-data.html",
    "href": "demos/14-text-data.html",
    "title": "Demo 14: Tidy Text Data and Sentiment Analysis",
    "section": "",
    "text": "The Office Dinner Party script\nIn this demo we’ll work with the script from the best episode of ‘The Office’: Season 4, Episode 13 - ‘Dinner Party’. We can access the script using the schrute package (yes this is a real thing):\n\n# install.packages(\"schrute\")\nlibrary(tidyverse)\nlibrary(schrute)\n\n# Create a table from this package just corresponding to the Dinner Party episode:\ndinner_party_table &lt;- theoffice |&gt;\n  dplyr::filter(season == 4) |&gt;\n  dplyr::filter(episode == 13) |&gt;\n  # Just select columns of interest:\n  dplyr::select(index, character, text)\n\n\n\nTidyText processing of script\nIt’s very common to access data where a single column contains a long string of text, so that you have a long string of text in each row that you have to process. For instance, with the dinner_party_table dataset constructed above, the text column contains long strings which we can see by printing out the first so many rows:\n\nhead(dinner_party_table)\n\n# A tibble: 6 × 3\n  index character text                                                          \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;                                                         \n1 16791 Stanley   This is ridiculous.                                           \n2 16792 Phyllis   Do you have any idea what time we'll get out of here?         \n3 16793 Michael   Nobody likes to work late, least of all me. Do you have plans…\n4 16794 Jim       Nope I don't, remember when you told us not to make plans 'ca…\n5 16795 Michael   Yes I remember. Mmm, this is B.S. This is B.S. Why are we her…\n6 16796 Dwight    Thank you Michael.                                            \n\n\nThere are many ways to process text, but by far the simplest is to use the tidytext package which has a fantastic free online book demonstrating how to use it. Rather than working with document-term matrices directly like the tm package, instead we’ll consider a long, tidy table with one-token-per-document-per-row. Following the definition in the tidytext book:\n\nA token is a meaningful unit of text, most often a word, that we are interested in using for further analysis, and tokenization is the process of splitting text into tokens.\n\nWe can create this tidy dataset from our dinner_party_table using the unnest_tokens() function to break the various lines into tokens (aka tokenization). We specify what the new column will be called, word, given the source of the text, which is text here:\n\n# install.packages(\"tidytext\")\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.2.3\n\ntidy_dinner_party_tokens &lt;- dinner_party_table |&gt;\n  unnest_tokens(word, text)\n# View the first so many rows:\nhead(tidy_dinner_party_tokens)\n\n# A tibble: 6 × 3\n  index character word      \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     \n1 16791 Stanley   this      \n2 16791 Stanley   is        \n3 16791 Stanley   ridiculous\n4 16792 Phyllis   do        \n5 16792 Phyllis   you       \n6 16792 Phyllis   have      \n\n\nEach row of tidy_dinner_party_tokens corresponds to a single word spoken by a character (see character column) in the show during a single line (as reflected by the index column). We can see that the other columns are still in the data, but the text column has essentially been broken up into many rows. By default, punctuation marks are removed and all of the text is converted to lower case (which was helpful given the many lines in this episode that are in all caps!).\nBefore we continue to explore this dataset, we will remove stop words and perform stemming. First, we load a table of stop words from the tidytext package and simply filter them out from our data:\n\n# load stop words in the tidytext package\ndata(stop_words)\n\n# Next we can use the filter function to remove all stop words:\ntidy_dinner_party_tokens &lt;- tidy_dinner_party_tokens |&gt;\n  filter(!(word %in% stop_words$word))\n# Alternatively, we can do this with the anti_join function:\n# tidy_dinner_party_tokens &lt;- tidy_dinner_party_tokens |&gt;\n#   anti_join(stop_words)\n\nNext, we will use the SnowballC package to perform stemming:\n\n# install.packages(\"SnowballC\")\nlibrary(SnowballC)\n\ntidy_dinner_party_tokens &lt;- tidy_dinner_party_tokens |&gt;\n  # Create a new column stem with the wordStem function:\n  mutate(stem = wordStem(word))\n\n# View the update:\nhead(tidy_dinner_party_tokens)\n\n# A tibble: 6 × 4\n  index character word       stem   \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;  \n1 16791 Stanley   ridiculous ridicul\n2 16792 Phyllis   idea       idea   \n3 16792 Phyllis   time       time   \n4 16793 Michael   likes      like   \n5 16793 Michael   late       late   \n6 16793 Michael   plans      plan   \n\n\n\n\nFrequency and TF-IDF review\nFrom this dataset, we technically have everything we need to construct the wider document-term (or term-document) matrices. But in the current format, we can take advantage of the tidyverse syntax we’re used to with group_by() and summarize() to compute quantities we’re interested in the ways we want. For instance, we can compute the frequency of each stem across all spoken lines and characters then create a word cloud by just grabbing the appropriate columns.\nWord clouds are probably the most well-known way of visualizing text data. Word clouds show the most common words in a set of documents, with more common words being bigger in the word cloud. To make word clouds in R, you’ll need the wordcloud library. When you look at the help documentation for the wordcloud() function, you’ll see that there are two main arguments: words and freq, which correspond to the words in the documents and their frequencies, respectively.\n\n# First create a dataset counting the number of each stem across the full script\ntoken_summary &lt;- tidy_dinner_party_tokens |&gt;\n  group_by(stem) |&gt;\n  # Instead of summarize we can use the count function\n  count() |&gt;\n  ungroup() \n\n# Now make a wordcloud :\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nwordcloud(words = token_summary$stem, freq = token_summary$n)\n\n\n\n\n\n\n\n\nThere are a few things we can do to make the word cloud a bit prettier. First, it is more typical to place the biggest words in the center of the word cloud; this can be done by setting random.order = FALSE within wordcloud(). Furthermore, in the above word cloud we see that there are a bunch of tiny terms, and usually these are thrown out; you can control how many words are plotted on your word cloud by controlling max.words within wordcloud(). For example, we set max.words = 100, meaning the resulting word cloud displays the 100 most frequent words. We can also change the color palette; above you can see that loading the wordcloud library automatically loads the RColorBrewer library, and you can use functions like brewer.pal() like I do below to set the colors of the word cloud. There are lots of arguments within the wordcloud() function, so I encourage you to explore this function if you’re interested.\n\nwordcloud(words = token_summary$stem, freq = token_summary$n, \n          random.order = FALSE, max.words = 100, \n          colors = brewer.pal(8, \"Dark2\"))\n\n\n\n\n\n\n\n\nWhile could treat each spoken line as a separate document (index), instead we can focus on a collection of documents based on the characters speaking the lines (character). This corresponds to grouping by character instead of index. The following creates the token/stem count summary by character:\n\ncharacter_token_summary &lt;- tidy_dinner_party_tokens |&gt;\n  group_by(character, stem) |&gt;\n  # Instead of summarize we can use the count function\n  count() |&gt;\n  ungroup() \nhead(character_token_summary)\n\n# A tibble: 6 × 3\n  character stem       n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 All       cheer      1\n2 Andy      anim       1\n3 Andy      bet        1\n4 Andy      capit      1\n5 Andy      dinner     1\n6 Andy      flower     2\n\n\nWe can easily compute and join the TF-IDF values for each stem treating the different characters as documents with the bind_tf_idf() function. All this requires is three input columns referring to words, documents, and counts. TF-IDF will down-weigh words that occur frequently across all documents and will upweigh words that uniquely occur in certain documents. In our example, these are: stem, character, n:\n\ncharacter_token_summary &lt;- character_token_summary |&gt;\n  bind_tf_idf(stem, character, n)\nhead(character_token_summary)\n\n# A tibble: 6 × 6\n  character stem       n     tf   idf tf_idf\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 All       cheer      1 1      2.77  2.77  \n2 Andy      anim       1 0.0476 2.77  0.132 \n3 Andy      bet        1 0.0476 2.08  0.0990\n4 Andy      capit      1 0.0476 2.77  0.132 \n5 Andy      dinner     1 0.0476 0.981 0.0467\n6 Andy      flower     2 0.0952 2.77  0.264 \n\n\nNote that the tf column here refers to the term-frequency calculated as the number of times the character said that specific word/stem (by that character) divided by the total number of words/stems spoken by that character. The idf column refers to the inverse-document frequency column from lecture.\nWe can proceed to create a visual displaying the top 10 words/stems in terms of importance by TF-IDF for characters in the episode. For simplicity, we’ll just consider the four characters with the most lines in the episode based on the following:\n\nsort(table(dinner_party_table$character), decreasing = TRUE)\n\n\n    Michael         Jan         Jim         Pam      Dwight      Angela \n         89          73          38          28          16          12 \n       Andy       Woman   Officer 1   Officer 2         All Hunter's CD \n         10           5           3           2           1           1 \n     Michae     Officer     Phyllis     Stanley \n          1           1           1           1 \n\n\nThe following code chunk now displays the top 10 stems sorted by TF-IDF for Michael, Jan, Jim, and Pam. We use the reorder_within() function from the tidytext package to make sure the bars are sorted within the facets:\n\ncharacter_token_summary |&gt;\n  # Just grab the four characters of interest:\n  filter(character %in% c(\"Michael\", \"Jan\", \"Jim\", \"Pam\")) |&gt;\n  # Group by character and just use the top 10 based on tf_idf with slice_max\n  group_by(character) |&gt;\n  slice_max(tf_idf, n = 10, with_ties = FALSE) |&gt;\n  ungroup() |&gt;\n  mutate(stem = reorder_within(stem, tf_idf, character)) |&gt;\n  ggplot(aes(y = tf_idf, \n             # We use the fct_reorder to sort the bars within the facets\n             x = stem),\n         fill = \"darkblue\", alpha = 0.5) +\n  geom_col() +\n  coord_flip() +\n  scale_x_reordered() +\n  facet_wrap(~ character, ncol = 2, scales = \"free\") +\n  labs(y = \"TF-IDF\", x = NULL)\n\n\n\n\n\n\n\n\n\n\nSentiment Analysis\nThere are also techniques for measuring the overall “sentiment” of a document. These techniques are usually dictionary-based; for example, one can come up with a list of “positive” words and “negative” words (i.e., positive and negative dictionaries), and then count the number of positive and negative words that occur in a document. Thus, you can get an aggregate measure of how “positive” or “negative” a piece of text is. There are many different ways to define the “sentiment” of text (see the References and Resources section below); for this example, we’re just going to focus on “positive” versus “negative” text.\nAgain we’re going to focus on the episode’s script, and first, we’re going to process the data as we did before in the previous section. However, unlike the previous section, we are not going to remove stop words or perform stemming - for sentiment analysis, it’s important to keep individual words that may have certain types of sentiment.\n\ntidy_all_tokens &lt;- dinner_party_table |&gt;\n  unnest_tokens(word, text)\n\nNow: Which of these words are “positive” words and which of these words are “negative” words? There’s a function in the tidytext library called get_sentiments(). There are many different “sentiment dictionaries” available within this function, and we’re going to just focus on get_sentiments(\"bing\"), which provides a bunch of words that are deemed either “positive” or “negative” in a binary fashion:\n\nget_sentiments(\"bing\")\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows\n\n\nIn what follows, we’re going to join the binary sentiment assignment to the tidy token table - and only keep the words with an assignment based on the inner_join() function:\n\n# Label each word with a positive or negative sentiment\n# (if it has one)\ntidy_sentiment_tokens &lt;- tidy_all_tokens |&gt;\n  inner_join(get_sentiments(\"bing\"))\n# View the head of this:\nhead(tidy_sentiment_tokens)\n\n# A tibble: 6 × 4\n  index character word       sentiment\n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    \n1 16791 Stanley   ridiculous negative \n2 16793 Michael   likes      positive \n3 16793 Michael   work       positive \n4 16795 Michael   enough     positive \n5 16795 Michael   enough     positive \n6 16795 Michael   mad        negative \n\n\nSo now we can see how the characters vary in terms of “positive” or “negative” words. We can simply count up the total number of “positive” and “negative” words said by each character:\n\ncharacter_sentiment_summary &lt;- tidy_sentiment_tokens |&gt;\n  group_by(character, sentiment) |&gt;\n  summarize(n_words = n()) |&gt;\n  ungroup()\n\nWe can then display how the characters compare with stacked bar charts indicating how many positive and negative words were said by each character (ordering the characters by the total number of assigned words):\n\ncharacter_sentiment_summary |&gt;\n  group_by(character) |&gt;\n  mutate(total_assigned_words = sum(n_words)) |&gt;\n  ungroup() |&gt;\n  # Reorder the characters by the total number of words:\n  mutate(character = fct_reorder(character, total_assigned_words)) |&gt;\n  # Display stacked bars\n  ggplot(aes(x = character, y = n_words, fill = sentiment)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  scale_fill_manual(values = c(\"darkred\", \"darkblue\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFrom this, we can see the most characters seem to say more positive words than negative words (except for Angela…). But we can see how the proportions within characters appears to vary, e.g., Pam has a much higher proportion of positive words than Jim.\nIt’s also common to make word clouds of “positive” words and “negative” words, so that one can get an idea of the common words expressing “positive” or “negative” sentiment in a set of documents. Below, we separate words said by the two characters with the most dialogue in the episode, Michael and Jan, then separate out their positive and negative words to create word clouds.\n\n# First for Michael, make the positive summary:\nmichael_positive_summary &lt;- tidy_sentiment_tokens |&gt;\n  filter(character == \"Michael\", sentiment == \"positive\") |&gt;\n  group_by(word) |&gt;\n  count()\n# Now Michael's negative summary:\nmichael_negative_summary &lt;- tidy_sentiment_tokens |&gt;\n  filter(character == \"Michael\", sentiment == \"negative\") |&gt;\n  group_by(word) |&gt;\n  count()\n\n# Now make the word clouds for these two tables:\npar(mfrow = c(1,2))\n# word cloud for Michael's positive words\nwordcloud(words = michael_positive_summary$word,\n          freq = michael_positive_summary$n,\n          random.order = FALSE, color = \"darkblue\", \n          min.freq = 1)\ntitle(main = \"Michael's Positive Words\")\n# word cloud for Michael's negative words\nwordcloud(words = michael_negative_summary$word,\n          freq = michael_negative_summary$n,\n          random.order = FALSE, color = \"darkred\", \n          min.freq = 1)\ntitle(main = \"Michael's Negative Words\")\n\n\n\n\n\n\n\n# Next for Jan, make the positive summary:\njan_positive_summary &lt;- tidy_sentiment_tokens |&gt;\n  filter(character == \"Jan\", sentiment == \"positive\") |&gt;\n  group_by(word) |&gt;\n  count()\n# Now Jan's negative summary:\njan_negative_summary &lt;- tidy_sentiment_tokens |&gt;\n  filter(character == \"Jan\", sentiment == \"negative\") |&gt;\n  group_by(word) |&gt;\n  count()\n\n# Now make the word clouds for these two tables:\npar(mfrow = c(1,2))\n# word cloud for Jan's positive words\nwordcloud(words = jan_positive_summary$word,\n          freq = jan_positive_summary$n,\n          random.order = FALSE, color = \"darkblue\", \n          min.freq = 1)\ntitle(main = \"Jan's Positive Words\")\n# word cloud for Jan's negative words\nwordcloud(words = jan_negative_summary$word,\n          freq = jan_negative_summary$n,\n          random.order = FALSE, color = \"darkred\", \n          min.freq = 1)\ntitle(main = \"Jan's Negative Words\")\n\n\n\n\n\n\n\n\n\n\nMore resources\n\nMore on using the tidytext package\nTidy Text Mining with R\nSupervised Machine Learning for Text Analysis in R\nquanteda package"
  },
  {
    "objectID": "demos/05-heatmap-highdim.html",
    "href": "demos/05-heatmap-highdim.html",
    "title": "Demo 05: Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs.\nFor the first part of this demo we’ll use a dataset of shots by LeBron James. For people that are interested, this was constructed using the hoopR package.\nYou can read in the dataset with the following code:\n\nlibrary(tidyverse)\nlebron_shots &lt;- read_csv(\"https://raw.githubusercontent.com/ryurko/DataViz-Class-Data/main/lebron_shots.csv\")\n\n\n\nWe’re all experts in 1D kernel density estimation by now. Let’s move on to 2D kernel density estimation.\n\n\nHere let’s focus on plotting the joint distribution of coordinate_x and coordinate_y, which are both quantitative (i.e., observing the joint distribution of shots). We’re already very familiar with how to make scatterplots:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = .5)\n\n\n\n\n\n\n\n\nIt’s really easy to add a two-dimensional density (via contour lines) to the plot: we just use geom_density2d():\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = .5) +\n  geom_density2d()\n\n\n\n\n\n\n\n\nSimilar to the contour lines on a topological map, the inner lines denote the “peaks” of the density. Note that the contour lines won’t necessarily encapsulate every data point.\nWe can also plot the contour lines without the points if you’d like (see below), but this is a bit misleading, because it automatically throws out areas of the plot where there were points but the density was low. To see this, compare the plot below to the plot above.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_density2d()\n\n\n\n\n\n\n\n\n\n\n\nFor example, we can change the fill type, which gives two benefits: (1) It looks cooler, and (2) Now we can see what the actual density values are.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(level)), geom = \"polygon\") + \n  geom_point(alpha = .5) #+\n\n\n\n\n\n\n\n  #scale_fill_gradient(low = \"darkblue\", high = \"darkorange\")\n\nNote: To change the color, you can uncomment the code above. This uses the scale_fill_gradient() function, which we’ve seen before in previous homeworks.\nWe might also want to change the bandwidth. In 2D kernel density estimation, we must specify two bandwidths – one for the x-direction, one for the y-direction. We’ll see how to do this in homework.\n\n\n\nHeat maps: Divide the space into a grid and color the grid according to high/low values.\nTo do this with densities, include fill = after_stat(density), geom = \"tile\", contour = FALSE in your call to stat_density2d, as below:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(density)), geom = \"tile\",\n                 contour = FALSE) + \n  geom_point(alpha = .5)\n\n\n\n\n\n\n\n\nAgain, I recommend changing the default color scheme (it’s pretty awful…), as below:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(density)), geom = \"tile\",\n                 contour = FALSE) + \n  geom_point(alpha = .5) + \n  scale_fill_gradient(low = \"white\", high = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nWe make hexagonal heatmap plots using geom_hex(), can specify binwidth in both directions. This avoids limitations and issues with smoothing and challenges with multivariate density estimation. Note: You need to have the hexbin package installed prior to creating these visuals.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_hex() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nUnrelated to 2D density estimation and viewing the joint frequency of points, we can alternatively view some statistical summary within various hexagonal bins displayed on two axes of interest. For example, the following graph displays the percentage of shots made within each hexagonal bin. We do this by mapping as.numeric(scoring_play) to the z aesthetic (since scoring_play is a boolean TRUE/FALSE and as.numeric() converts it to 1/0) and using the stat_summary_hex() layer with a specified function via fun = mean.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y, \n             z = as.numeric(scoring_play))) +\n  stat_summary_hex(fun = mean) +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "demos/05-heatmap-highdim.html#d-density-estimates",
    "href": "demos/05-heatmap-highdim.html#d-density-estimates",
    "title": "Demo 05: Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "",
    "text": "We’re all experts in 1D kernel density estimation by now. Let’s move on to 2D kernel density estimation.\n\n\nHere let’s focus on plotting the joint distribution of coordinate_x and coordinate_y, which are both quantitative (i.e., observing the joint distribution of shots). We’re already very familiar with how to make scatterplots:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = .5)\n\n\n\n\n\n\n\n\nIt’s really easy to add a two-dimensional density (via contour lines) to the plot: we just use geom_density2d():\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = .5) +\n  geom_density2d()\n\n\n\n\n\n\n\n\nSimilar to the contour lines on a topological map, the inner lines denote the “peaks” of the density. Note that the contour lines won’t necessarily encapsulate every data point.\nWe can also plot the contour lines without the points if you’d like (see below), but this is a bit misleading, because it automatically throws out areas of the plot where there were points but the density was low. To see this, compare the plot below to the plot above.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_density2d()\n\n\n\n\n\n\n\n\n\n\n\nFor example, we can change the fill type, which gives two benefits: (1) It looks cooler, and (2) Now we can see what the actual density values are.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(level)), geom = \"polygon\") + \n  geom_point(alpha = .5) #+\n\n\n\n\n\n\n\n  #scale_fill_gradient(low = \"darkblue\", high = \"darkorange\")\n\nNote: To change the color, you can uncomment the code above. This uses the scale_fill_gradient() function, which we’ve seen before in previous homeworks.\nWe might also want to change the bandwidth. In 2D kernel density estimation, we must specify two bandwidths – one for the x-direction, one for the y-direction. We’ll see how to do this in homework.\n\n\n\nHeat maps: Divide the space into a grid and color the grid according to high/low values.\nTo do this with densities, include fill = after_stat(density), geom = \"tile\", contour = FALSE in your call to stat_density2d, as below:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(density)), geom = \"tile\",\n                 contour = FALSE) + \n  geom_point(alpha = .5)\n\n\n\n\n\n\n\n\nAgain, I recommend changing the default color scheme (it’s pretty awful…), as below:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(density)), geom = \"tile\",\n                 contour = FALSE) + \n  geom_point(alpha = .5) + \n  scale_fill_gradient(low = \"white\", high = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nWe make hexagonal heatmap plots using geom_hex(), can specify binwidth in both directions. This avoids limitations and issues with smoothing and challenges with multivariate density estimation. Note: You need to have the hexbin package installed prior to creating these visuals.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_hex() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "demos/05-heatmap-highdim.html#bonus-statistical-summaries-within-hexagonal-bins",
    "href": "demos/05-heatmap-highdim.html#bonus-statistical-summaries-within-hexagonal-bins",
    "title": "Demo 05: Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "",
    "text": "Unrelated to 2D density estimation and viewing the joint frequency of points, we can alternatively view some statistical summary within various hexagonal bins displayed on two axes of interest. For example, the following graph displays the percentage of shots made within each hexagonal bin. We do this by mapping as.numeric(scoring_play) to the z aesthetic (since scoring_play is a boolean TRUE/FALSE and as.numeric() converts it to 1/0) and using the stat_summary_hex() layer with a specified function via fun = mean.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y, \n             z = as.numeric(scoring_play))) +\n  stat_summary_hex(fun = mean) +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "demos/05-heatmap-highdim.html#correlograms-with-ggcorrplot",
    "href": "demos/05-heatmap-highdim.html#correlograms-with-ggcorrplot",
    "title": "Demo 05: Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Correlograms with ggcorrplot",
    "text": "Correlograms with ggcorrplot\nWe can visualize the correlation matrix for the variables in a dataset using the ggcorrplot package. You need to install the package:\n\ninstall.packages(\"ggcorrplot\")\n\nNext, we’ll load the package and create a correlogram using only the continuous variables. To do this, we first need to compute the correlation matrix for these variables:\n\npenguins_cor_matrix &lt;- penguins |&gt;\n  dplyr::select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) |&gt;\n  cor(use = \"complete.obs\")\npenguins_cor_matrix\n\n                  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\nbill_length_mm         1.0000000    -0.2350529         0.6561813   0.5951098\nbill_depth_mm         -0.2350529     1.0000000        -0.5838512  -0.4719156\nflipper_length_mm      0.6561813    -0.5838512         1.0000000   0.8712018\nbody_mass_g            0.5951098    -0.4719156         0.8712018   1.0000000\n\n\nNOTE: Since there are missing values in the penguins data we need to indicate in the cor() function how to handle missing values using the use argument. By default, the correlations are returned as NA, which is not what we want. Instead, we can change this to only use observations without NA values for the considered columns (see help(cor) for more options).\nNow, we can create the correlogram using ggcorrplot() using this correlation matrix:\n\nlibrary(ggcorrplot)\nggcorrplot(penguins_cor_matrix)\n\n\n\n\n\n\n\n\nThere are several ways we can improve this correlogram:\n\nwe can avoid redundancy by only using one half of matrix by changing the type input: the default is full, we can make it lower or upper instead:\n\n\nggcorrplot(penguins_cor_matrix, type = \"lower\")\n\n\n\n\n\n\n\n\n\nwe can rearrange the variables using hierarchical clustering so that variables displaying stronger levels of correlation are closer together along the diagonal by setting hc.order = TRUE:\n\n\nggcorrplot(penguins_cor_matrix, type = \"lower\", hc.order = TRUE)\n\n\n\n\n\n\n\n\n\nif we want to add the correlation values directly to the plot, we can include those labels setting lab = TRUE - but we should round the correlation values first using the round() function:\n\n\nggcorrplot(round(penguins_cor_matrix, digits = 4), \n           type = \"lower\", hc.order = TRUE, lab = TRUE)\n\n\n\n\n\n\n\n\n\nif we want to place more stress on the correlation magnitude, we can change the method input to circle so that the size of the displayed circles is mapped to the absolute value of the correlation value:\n\n\nggcorrplot(penguins_cor_matrix, type = \"lower\", hc.order = TRUE,\n           method = \"circle\")\n\n\n\n\n\n\n\n\nYou can ignore the Warning message that is displayed - just from the differences in ggplot implementation."
  },
  {
    "objectID": "demos/05-heatmap-highdim.html#parallel-coordinates-plot-with-ggally",
    "href": "demos/05-heatmap-highdim.html#parallel-coordinates-plot-with-ggally",
    "title": "Demo 05: Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Parallel coordinates plot with GGally",
    "text": "Parallel coordinates plot with GGally\nIn a parallel coordinates plot, we create an axis for each varaible and align these axes side-by-side, drawing lines between observations from one axis to the next. This can be useful for visualizing structure among both the variables and observations in our dataset. These are useful when working with a moderate number of observations and variables - but can be overwhelming with too many.\nWe use the ggparcoord() function from the GGally package to make parallel coordinates plots:\n\nlibrary(GGally)\n\nWarning: package 'GGally' was built under R version 4.2.3\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\npenguins |&gt;\n  ggparcoord(columns = 3:6)\n\n\n\n\n\n\n\n\nThere are several ways we can modify this parallel coordinates plot:\n\nwe should always adjust the transparency of the lines using the alphaLines input to help handle overlap:\n\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2)\n\n\n\n\n\n\n\n\n\nwe can color each observation’s lines by a categorical variable, which can be useful for revealing group structure:\n\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2, groupColumn = \"species\")\n\n\n\n\n\n\n\n\n\nwe can change how the y-axis is constructed by modifying the scale input, which by default is std that is simply subtracting the mean and dividing by the standard deviation. We could instead use uniminmax so that minimum of the variable is zero and the maximum is one:\n\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2, groupColumn = \"species\",\n             scale = \"uniminmax\")\n\n\n\n\n\n\n\n\n\nwe can also reorder the variables a number of different ways with the order input (see help(ggparcoord) for details). There appears to be some weird errors however with the different options, but you can still manually provide the order of indices as follows:\n\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2, groupColumn = \"species\",\n             order = c(6, 5, 3, 4))"
  },
  {
    "objectID": "demos/06-mds.html",
    "href": "demos/06-mds.html",
    "title": "Demo 06: Visualizing Distances for High-Dimensional Data",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs.\nThroughout this demo we use a dataset about Starbucks drinks available in the #TidyTuesday project.\nYou can read in and manipulate various columns in the dataset with the following code:\nlibrary(tidyverse)\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g),\n         fiber_g = as.numeric(fiber_g))"
  },
  {
    "objectID": "demos/06-mds.html#so-far",
    "href": "demos/06-mds.html#so-far",
    "title": "Demo 06: Visualizing Distances for High-Dimensional Data",
    "section": "So far…",
    "text": "So far…\nWe’ve been working with “tidy data” – data that has \\(n\\) rows and \\(p\\) columns, where each row is an observation, and each column is a variable describing some feature of each observation.\nNow we’ll discuss more complicated data structures."
  },
  {
    "objectID": "demos/06-mds.html#distance-matrices",
    "href": "demos/06-mds.html#distance-matrices",
    "title": "Demo 06: Visualizing Distances for High-Dimensional Data",
    "section": "Distance matrices",
    "text": "Distance matrices\nA distance matrix is a data structure that specifies the “distance” between each pair of observations in the original \\(n\\)-row, \\(p\\)-column dataset. For each pair of observations (e.g. \\(x_i, x_j\\)) in the original dataset, we compute the distance between those observations, denoted as \\(d(x_i, x_j)\\) or \\(d_{ij}\\) for short.\nA variety of approaches for calculating the distance between a pair of observations can be used. The most commonly used approach (when we have quantitative variables) is called “Euclidean Distance”. The Euclidean distance between observations \\(x_i\\) and \\(x_j\\) is defined as follows: \\(d(x_i, x_j) = \\sqrt{\\sum_{l = 1}^p (x_{i,l} - x_{j,l}) ^ 2}\\). That is, it is the square root of the sum of squared differences between each column (\\(l \\in \\{1, ..., p\\}\\)) of \\(x_i\\) and \\(x_j\\) (remember, there are \\(p\\) original columns / variables).\nNote that if some variables in our dataset have substantially higher variance than others, the high-variance variables will dominate the calculation of distance, skewing our resulting distances towards the differences in these variables. As such, it’s common to scale the original dataset before calculating the distance, so that each variable is on the same scale."
  },
  {
    "objectID": "demos/06-mds.html#starbucks-drinks-dataset",
    "href": "demos/06-mds.html#starbucks-drinks-dataset",
    "title": "Demo 06: Visualizing Distances for High-Dimensional Data",
    "section": "Starbucks drinks dataset",
    "text": "Starbucks drinks dataset\nIn this R demo we will look at Starbucks drinks (courtesy of the #TidyTuesday project). In short, this is a dataset containing nutritional information about Starbucks drinks. We’re going to consider all of the quantitative variables in this dataset, starting with fifth column serv_size_m_l to the final column caffeine_mg. You can read about the columns in the dataset here. After selecting the desired columns, the first thing we’re going to do is use the scale() function to ensure each variable on the same scale, i.e., variances are equal to 1.\n\n# Select the variables of interest:\nstarbucks_quant_data &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg)\n# Now scale each column so that the variance is 1 using the scale function:\n# We specify here to not center the data and need to follow the directions in\n# the help page of scale to ensure we are properly standardizing the variance\nstarbucks_scaled_quant_data &lt;- \n  scale(starbucks_quant_data, center = FALSE, \n        scale = apply(starbucks_quant_data,\n                      2, sd, na.rm = TRUE)) \n\n# Just for reference - this is equivalent to the following commented out code:\n# starbucks_quant_data &lt;- starbucks |&gt;\n#   dplyr::select(serv_size_m_l:caffeine_mg)\n# starbucks_scaled_quant_data &lt;- apply(starbucks_quant_data, MARGIN = 2,\n#                                      FUN = function(x) x / sd(x))\n\nThe most common way to compute distances in R is to use the dist function. This takes in a dataset and returns the distance matrix for that dataset. By default this computes the euclidean distance (method = \"euclidean\"), but other distance metrics can be used.\n\n# Calculate distance matrix.\n# As an example, we'll just look at the first five rows:\ndist(starbucks_scaled_quant_data[1:5,])\n\n         1        2        3        4\n2 1.059790                           \n3 2.160501 1.101588                  \n4 3.388562 2.331643 1.232345         \n5 1.472299 2.380300 3.425819 4.643997\n\n# You can also include the diagonal if you want:\n# (the diagonal will always be 0s)\ndist(starbucks_scaled_quant_data[1:5,], diag = T)\n\n         1        2        3        4        5\n1 0.000000                                    \n2 1.059790 0.000000                           \n3 2.160501 1.101588 0.000000                  \n4 3.388562 2.331643 1.232345 0.000000         \n5 1.472299 2.380300 3.425819 4.643997 0.000000\n\n# You can also include the \"upper triangle\" if you want:\ndist(starbucks_scaled_quant_data[1:5,], upper = T)\n\n         1        2        3        4        5\n1          1.059790 2.160501 3.388562 1.472299\n2 1.059790          1.101588 2.331643 2.380300\n3 2.160501 1.101588          1.232345 3.425819\n4 3.388562 2.331643 1.232345          4.643997\n5 1.472299 2.380300 3.425819 4.643997         \n\n# Can also include both:\n# (this is the full distance matrix)\ndist(starbucks_scaled_quant_data[1:5,], diag = T, upper = T)\n\n         1        2        3        4        5\n1 0.000000 1.059790 2.160501 3.388562 1.472299\n2 1.059790 0.000000 1.101588 2.331643 2.380300\n3 2.160501 1.101588 0.000000 1.232345 3.425819\n4 3.388562 2.331643 1.232345 0.000000 4.643997\n5 1.472299 2.380300 3.425819 4.643997 0.000000\n\n# Can also consider other distance metrics\n# The default is euclidean, as you can see below:\n# (compare to what you see above)\ndist(starbucks_scaled_quant_data[1:5,], method = \"euclidean\", diag = T, upper = T)\n\n         1        2        3        4        5\n1 0.000000 1.059790 2.160501 3.388562 1.472299\n2 1.059790 0.000000 1.101588 2.331643 2.380300\n3 2.160501 1.101588 0.000000 1.232345 3.425819\n4 3.388562 2.331643 1.232345 0.000000 4.643997\n5 1.472299 2.380300 3.425819 4.643997 0.000000\n\n# For example, can consider the Manhattan distance:\ndist(starbucks_scaled_quant_data[1:5,], method = \"manhattan\", diag = T, upper = T)\n\n         1        2        3        4        5\n1 0.000000 1.552865 3.109038 4.818573 1.472299\n2 1.552865 0.000000 1.556173 3.265708 3.025165\n3 3.109038 1.556173 0.000000 1.709535 4.581337\n4 4.818573 3.265708 1.709535 0.000000 6.290872\n5 1.472299 3.025165 4.581337 6.290872 0.000000\n\n\nFor the purposes of this class, we’ll mostly focus on the Euclidean distance, so let’s define that here:\n\ndist_euc &lt;- dist(starbucks_scaled_quant_data)"
  },
  {
    "objectID": "demos/06-mds.html#implementing-multi-dimensional-scaling",
    "href": "demos/06-mds.html#implementing-multi-dimensional-scaling",
    "title": "Demo 06: Visualizing Distances for High-Dimensional Data",
    "section": "Implementing multi-dimensional scaling",
    "text": "Implementing multi-dimensional scaling\nNow we will implement multi-dimensional scaling (MDS) in R. As a reminder, MDS tries to find the “best” \\(k\\)-dimensional projection of the original \\(p\\)-dimensional dataset (\\(k &lt; p\\)).\nAs such, MDS tries to preserve the order of the pairwise distances. That is, pairs of observations with low distances in the original \\(p\\)-column dataset will still be have low distances in the smaller \\(k\\)-column dataset. Similarly, pairs of observations with high distances in the original \\(p\\)-column dataset will still be have high distances in the smaller \\(k\\)-column dataset.\nMDS can be implemented in R using the cmdscale function. This function takes a distance matrix (not a dataset!!):\n\nstarbucks_mds &lt;- cmdscale(d = dist_euc, k = 2)\n\nNote that you can change \\(k\\) to be greater than 2 if you want, but usually we want \\(k = 2\\) so that we can plot the (projected) distances in a scatterplot; see below.\nFor the purposes of plotting, let’s add the two coordinates of mds to our original dataset:\n\nstarbucks &lt;- starbucks |&gt;\n  mutate(mds1 = starbucks_mds[,1], mds2 = starbucks_mds[,2])\n\nThen, we can make a plot with ggplot:\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\")\n\n\n\n\n\n\n\n\nIt can be helpful to add colors and/or shapes of the plot according to categorical variables. For example, here’s the plot colored by size:\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2, color = size)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\")\n\n\n\n\n\n\n\n\nTo get some insight into the contributions by the different continous variables, we could also map them to various aesthetics. For example, the following plots displays points colored by sugar_g:\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2, color = sugar_g)) +\n  geom_point(alpha = 0.5) +\n  scale_color_gradient(low = \"darkblue\", high = \"darkorange\") +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\")\n\n\n\n\n\n\n\n\nWhat do these two colored plots tell us about the data?"
  },
  {
    "objectID": "demos/11-anim-colors-themes.html",
    "href": "demos/11-anim-colors-themes.html",
    "title": "Demo 11: Animations, modifying colors and themes",
    "section": "",
    "text": "By far, the simplest way to create visualizations with animations is to use the gganimate package. This effectively works as an extension to ggplot figures but with the inclusion of various transition_* functions\n\n\nFirst, let’s think about when you should NOT animate a plot. We first create a visualization of the penguins data from before, of bill length on the y-axis against the body mass on the x-axis colored by species:\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\npenguins |&gt; \n  ggplot(aes(x = body_mass_g, y = bill_length_mm, color = species)) +\n  geom_point(alpha = 0.5, size = 2) +\n  labs(x = \"Body Mass (g)\", y = \"Bill Length (mm)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nNow, we could do the following: use the gganimate package to only display one species at a time with the transition_states() function:\n\nlibrary(gganimate)\npenguins |&gt; \n  ggplot(aes(x = body_mass_g, y = bill_length_mm, color = species)) +\n  geom_point(alpha = 0.5, size = 2) +\n  labs(x = \"Body Mass (g)\", y = \"Bill Length (mm)\") +\n  theme_bw() +\n  transition_states(species,\n                    transition_length = 0.5,\n                    state_length = 1)\n\n\n\n\n\n\n\n\nThe use of transition_length and state_length indicate how much relative time should take place when transitioning between states and the pause at each state, respectively. But the above use of animation is useless!\n\nSo when should you consider using animation?\nOne appropriate usage is in the context of storytelling with data, to emphasize some aspect of your visual display. For instance, we’ll borrow this F1 racing dataset from Meghan Hall’s data viz to compare the performance of three racing teams:\n\n# First load the data from Meghan's github\nf1_data_ex &lt;- read_csv('https://raw.githubusercontent.com/meghall06/CMU-36-315-site/main/data/constructor_pts.csv') |&gt;\n  filter(name %in% c(\"McLaren\", \"Renault\", \"Racing Point\"),\n         year == 2020)\n\n# Now display the results across the rounds:\nf1_data_ex |&gt;\n  ggplot(aes(x = round, y = points, group = name, color = name)) +\n  geom_line(size = 2) +\n  scale_x_continuous(breaks = seq(1, 17, 1)) +\n  labs(title = \"The race for third place in the 2020 F1 season\",\n       y = \"Accumulated points\", x = NULL) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFrom above we can see the accumulated points increasing over time for each team, with McLaren finishing better than both, Racing Point and Renault, at the end. But we could incrementally reveal the results at each stage emphasize the story of progression. We’re not adding another dimension to the display, but we emphasize the intermediate results through animation with the transition_reveal() function:\n\nf1_data_ex |&gt;\n  ggplot(aes(x = round, y = points, group = name, color = name)) +\n  geom_line(size = 2) +\n  scale_x_continuous(breaks = seq(1, 17, 1)) +\n  labs(title = \"The race for third place in the 2020 F1 season\",\n       y = \"Accumulated points\", x = NULL) +\n  theme_bw() +\n  # Reveal the results by round\n  transition_reveal(round)\n\n\n\n\n\n\n\n\nThe most effective use of animation is when it adds another dimension to your visualization, typically in the form of time. The previous visualization only animated across the x-axis - it did NOT add another variable in our data. However, animation can let us bring in another dimension so that we can see differences between relationships of variables in various ways. You should watch Hans Rosling’s 200 Countries, 200 Years, 4 Minutes to see one example in action. We can make similar visualizations with gganimate.\nIn the code chunk below, we’re going to display yearly summaries about housing sales in TX (dataset comes loaded with ggplot2). We’re going to plot the average number of active listings and average median sale price for each city-year combination in the data. For context, we’re going to highlight the data for Houston in red with a larger point size:\n\n# Load the scales package for better labeling of the axes\ntxhousing |&gt; \n  group_by(city, year) |&gt; \n  summarize(median = mean(median, na.rm = TRUE),\n            listings = mean(listings, na.rm = TRUE)) |&gt; \n  ggplot(aes(x = median, y = listings, \n             color = (city == \"Houston\"),\n             size = (city == \"Houston\"))) +\n  # Hide the legend for the point layer\n  geom_point(alpha = 0.5, show.legend = FALSE) +\n  # Manual color label\n  scale_color_manual(values = c(\"black\", \"darkred\")) +\n  # Manual size adjustment\n  scale_size_manual(values = c(2, 4)) +\n  scale_x_continuous(labels = scales::dollar, name = \"Median Price\") +\n  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale())) +\n  theme_bw() +\n  labs(x = \"Median Price\", y = \"Avg. of Monthly Listings\",\n       subtitle = \"Houston in red\")\n\n`summarise()` has grouped output by 'city'. You can override using the\n`.groups` argument.\n\n\nWarning: Removed 68 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIn the figure above we do not have year included in any way. But we can use the transition_time() function to animate the visual over time, while also updating the plot title to include the displayed year:\n\n# Load the scales package for better labeling of the axes\ntxhousing |&gt; \n  group_by(city, year) |&gt; \n  summarize(median = mean(median, na.rm = TRUE),\n            listings = mean(listings, na.rm = TRUE)) |&gt; \n  ggplot(aes(x = median, y = listings, \n             color = (city == \"Houston\"),\n             size = (city == \"Houston\"))) +\n  # Hide the legend for the point layer\n  geom_point(alpha = 0.5, show.legend = FALSE) +\n  # Manual color label\n  scale_color_manual(values = c(\"black\", \"darkred\")) +\n  # Manual size adjustment\n  scale_size_manual(values = c(2, 4)) +\n  scale_x_continuous(labels = scales::dollar, name = \"Median Price\") +\n  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale())) +\n  theme_bw() +\n  labs(x = \"Median Price\", y = \"Avg. of Monthly Listings\",\n       subtitle = \"Houston in red\", \n       title = \"Year: {frame_time}\") +\n  transition_time(year)\n\n\n\n\n\n\n\n\nFrom viewing the above visual, you can see how animation makes changes appear more dramatic between years - versus plotting each year separately with facets. We can then save the above animation as a GIF with the anim_save(\"INSERT/FILEPATH\") function, which will save the last animation you made by default.\n\nanim_save(\"examples/txhousing.gif\")\n\nSome key points to think about before adding animation to a visualization:\n\nAlways make and describe the original / base graphic first that does NOT include animation.\nBefore adding animation to the graph, ask yourself: How would animation give you additional insights about the data that you would otherwise not be able to?\nNever add animation just because it’s cool!\nWhen presenting, make sure you explain exactly what is being displayed with animation and what within the animation you want to emphasize. This will help you determine if animation is actually worth including."
  },
  {
    "objectID": "demos/11-anim-colors-themes.html#when-should-we-animate-plots",
    "href": "demos/11-anim-colors-themes.html#when-should-we-animate-plots",
    "title": "Demo 11: Animations, modifying colors and themes",
    "section": "",
    "text": "First, let’s think about when you should NOT animate a plot. We first create a visualization of the penguins data from before, of bill length on the y-axis against the body mass on the x-axis colored by species:\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\npenguins |&gt; \n  ggplot(aes(x = body_mass_g, y = bill_length_mm, color = species)) +\n  geom_point(alpha = 0.5, size = 2) +\n  labs(x = \"Body Mass (g)\", y = \"Bill Length (mm)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nNow, we could do the following: use the gganimate package to only display one species at a time with the transition_states() function:\n\nlibrary(gganimate)\npenguins |&gt; \n  ggplot(aes(x = body_mass_g, y = bill_length_mm, color = species)) +\n  geom_point(alpha = 0.5, size = 2) +\n  labs(x = \"Body Mass (g)\", y = \"Bill Length (mm)\") +\n  theme_bw() +\n  transition_states(species,\n                    transition_length = 0.5,\n                    state_length = 1)\n\n\n\n\n\n\n\n\nThe use of transition_length and state_length indicate how much relative time should take place when transitioning between states and the pause at each state, respectively. But the above use of animation is useless!\n\nSo when should you consider using animation?\nOne appropriate usage is in the context of storytelling with data, to emphasize some aspect of your visual display. For instance, we’ll borrow this F1 racing dataset from Meghan Hall’s data viz to compare the performance of three racing teams:\n\n# First load the data from Meghan's github\nf1_data_ex &lt;- read_csv('https://raw.githubusercontent.com/meghall06/CMU-36-315-site/main/data/constructor_pts.csv') |&gt;\n  filter(name %in% c(\"McLaren\", \"Renault\", \"Racing Point\"),\n         year == 2020)\n\n# Now display the results across the rounds:\nf1_data_ex |&gt;\n  ggplot(aes(x = round, y = points, group = name, color = name)) +\n  geom_line(size = 2) +\n  scale_x_continuous(breaks = seq(1, 17, 1)) +\n  labs(title = \"The race for third place in the 2020 F1 season\",\n       y = \"Accumulated points\", x = NULL) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFrom above we can see the accumulated points increasing over time for each team, with McLaren finishing better than both, Racing Point and Renault, at the end. But we could incrementally reveal the results at each stage emphasize the story of progression. We’re not adding another dimension to the display, but we emphasize the intermediate results through animation with the transition_reveal() function:\n\nf1_data_ex |&gt;\n  ggplot(aes(x = round, y = points, group = name, color = name)) +\n  geom_line(size = 2) +\n  scale_x_continuous(breaks = seq(1, 17, 1)) +\n  labs(title = \"The race for third place in the 2020 F1 season\",\n       y = \"Accumulated points\", x = NULL) +\n  theme_bw() +\n  # Reveal the results by round\n  transition_reveal(round)\n\n\n\n\n\n\n\n\nThe most effective use of animation is when it adds another dimension to your visualization, typically in the form of time. The previous visualization only animated across the x-axis - it did NOT add another variable in our data. However, animation can let us bring in another dimension so that we can see differences between relationships of variables in various ways. You should watch Hans Rosling’s 200 Countries, 200 Years, 4 Minutes to see one example in action. We can make similar visualizations with gganimate.\nIn the code chunk below, we’re going to display yearly summaries about housing sales in TX (dataset comes loaded with ggplot2). We’re going to plot the average number of active listings and average median sale price for each city-year combination in the data. For context, we’re going to highlight the data for Houston in red with a larger point size:\n\n# Load the scales package for better labeling of the axes\ntxhousing |&gt; \n  group_by(city, year) |&gt; \n  summarize(median = mean(median, na.rm = TRUE),\n            listings = mean(listings, na.rm = TRUE)) |&gt; \n  ggplot(aes(x = median, y = listings, \n             color = (city == \"Houston\"),\n             size = (city == \"Houston\"))) +\n  # Hide the legend for the point layer\n  geom_point(alpha = 0.5, show.legend = FALSE) +\n  # Manual color label\n  scale_color_manual(values = c(\"black\", \"darkred\")) +\n  # Manual size adjustment\n  scale_size_manual(values = c(2, 4)) +\n  scale_x_continuous(labels = scales::dollar, name = \"Median Price\") +\n  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale())) +\n  theme_bw() +\n  labs(x = \"Median Price\", y = \"Avg. of Monthly Listings\",\n       subtitle = \"Houston in red\")\n\n`summarise()` has grouped output by 'city'. You can override using the\n`.groups` argument.\n\n\nWarning: Removed 68 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIn the figure above we do not have year included in any way. But we can use the transition_time() function to animate the visual over time, while also updating the plot title to include the displayed year:\n\n# Load the scales package for better labeling of the axes\ntxhousing |&gt; \n  group_by(city, year) |&gt; \n  summarize(median = mean(median, na.rm = TRUE),\n            listings = mean(listings, na.rm = TRUE)) |&gt; \n  ggplot(aes(x = median, y = listings, \n             color = (city == \"Houston\"),\n             size = (city == \"Houston\"))) +\n  # Hide the legend for the point layer\n  geom_point(alpha = 0.5, show.legend = FALSE) +\n  # Manual color label\n  scale_color_manual(values = c(\"black\", \"darkred\")) +\n  # Manual size adjustment\n  scale_size_manual(values = c(2, 4)) +\n  scale_x_continuous(labels = scales::dollar, name = \"Median Price\") +\n  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale())) +\n  theme_bw() +\n  labs(x = \"Median Price\", y = \"Avg. of Monthly Listings\",\n       subtitle = \"Houston in red\", \n       title = \"Year: {frame_time}\") +\n  transition_time(year)\n\n\n\n\n\n\n\n\nFrom viewing the above visual, you can see how animation makes changes appear more dramatic between years - versus plotting each year separately with facets. We can then save the above animation as a GIF with the anim_save(\"INSERT/FILEPATH\") function, which will save the last animation you made by default.\n\nanim_save(\"examples/txhousing.gif\")\n\nSome key points to think about before adding animation to a visualization:\n\nAlways make and describe the original / base graphic first that does NOT include animation.\nBefore adding animation to the graph, ask yourself: How would animation give you additional insights about the data that you would otherwise not be able to?\nNever add animation just because it’s cool!\nWhen presenting, make sure you explain exactly what is being displayed with animation and what within the animation you want to emphasize. This will help you determine if animation is actually worth including."
  },
  {
    "objectID": "demos/11-anim-colors-themes.html#options-for-ggplot2-colors",
    "href": "demos/11-anim-colors-themes.html#options-for-ggplot2-colors",
    "title": "Demo 11: Animations, modifying colors and themes",
    "section": "Options for ggplot2 colors",
    "text": "Options for ggplot2 colors\nThe default color scheme is pretty bad to put it bluntly, but ggplot2 has ColorBrewer built in which makes it easy to customize your color scales. For instance, we change the palette for the species plot from before.\n\npenguins |&gt; \n  ggplot(aes(x = body_mass_g, y = bill_length_mm, color = species)) +\n  geom_point(alpha = 0.5, size = 2) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"Body Mass (g)\", y = \"Bill Length (mm)\") +\n  theme_bw()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nSomething you should keep in mind is to pick a color-blind friendly palette. One simple way to do this is by using the ggthemes package which has color-blind friendly palettes included:\n\npenguins |&gt; \n  ggplot(aes(x = body_mass_g, y = bill_length_mm, color = species)) +\n  geom_point(alpha = 0.5, size = 2) +\n  ggthemes::scale_color_colorblind() +\n  labs(x = \"Body Mass (g)\", y = \"Bill Length (mm)\") +\n  theme_bw()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIn terms of displaying color from low to high, the viridis scales are excellent choices (and are also color-blind friendly!).\n\npenguins |&gt; \n  ggplot(aes(x = body_mass_g, y = bill_length_mm, \n             color = flipper_length_mm)) +\n  geom_point(alpha = 0.5, size = 2) +\n  scale_color_viridis_c() +\n  labs(x = \"Body Mass (g)\", y = \"Bill Length (mm)\",\n       color = \"Flipper Length (mm)\") +\n  theme_bw()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/15-topic-models.html",
    "href": "demos/15-topic-models.html",
    "title": "Demo 15: Topic Models",
    "section": "",
    "text": "In this demo we’ll work with the dialogue from Stranger Things, which is available from TidyTuesday:\n\nlibrary(tidyverse)\nstranger_things_text &lt;-\n  read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-10-18/stranger_things_all_dialogue.csv') |&gt;\n  # Drop any lines with missing dialogue\n  filter(!is.na(dialogue))"
  },
  {
    "objectID": "demos/15-topic-models.html#latent-dirichlet-allocation",
    "href": "demos/15-topic-models.html#latent-dirichlet-allocation",
    "title": "Demo 15: Topic Models",
    "section": "Latent Dirichlet allocation",
    "text": "Latent Dirichlet allocation\nLatent Dirichlet Allocation (LDA) is one of the most common algorithms for topic modeling. Without diving into the math behind the model, we can understand it as being guided by two principles:\n\nEvery document is a mixture of topics: We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”\nEvery topic is a mixture of words: For example, we could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally.\n\nLDA is a mathematical method for estimating both of these at the same time: finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document. There are a number of existing implementations of this algorithm, and we’ll explore one of them in depth.\nThe most common R package for implementing topic models is called, unsurprisingly, topicmodels. However, the topicmodels package requires a DocumentTermMatrix object. To address this, we can use tidytext’s cast_dtm() function on our st_episode_word_summary table from before - treating each episode as a separate document:\n\nepisode_dtm &lt;- st_episode_word_summary |&gt;\n  # Use the words\n  cast_dtm(episode_id, word, n)\n\nepisode_dtm\n\n&lt;&lt;DocumentTermMatrix (documents: 34, terms: 7226)&gt;&gt;\nNon-/sparse entries: 22122/223562\nSparsity           : 91%\nMaximal term length: 27\nWeighting          : term frequency (tf)\n\n\nNotice that this summary output displays information about the matrix, reflecting the high sparsity (proportion of 0s) in the matrix at 91% - due to the high number of unique words/tokens (7226). We are being a bit lazy here, since numbers are included - but we could remove those if we wanted to.\nPerforming Latent Dirichlet Allocation (LDA) is actually really easy in R; we just use the LDA() function from the topicmodels package. For example, setting k = 2 creates a two-topic LDA model:\n\nlibrary(topicmodels)\n\nWarning: package 'topicmodels' was built under R version 4.2.3\n\n# set a seed so that the output of the model is predictable\nst_lda &lt;- LDA(episode_dtm, k = 2, control = list(seed = 1234))\nst_lda\n\nA LDA_VEM topic model with 2 topics.\n\n\nAlmost any topic model in practice will use a larger k; we’re just using k = 2 as an example. But there are a variety of approaches for tuning k that are beyond the scope of this class.\nFitting the model was the easy part. The rest of the analysis will involve exploring and interpreting the model. The main thing you need to know is that the LDA() function gives you:\n\nFor each document, the proportion of each topic (there will be k = 2 of these for each document/episode). These proportions will be called gamma.\nFor each topic, the probability of a certain word occurring (there are 7226 words, so there will be 7226 of these for each topic). These probabilities will be called beta.\n\nTypically, it’s much more interesting to look at the second item (the probability of certain words occurring within a given topic) so we are going to focus on visualizing that below. We’ll discuss the first item afterwards."
  },
  {
    "objectID": "demos/15-topic-models.html#word-topic-probabilities",
    "href": "demos/15-topic-models.html#word-topic-probabilities",
    "title": "Demo 15: Topic Models",
    "section": "Word-topic probabilities",
    "text": "Word-topic probabilities\nThe tidy() function within the tidytext package provides an easy way to extract the per-topic-per-word probabilities, called \\(\\beta\\) (“beta”), from the model.\n\nst_topics &lt;- tidy(st_lda, matrix = \"beta\")\nst_topics\n\n# A tibble: 14,452 × 3\n   topic term      beta\n   &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 00    8.62e- 4\n 2     2 00    6.96e- 4\n 3     1 10    2.40e- 4\n 4     2 10    4.64e- 5\n 5     1 100   3.32e- 4\n 6     2 100   9.72e- 5\n 7     1 12    2.38e- 4\n 8     2 12    4.89e- 5\n 9     1 12.3  4.81e- 5\n10     2 12.3  1.11e-90\n# ℹ 14,442 more rows\n\n# note that these probabilities sum up to 1 within a given topic:\nst_topics |&gt;\n  filter(topic == 1) |&gt;\n  pull(beta) |&gt;\n  sum()\n\n[1] 1\n\n\nThe above gives us a one-topic-per-term-per-row format. For each combination, the model computes the probability of that term being generated from that topic. For example, the token “00” has a \\(8.6214123\\times 10^{-4}\\) probability of being generated from topic 1, but a \\(6.9575687\\times 10^{-4}\\) probability of being generated from topic 2.\nWhat are the most prevalent words in each topic? In other words, which words have the largest beta within each topic? To answer this question, we can use the top_n() function within the dplyr package to find the 10 terms that are most common within each topic:\n\n# Grab the words with the top ten probabilities (betas), and then organize \n# the data by topic, decreasing by beta\nst_top_terms &lt;- st_topics |&gt;\n  group_by(topic) |&gt;\n  top_n(10, beta) |&gt;\n  ungroup() |&gt;\n  arrange(topic, -beta)\n\nst_top_terms\n\n# A tibble: 20 × 3\n   topic term     beta\n   &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1     1 yeah  0.0279 \n 2     1 hey   0.0173 \n 3     1 gonna 0.00976\n 4     1 uh    0.00967\n 5     1 shit  0.00926\n 6     1 mike  0.00767\n 7     1 mom   0.00721\n 8     1 time  0.00668\n 9     1 stop  0.00577\n10     1 god   0.00572\n11     2 yeah  0.0244 \n12     2 hey   0.0129 \n13     2 shit  0.0122 \n14     2 uh    0.0107 \n15     2 gonna 0.0104 \n16     2 max   0.00714\n17     2 time  0.00679\n18     2 eddie 0.00502\n19     2 stop  0.00499\n20     2 el    0.00457\n\n# Plot the data such that there is a plot for each topic, and the probabilities\n# are in decreasing order. There are many ways to do this, and this is just one:\nst_top_terms |&gt;\n  mutate(term = reorder_within(term, beta, topic)) |&gt;\n  ggplot(aes(term, beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  coord_flip() +\n  scale_x_reordered()\n\n\n\n\n\n\n\n\nThis visualization attempts to help us understand the two topics that were extracted from the episodes’ words. Obviously, there is noticeable overlap between these topics as both feature “yeah”, “hey”, “uh”, and other frequently used words across the show. This is an advantage of topic modeling as opposed to “hard clustering” methods: topics used in natural language could have some overlap in terms of words. (“Hard clustering” methods would force each word to be in a single topic, rather than letting there be overlap between topics.) But there are differences, such as “mom” and “mike” in Topic 1 versus “max” and “eddi” in Topic 2. This might hint at Topic 2 being associated with later seasons of the show - such as the rather long and verbose Season 4…\nNote that we can also make a word cloud for each topic, where the \\(\\beta\\)s act as the “frequency” for each word. As an example, the following code makes a word cloud for the first and second topics:\n\n# Subset into different topics\ntopic1 &lt;- st_topics |&gt;\n  filter(topic == 1)\ntopic2 &lt;- st_topics |&gt;\n  filter(topic == 2)\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\npar(mfrow = c(1, 2))\nwordcloud(words = topic1$term, freq = topic1$beta,\n          random.order = FALSE,\n          max.words = 100,\n          col = \"red\",\n          scale=c(2,.5))\ntitle(main = \"Topic 1\")\nwordcloud(words = topic2$term, freq = topic2$beta,\n          random.order = FALSE,\n          max.words = 100,\n          col = \"blue\", \n          scale=c(2,.5))\ntitle(main = \"Topic 2\")\n\n\n\n\n\n\n\n\nThe above visuals tell us which words are frequent within each topic, but they do not necessarily tell us what the most “important” words are in each topic. In other words: Are there words that occur frequently in one topic but not another topic?\nTo answer this question, let’s consider the terms that have the greatest difference in \\(\\beta\\) between Topic 1 and Topic 2. This can be estimated based on the log ratio of the two: \\(\\log_2(\\frac{\\beta_2}{\\beta_1})\\) (a log ratio is useful because it makes the difference symmetric: \\(\\beta_2\\) being twice as large leads to a log ratio of 1, while \\(\\beta_1\\) being twice as large results in -1). To constrain it to a set of especially relevant words, we can filter for relatively common words, such as those that have a \\(\\beta\\) greater than 1/1000 in at least one topic. (This is another threshold that is more an art than a science; in practice, you have to toy around with this threshold until you get a visual that seems to produce “conceptual separation” between the two topics.)\nTo get at this question, it’s useful to create a dataset where you have (for each word) the betas for each topic side by side, as well as their log-ratio. There are many ways to do this, but we opt to use the pivot_wider() function:\n\nbeta_spread &lt;- st_topics |&gt;\n  mutate(topic = paste0(\"topic\", topic)) |&gt;\n  pivot_wider(names_from = topic, values_from = beta) |&gt;\n  filter(topic1 &gt; .001 | topic2 &gt; .001) |&gt;\n  mutate(log_ratio = log2(topic2 / topic1)) |&gt;\n  arrange(log_ratio)\n\nbeta_spread\n\n# A tibble: 213 × 4\n   term        topic1    topic2 log_ratio\n   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 mama       0.00135 0.0000471     -4.84\n 2 suzie      0.00173 0.0000979     -4.14\n 3 sunflower  0.00101 0.0000922     -3.45\n 4 bob        0.00149 0.000231      -2.69\n 5 demogorgon 0.00105 0.000190      -2.47\n 6 breathe    0.00225 0.000429      -2.39\n 7 russians   0.00116 0.000225      -2.37\n 8 rainbow    0.00135 0.000279      -2.27\n 9 field      0.00109 0.000343      -1.66\n10 barb       0.00129 0.000428      -1.59\n# ℹ 203 more rows\n\n\nAfter creating such a dataset, it can be helpful to make a plot with the log-ratio on the x-axis and words on the y-axis. Again we use the top_n() function to plot just the top 10 log-ratios in either direction (positive or negative):\n\nbeta_spread |&gt;\n  group_by(direction = log_ratio &gt; 0) |&gt;\n  top_n(10, abs(log_ratio)) |&gt;\n  ungroup() |&gt;\n  mutate(term = reorder(term, log_ratio)) |&gt;\n  ggplot(aes(term, log_ratio, fill = direction)) +\n  geom_col(show.legend = FALSE) +\n  labs(y = \"Log2 ratio of beta in topic 2 / topic 1\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nThe way to interpret the above plot is that very positive values indicate that that word is relatively “important” for Topic 2, while very negative values indicate relative “importance” for Topic 1. We can see that the most important words in Topic 2 include words related to Season 4 of the show like “nina”, “yuri”, and “munson”. Topic 1 appears to be more characterized by words and names from the first three seasons like “barb” and “clark” (their teacher!). This type of analysis can help identify differences between topics."
  },
  {
    "objectID": "demos/15-topic-models.html#document-topic-probabilities",
    "href": "demos/15-topic-models.html#document-topic-probabilities",
    "title": "Demo 15: Topic Models",
    "section": "Document-topic probabilities",
    "text": "Document-topic probabilities\nBesides estimating each topic as a mixture of words, LDA also models each document as a mixture of topics. Similar to what we did in the previous section, we can examine the per-document-per-topic probabilities, called \\(\\gamma\\) (“gamma”), with the matrix = \"gamma\" argument to tidy():\n\nst_documents &lt;- tidy(st_lda, matrix = \"gamma\")\nst_documents\n\n# A tibble: 68 × 3\n   document topic     gamma\n   &lt;chr&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 1_1          1 1.00     \n 2 1_2          1 1.00     \n 3 1_3          1 0.976    \n 4 1_4          1 0.202    \n 5 1_5          1 0.0000384\n 6 1_6          1 1.00     \n 7 1_7          1 1.00     \n 8 1_8          1 1.00     \n 9 2_1          1 1.00     \n10 2_2          1 0.0000356\n# ℹ 58 more rows\n\n# for example, look at the probabilities for the first episode in each season:\nst_documents |&gt;\n  filter(document %in% c(\"1_1\", \"2_1\", \"3_1\", \"4_1\"))\n\n# A tibble: 8 × 3\n  document topic     gamma\n  &lt;chr&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 1_1          1 1.00     \n2 2_1          1 1.00     \n3 3_1          1 1.00     \n4 4_1          1 0.0000218\n5 1_1          2 0.0000337\n6 2_1          2 0.0000368\n7 3_1          2 0.0000400\n8 4_1          2 1.00     \n\n\nEach of these values is an estimated proportion of words from that document that are generated from that topic. For example, the model estimates that basically 100% of the words in the first episode for seasons 1-3 (1_1, 2_1, 3_1) were generated from Topic 1. In comparison, the first episode in season 4 (4_1) displays the opposite behavior with 100% of the words from Topic 2."
  },
  {
    "objectID": "demos/02-plot-2dquant.html",
    "href": "demos/02-plot-2dquant.html",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs. Throughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")"
  },
  {
    "objectID": "demos/02-plot-2dquant.html#colors",
    "href": "demos/02-plot-2dquant.html#colors",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Colors",
    "text": "Colors\nWe can color by a third variable (e.g., different color for each category).\nNote that, just like the x and y aesthetics, you can put color = inside ggplot or geom_point - both display the same visualization:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm, color = species)) +\n  geom_point(alpha = 0.5)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(color = species))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nYou can also color by a quantitative variable using a color scale/gradient:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(color = body_mass_g))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe default color gradient is not the most appealing, while there are a number of possibilities - blue to orange is a good choice since these colors are opposites on the color spectrum:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(color = body_mass_g)) +\n  scale_color_gradient(low = \"darkblue\", high = \"darkorange\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#point-size-size",
    "href": "demos/02-plot-2dquant.html#point-size-size",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Point size (size)",
    "text": "Point size (size)\nWe can also map variables to other aesthetics, e.g. size:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(size = body_mass_g))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#point-type-shape",
    "href": "demos/02-plot-2dquant.html#point-type-shape",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Point type (shape)",
    "text": "Point type (shape)\nOr the type (shape) of points:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(shape = species))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#combining-aesthetics",
    "href": "demos/02-plot-2dquant.html#combining-aesthetics",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Combining aesthetics",
    "text": "Combining aesthetics\nWe can even do several of these at once:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm,\n             color = species, shape = island,\n             size = body_mass_g)) +\n  geom_point(alpha = 0.5)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe above graph may be a bit difficult to read, but it contains a lot of information in the sense that it is a 5-dimensional graphic:\n\nx = bill depth (mm)\ny = bill length (mm)\ncolor = species\nsize = body mass (g)\nshape = island\n\n\nBut be careful!\nThe more complications you add, the more difficult your graph is to explain."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#linear-regression",
    "href": "demos/02-plot-2dquant.html#linear-regression",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Linear regression",
    "text": "Linear regression\nTo do this, we can use + geom_smooth(method = lm):\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n##Linear regression (with error bars)\nAbove, I added se = FALSE so that the standard error bars do not show up in the graph. Setting this parameter to TRUE produces (by default) 95% confidence intervals.\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = TRUE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nYou can change the level of the confidence intervals using the level argument:\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = TRUE, level = 0.99)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHowever, from these graphs, it’s not clear if the linear regression is a good fit. We can “eyeball” this by looking at the fitted versus residuals, or we can make a residual-versus-fit plot.\nWhat’s a residual-versus-fit plot? In short, the “fits” are the estimated y-values from the linear regression (i.e., the y-values along the linear regression line). Meanwhile, the “residuals” are the distance between the actual y-values and the fits. A residual-versus-fit plot is itself a scatterplot, with fits on the x-axis and residuals on the y-axis.\n\nlin_reg &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\nfitted_vals &lt;- fitted(lin_reg)\nres_vals &lt;- residuals(lin_reg)\ntibble(fits = fitted_vals, \n       residuals = res_vals) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\nWe look for two things when looking at residual-versus-fit plots:\n\nIs there any trend around the 0 horizontal line? If so, that might be a violation of the linearity assumption (more on this next class).\nDo the points have equal vertical spread from left to right? If not, that might be a violation of the equal variance assumption.\n\nWe’ll talk about these assumptions more next class."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#transformations-of-the-outcome",
    "href": "demos/02-plot-2dquant.html#transformations-of-the-outcome",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Transformations of the outcome",
    "text": "Transformations of the outcome\nWe can transform variables as well – again, within the plot. First, we will focus on the outcome y; in particular, we will focus on log transformations. This can be done through the y argument…\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = log(body_mass_g))) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = TRUE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#transformations-of-covariates",
    "href": "demos/02-plot-2dquant.html#transformations-of-covariates",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Transformations of Covariates",
    "text": "Transformations of Covariates\nIt’s also possible to include transformations of the covariates instead of (or in addition to) transformations of the outcome. For example, the following plots a quadratic regression model (i.e., plots y ~ x + x^2).\nNote that the formula argument in geom_smooth requires you to write in terms of y and x, NOT the variable names!\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", \n              formula = y ~ x + I(x^2))\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nTo assess if this is a better fit, we can again make a fitted-versus-residual plot (this looks better!):\n\nquad_lin_reg &lt;- lm(body_mass_g ~ flipper_length_mm + I(flipper_length_mm^2), \n                   data = penguins)\nquad_fitted_vals &lt;- fitted(quad_lin_reg)\nquad_res_vals &lt;- residuals(quad_lin_reg)\ntibble(fits = quad_fitted_vals, \n       residuals = quad_res_vals) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")"
  },
  {
    "objectID": "demos/02-plot-2dquant.html#non-linear-trends",
    "href": "demos/02-plot-2dquant.html#non-linear-trends",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Non-Linear Trends",
    "text": "Non-Linear Trends\nWe can also do other types of modeling, e.g. local regression / loess smoothing:\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"loess\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nCheck the help documentation for geom_smooth() and stat_smooth() to see what methods are available and how to use them. The most common choices are “lm”, “glm”, “gam”, and “loess”.\nNote that if you don’t put anything in geom_smooth, it will select “auto”, which typically uses loess for small datasets and gam for large datasets. However, it uses a particular form of smoothing splines, so in practice I recommend you specify a particular statistical method (e.g., “lm”, “loess”) so you actually know what you’re plotting."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#useful-for-residual-diagnostics",
    "href": "demos/02-plot-2dquant.html#useful-for-residual-diagnostics",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Useful for residual diagnostics",
    "text": "Useful for residual diagnostics\nConvenient to add + geom_smooth() to residual plots to help display any trends:\n\ntibble(fits = fitted_vals, \n       residuals = res_vals) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nwhich appears to be alleviated with the quadratic transformation:\n\ntibble(fits = quad_fitted_vals, \n       residuals = quad_res_vals) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "demos/02-plot-2dquant.html#leave-the-points-take-the-regression-model-this-is-a-bad-idea",
    "href": "demos/02-plot-2dquant.html#leave-the-points-take-the-regression-model-this-is-a-bad-idea",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Leave The Points, take The Regression Model? (This is a bad idea…)",
    "text": "Leave The Points, take The Regression Model? (This is a bad idea…)\nWe don’t even need to plot the points to do this – you can plot the regression model by itself by simply omitting geom_point():\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_smooth(method = \"loess\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\nAs always, you can adjust some parameters (like color, alpha, etc.):\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_smooth(method = \"loess\", se = TRUE, fill = \"darkorange\", \n              color = \"darkblue\", size = 2, alpha = 0.2) +\n  theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\nHowever, in general I don’t recommend doing this, because it hides the data entirely, making it unclear which data points are influencing the regression line."
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Lecture\nDate\nTitle\nMaterials\n\n\n\n\n1\nJan 13\nIntroduction and Grammar of Graphics\nslides\n\n\n2\nJan 15\n1D Categorical Data\nslides\n\n\n3\nJan 22\nStatistical Inference for 1D Categorical Data\nslides\n\n\n4\nJan 27\nPower and Multiple Testing\nslides\n\n\n5\nJan 29\nVisualizations and Inference for 2D Categorical Data\nslides\n\n\n6\nFeb 3\nVisualizing 1D Quantitative Data\nslides\n\n\n7\nFeb 5\nDensity Estimation\nslides\n\n\n8\nFeb 10\nGraphical Inference for 1D Quantitative Data\nslides\n\n\n9\nFeb 12\nComparing Distributions and Statistical Power\nslides\n\n\n10\nFeb 17\nScatterplots and Linear Regression\nslides\n\n\n11\nFeb 19\nInference with Linear Regression\nslides\n\n\n12\nFeb 24\nMidsemester Review\nslides\n\n\n13\nMar 10\nNonlinear Regression and Pairs Plots\nslides\n\n\n14\nMar 12\nContour Plots, Heat Maps, and Into High-Dimensional Data\nslides\n\n\n15\nMar 17\nVisualizing Distances for High-Dimensional Data\nslides\n\n\n16\nMar 19\nDendrograms for Visualizing Distances and Clusters\nslides\n\n\n17\nMar 24\nPrincipal Component Analysis\nslides\n\n\n18\nMar 26\nVisualizing Trends\nslides\n\n\n19\nMar 31\nTime Series, Autocorrelation, and Seasonal Decomposition\nslides\n\n\n20\nApr 2\nAnimations, infographics, and annotations\nslides\n\n\n21\nApr 7\nVisualizations and Inference for Spatial Data\nslides\n\n\n22\nApr 9\nVisualizations and Inference for Areal Data\nslides\n\n\n23\nApr 14\nVisualizing Text Data\nslides\n\n\n24\nApr 16\nSentiment Analysis and Topic Models\nslides",
    "crumbs": [
      "Lectures"
    ]
  },
  {
    "objectID": "lectures/10-2dquant.html#announcements-previously-and-today",
    "href": "lectures/10-2dquant.html#announcements-previously-and-today",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW4 is due Wednesday by 11:59 PM and you have Lab 6 again on Friday!\nTake-home exam is next week Wednesday Feb 26th\nHere’s how the exam will work:\n\nI’ll post the exam Monday evening, and it’s due Wednesday by 11:59 PM EDT (Feb 26th)\nExam will cover material from HW 1-4 and Labs 1-6\nQuestions will be similar to homeworks but more open-ended, e.g, instead of “make a side-by-side violin plot…” I’ll ask “Make a plot that compares the conditional distributions…”\nThere will NOT be class on Wednesday Feb 26th\nConflict Feb 26th? Let me know ASAP! Day-of accommodations will NOT be made, late submissions will NOT be accepted\n\n\n\n\nDiscussed power in the context of visualizations and statistical tests\nEven if there is a true effect, you may have limited power to detect it\nSeveral ways to formally compare distributions:\n\n\\(t\\)-test: Compare means\nBartlett’s test: Compare variances\nKS test: Compare distributions\n\n\n\n\n\nTODAY: 2D quantitative data, scatterplots, and linear regression"
  },
  {
    "objectID": "lectures/10-2dquant.html#d-quantitative-data",
    "href": "lectures/10-2dquant.html#d-quantitative-data",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "2D quantitative data",
    "text": "2D quantitative data\n\nWe’re working with two variables: \\((X, Y) \\in \\mathbb{R}^2\\), i.e., dataset with \\(n\\) rows and 2 columns\n\n\n\nGoals:\n\ndescribing the relationships between two variables\ndescribing the conditional distribution \\(Y | X\\) via regression analysis\ndescribing the joint distribution \\(X,Y\\) via contours, heatmaps, etc.\n\n\n\n\n\nFew big picture ideas to keep in mind:\n\nscatterplots are by far the most common visual\nregression analysis is by far the most popular analysis (you have a whole class on this…)\nrelationships may vary across other variables, e.g., categorical variables"
  },
  {
    "objectID": "lectures/10-2dquant.html#making-scatterplots-with-geom_point",
    "href": "lectures/10-2dquant.html#making-scatterplots-with-geom_point",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Making scatterplots with geom_point()",
    "text": "Making scatterplots with geom_point()\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()"
  },
  {
    "objectID": "lectures/10-2dquant.html#always-adjust-the-alpha",
    "href": "lectures/10-2dquant.html#always-adjust-the-alpha",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "ALWAYS adjust the alpha",
    "text": "ALWAYS adjust the alpha\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#map-categorical-variable-to-color",
    "href": "lectures/10-2dquant.html#map-categorical-variable-to-color",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Map categorical variable to color",
    "text": "Map categorical variable to color\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g,\n             color = species)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#map-continuous-variable-to-color",
    "href": "lectures/10-2dquant.html#map-continuous-variable-to-color",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Map continuous variable to color",
    "text": "Map continuous variable to color\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g,\n             color = body_mass_g)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#map-continuous-variable-to-color-1",
    "href": "lectures/10-2dquant.html#map-continuous-variable-to-color-1",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Map continuous variable to color",
    "text": "Map continuous variable to color\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g,\n             color = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  scale_color_gradient(low = \"darkblue\", high = \"darkorange\")"
  },
  {
    "objectID": "lectures/10-2dquant.html#map-continuous-variable-to-size",
    "href": "lectures/10-2dquant.html#map-continuous-variable-to-size",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Map continuous variable to size",
    "text": "Map continuous variable to size\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g,\n             size = body_mass_g)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#map-categorical-variable-to-shape",
    "href": "lectures/10-2dquant.html#map-categorical-variable-to-shape",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Map categorical variable to shape",
    "text": "Map categorical variable to shape\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g,\n             shape = species)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#all-at-once",
    "href": "lectures/10-2dquant.html#all-at-once",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "ALL AT ONCE!",
    "text": "ALL AT ONCE!\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm,\n             color = species, shape = island, size = body_mass_g)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#displaying-trend-lines-linear-regression",
    "href": "lectures/10-2dquant.html#displaying-trend-lines-linear-regression",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Displaying trend lines: linear regression",
    "text": "Displaying trend lines: linear regression\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "lectures/10-2dquant.html#setup-and-motivation-for-linear-regression",
    "href": "lectures/10-2dquant.html#setup-and-motivation-for-linear-regression",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Setup and motivation for linear regression",
    "text": "Setup and motivation for linear regression\n\nConsider an outcome \\(Y \\in \\mathbb{R}\\) and covariate \\(X \\in \\mathbb{R}\\)\n\nWe have \\(n\\) observations: \\((X_1, Y_1), \\dots, (X_n, Y_n)\\)\n\nPurpose of regression is to model \\(\\mathbb{E}[Y | X]\\)\nConsider the case where \\(X\\) takes on discrete values \\(c_1, \\dots, c_k\\)\nThen most straightforward way to estimate \\(\\mathbb{E}[Y | X = c_j]\\) is to use the sample mean for subgroup \\(X_i = c_j\\):\n\\[\\hat{\\mathbb{E}}[Y|X = c_j] = \\frac{1}{N_j} \\sum_{i: X_i = c_j} Y_i\\]\n\nGraphs like side-by-side violin plots, facetted histograms, and overlaid density plots essentially compare \\(\\hat{\\mathbb{E}}[Y|X = c_j]\\) for different categories\n\nBut when \\(X\\) is quantitative, what do we do?\n\nUse statistical model to “guess” \\(\\mathbb{E}[Y|X = x]\\), even when we don’t observe \\(X = x\\)"
  },
  {
    "objectID": "lectures/10-2dquant.html#statistical-model-for-linear-regression",
    "href": "lectures/10-2dquant.html#statistical-model-for-linear-regression",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Statistical Model for Linear Regression",
    "text": "Statistical Model for Linear Regression\nLinear regression assumes \\(Y_i \\overset{iid}{\\sim} N(\\beta_0 + \\beta_1 X_i, \\sigma^2)\\)\n\n\\(\\beta_0\\): intercept - population mean outcome when \\(X = 0\\); i.e., \\(\\mathbb{E}[Y | X = 0]\\)\n\\(\\beta_1\\): slope - population mean change in \\(Y\\) when \\(X\\) increases by 1\n\\(\\beta_0\\) and \\(\\beta_1\\) are parameters that must be estimated"
  },
  {
    "objectID": "lectures/10-2dquant.html#assessing-assumptions-of-linear-regression",
    "href": "lectures/10-2dquant.html#assessing-assumptions-of-linear-regression",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Assessing assumptions of linear regression",
    "text": "Assessing assumptions of linear regression\nLinear regression assumes \\(Y_i \\overset{iid}{\\sim} N(\\beta_0 + \\beta_1 X_i, \\sigma^2)\\)\n\nIf this is true, then \\(Y_i - \\hat{Y}_i \\overset{iid}{\\sim} N(0, \\sigma^2)\\)\n\n\nPlot residuals against \\(\\hat{Y}_i\\), residuals vs fit plot\n\nUsed to assess linearity, any divergence from mean 0\nUsed to assess equal variance, i.e., if \\(\\sigma^2\\) is homogenous across predictions/fits \\(\\hat{Y}_i\\)\n\n\n\nMore difficult to assess the independence and fixed \\(X\\) assumptions\n\nMake these assumptions based on subject-matter knowledge"
  },
  {
    "objectID": "lectures/10-2dquant.html#residual-vs-fit-plots",
    "href": "lectures/10-2dquant.html#residual-vs-fit-plots",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Residual vs fit plots",
    "text": "Residual vs fit plots\n\nlin_reg &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\n\ntibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")"
  },
  {
    "objectID": "lectures/10-2dquant.html#residual-vs-fit-plots-1",
    "href": "lectures/10-2dquant.html#residual-vs-fit-plots-1",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Residual vs fit plots",
    "text": "Residual vs fit plots\n\ntibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth()"
  },
  {
    "objectID": "lectures/10-2dquant.html#examples-of-residual-vs-fit-plots",
    "href": "lectures/10-2dquant.html#examples-of-residual-vs-fit-plots",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Examples of Residual-vs-Fit Plots",
    "text": "Examples of Residual-vs-Fit Plots"
  },
  {
    "objectID": "lectures/10-2dquant.html#recap-and-next-steps",
    "href": "lectures/10-2dquant.html#recap-and-next-steps",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nScatterplots are the most common visual for 2D quantitative variables\n\nMany ways to incorporate additional dimensions in scatterplots, e.g., color and shape\n\nLinear regression is by far the most common model for describing the relationship between 2+ quantitative variables\n\nCan also: transform the outcome, transform the covariates, do nonparametric “smoothing”\n\nUse graphs to assess linear regression assumptions, i.e., residual diagnostic plots\n\n\n\n\nHW4 due Wednesday and you have Lab 6 on Friday\nGraphics critique due Feb 28th!\nNext time: Inference with Linear Regression\nRecommended reading: CW Chapter 12 Visualizing associations among two or more quantitative variables"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#announcements-previously-and-today",
    "href": "lectures/09-compare-distr-power.html#announcements-previously-and-today",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW3 is due TONIGHT by 11:59 PM and you have Lab 5 again on Friday!\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Tuesdays @ 11 AM\n\n\n\n\nFinished discussed density based visualizations\nIntroduced KS test for testing if distribution follows a particular distribution\nGraphics are extremely useful because human eyes can quickly compare and contrast distributions…\n\n\n\n\nTODAY:\n\nUnderstanding the statistical power of tests and graphics"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#flipper-length-example",
    "href": "lectures/09-compare-distr-power.html#flipper-length-example",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Flipper length example",
    "text": "Flipper length example"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#statistical-tests-for-comparing-distributions",
    "href": "lectures/09-compare-distr-power.html#statistical-tests-for-comparing-distributions",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Statistical Tests for Comparing Distributions",
    "text": "Statistical Tests for Comparing Distributions\n\n\nWe’ve focused on assessing if a single quantitative variable follows a particular distribution\n\nLogic of one-sample KS test: Compare empirical distribution to theoretical distribution\n\n\n\n\n\nHow do we compare multiple empirical distributions?\nVery common scenario: Determine if a quantitative variable depends on a categorical variable, examples:\n\nClinical trials with multiple treatments\nAssessing differences across race, gender, socioeconomic status\nIndustrial experiments, A/B testing\nComparing song duration across different genres?\n\nCan use overlayed densities, side-by-side violin plots, facetted histograms\nRemember: plotting conditional distributions… but when are differences in a graphic statistically significant?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre",
    "href": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre",
    "title": "Comparing Distributions and Statistical Power",
    "section": "TidyTuesday Spotify Songs - Duration by Genre",
    "text": "TidyTuesday Spotify Songs - Duration by Genre"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre-1",
    "href": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "TidyTuesday Spotify Songs - Duration by Genre",
    "text": "TidyTuesday Spotify Songs - Duration by Genre"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#kolmogorov-smirnov-test-again",
    "href": "lectures/09-compare-distr-power.html#kolmogorov-smirnov-test-again",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Kolmogorov-Smirnov Test… Again",
    "text": "Kolmogorov-Smirnov Test… Again\nKS test can also be used to compare two empirical distributions \\(\\hat{F}_A(x)\\) and \\(\\hat{F}_B\\), via test statistic for two samples:\n\\[\n\\text{max}_x |\\hat{F}_A(x) - \\hat{F}_B(x)|\n\\]\n\nspotify_songs &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')\nrap_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"rap\") |&gt; pull(duration_ms)\nrock_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"rock\") |&gt; pull(duration_ms)\n\nks.test(rap_duration, y = rock_duration)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  rap_duration and rock_duration\nD = 0.22386, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#kolmogorov-smirnov-test-again-1",
    "href": "lectures/09-compare-distr-power.html#kolmogorov-smirnov-test-again-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Kolmogorov-Smirnov Test… Again",
    "text": "Kolmogorov-Smirnov Test… Again\nKS test can also be used to compare two empirical distributions \\(\\hat{F}_A(x)\\) and \\(\\hat{F}_B\\), via test statistic for two samples:\n\\[\n\\text{max}_x |\\hat{F}_A(x) - \\hat{F}_B(x)|\n\\]"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#statistical-tests-for-comparing-distributions-1",
    "href": "lectures/09-compare-distr-power.html#statistical-tests-for-comparing-distributions-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Statistical Tests for Comparing Distributions",
    "text": "Statistical Tests for Comparing Distributions\n\nInfinite number of ways that you can compare multiple quantitative distributions, 3 common ways:\n\nAny difference at all? \nDifference in means?\n\n\nNull hypothesis: \\(H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_K\\) (use t.test or oneway.test() functions)\nCan assume the variances are all the same or differ\nIf reject, can only conclude not all means are equal\n\n\nDifference in variances?\n\n\nNull hypothesis: \\(H_0: \\sigma^2_1 = \\sigma^2_2 = \\cdots = \\sigma^2_K\\) (use bartlett.test() function)\nIf reject, can only conclude not all variances are equal\n\nUnlike the KS test, difference in means and variances are sensitive to non-Normality\n\nDifferent distributions can yield insignificant results"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock-1",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock-2",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock-2",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-pop-and-rap",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-pop-and-rap",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between pop and rap?",
    "text": "Test difference between pop and rap?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#recap-one-sample-ks-test",
    "href": "lectures/09-compare-distr-power.html#recap-one-sample-ks-test",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Recap: One-Sample KS Test",
    "text": "Recap: One-Sample KS Test\n\n\nHave a single sample \\(\\mathbf{X} = (X_1,\\dots,X_n)\\)\nWant to test: Does \\(\\mathbf{X}\\) follow a particular distribution?\nCompares the empirical CDF of \\(\\mathbf{X}\\) to the theoretical CDF of a particular distribution:\n\n\\[\\underbrace{F(x) = P(X \\leq x)}_{\\text{theoretical CDF}}, \\hspace{0.2in} \\underbrace{\\hat{F}(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(X_i \\leq x)}_{\\text{empirical CDF}}\\]\n\nNull hypothesis: \\(\\mathbf{X}\\) follows a distribution with CDF \\(F(x)\\)\nAlternative hypothesis: \\(\\mathbf{X}\\) does not follow this distribution\nTest statistic: \\(\\max_x |\\hat{F}(x) - F(x)|\\)\nIf \\(\\hat{F}(x)\\) is far away from \\(F(x)\\) \\(\\rightarrow\\) reject null"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#recap-two-sample-ks-test",
    "href": "lectures/09-compare-distr-power.html#recap-two-sample-ks-test",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Recap: Two-Sample KS Test",
    "text": "Recap: Two-Sample KS Test\n\n\nHave two samples \\(\\mathbf{X} = (X_1,\\dots,X_m)\\), \\(\\mathbf{Y} = (Y_1,\\dots,Y_n)\\)\nWant to test: Do \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) follow the same distribution?\nCompares the empirical CDFs of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\):\n\n\\[\\underbrace{\\hat{F}_X(z) = \\frac{1}{m} \\sum_{i=1}^m \\mathbb{I}(X_i \\leq z)}_{\\text{empirical CDF of } \\mathbf{X}} \\hspace{0.2in} \\underbrace{\\hat{F}_Y(z) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(Y_i \\leq z)}_{\\text{empirical CDF of } \\mathbf{Y}}\\]\n\nNull hypothesis: \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) follow the same distribution.\nAlternative hypothesis: \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) do not follow the same distribution\nTest statistic: \\(\\max_z |\\hat{F}_X(z) - \\hat{F}_Y(z)|\\)\nIf \\(\\hat{F}_X\\) and \\(\\hat{F}_Y\\) are far away from each other \\(\\rightarrow\\) reject null"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre-2",
    "href": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre-2",
    "title": "Comparing Distributions and Statistical Power",
    "section": "TidyTuesday Spotify Songs - Duration by Genre",
    "text": "TidyTuesday Spotify Songs - Duration by Genre"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#what-about-the-difference-between-pop-and-rap",
    "href": "lectures/09-compare-distr-power.html#what-about-the-difference-between-pop-and-rap",
    "title": "Comparing Distributions and Statistical Power",
    "section": "What about the difference between pop and rap?",
    "text": "What about the difference between pop and rap?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#what-about-the-difference-between-pop-and-rap-1",
    "href": "lectures/09-compare-distr-power.html#what-about-the-difference-between-pop-and-rap-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "What about the difference between pop and rap?",
    "text": "What about the difference between pop and rap?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#significant-difference-with-large-sample-size",
    "href": "lectures/09-compare-distr-power.html#significant-difference-with-large-sample-size",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Significant difference with large sample size",
    "text": "Significant difference with large sample size\n\nspotify_songs &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')\ntable(spotify_songs$playlist_genre)\n\n\n  edm latin   pop   r&b   rap  rock \n 6043  5155  5507  5431  5746  4951 \n\nrap_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"rap\") |&gt; pull(duration_ms)\npop_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"pop\") |&gt; pull(duration_ms)\n\nks.test(rap_duration, y = pop_duration)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  rap_duration and pop_duration\nD = 0.14569, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#what-happens-if-we-had-a-smaller-sample",
    "href": "lectures/09-compare-distr-power.html#what-happens-if-we-had-a-smaller-sample",
    "title": "Comparing Distributions and Statistical Power",
    "section": "What happens if we had a smaller sample?",
    "text": "What happens if we had a smaller sample?\n\nset.seed(2017)\nsample_songs &lt;- spotify_songs |&gt;\n  group_by(playlist_genre) |&gt; \n  slice_sample(n = 100)\n\ntable(sample_songs$playlist_genre)\n\n\n  edm latin   pop   r&b   rap  rock \n  100   100   100   100   100   100 \n\nsample_rap_duration &lt;- sample_songs |&gt; filter(playlist_genre == \"rap\") |&gt; pull(duration_ms)\nsample_pop_duration &lt;- sample_songs |&gt; filter(playlist_genre == \"pop\") |&gt; pull(duration_ms)\n\nks.test(sample_rap_duration, y = sample_pop_duration)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  sample_rap_duration and sample_pop_duration\nD = 0.16, p-value = 0.1545\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#but-it-still-looks-different",
    "href": "lectures/09-compare-distr-power.html#but-it-still-looks-different",
    "title": "Comparing Distributions and Statistical Power",
    "section": "But it still looks different???",
    "text": "But it still looks different???"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-means-and-variances",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-means-and-variances",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between means and variances?",
    "text": "Test difference between means and variances?\nCan test difference in means using t.test():\n\nt.test(sample_rap_duration, sample_pop_duration)\n\n\n    Welch Two Sample t-test\n\ndata:  sample_rap_duration and sample_pop_duration\nt = 0.83091, df = 172.78, p-value = 0.4072\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -8766.158 21512.638\nsample estimates:\nmean of x mean of y \n 221645.7  215272.5"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-means-and-variances-1",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-means-and-variances-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between means and variances?",
    "text": "Test difference between means and variances?\nCan test difference in variances using bartlett.test():\n\nbartlett.test(list(sample_rap_duration, sample_pop_duration))\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  list(sample_rap_duration, sample_pop_duration)\nBartlett's K-squared = 15.54, df = 1, p-value = 8.08e-05\n\n\nRejects at \\(\\alpha = 0.05\\) even with this smaller sample size!\n\n\nWhy did the KS test say they weren’t different when the graph were clearly different? Two possible reasons:\n\nThe sample size might be too small to detect a difference\nThe KS test is known to have low power"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#statistical-power",
    "href": "lectures/09-compare-distr-power.html#statistical-power",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Statistical power",
    "text": "Statistical power\nStatistical power is key to really understanding graphics - you need to know when you’re looking at real effects versus noise\nHere are two definitions of power (one in English, one in math):\n\nEnglish: The probability that we reject the null hypothesis when the null hypothesis is false.\nMath: \\(P(\\text{p-value} \\leq \\alpha | H_0\\) is false)"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#toy-example-for-understanding-statistical-power",
    "href": "lectures/09-compare-distr-power.html#toy-example-for-understanding-statistical-power",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Toy example for understanding statistical power",
    "text": "Toy example for understanding statistical power\n\nConsider two samples:\n\\[(X_1,\\dots,X_n) \\sim N(0, 1)\\] \\[(Y_1,\\dots,Y_n) \\sim N(\\delta, 1)\\]\nLet’s say we use t.test(x, y)\nWe’ll simulate \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) 1000 times for some \\(n\\) and \\(\\delta &gt; 0\\)\nWe’ll count the number of times we reject\n\\[\\text{Power} = P(\\text{p-value} \\leq \\alpha | H_0 \\text{ false}) \\\\\n            = P(\\text{p-value} \\leq \\alpha | \\delta &gt; 0) \\\\\n            \\approx \\frac{\\text{# times reject}}{1000}\\]\nWe’ll consider \\(n = 10, 20, \\dots, 1000\\) and \\(\\delta = 0.1\\) or \\(\\delta = 0.25\\)"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#toy-example-power-of-t-test",
    "href": "lectures/09-compare-distr-power.html#toy-example-power-of-t-test",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Toy example: power of \\(t\\)-test",
    "text": "Toy example: power of \\(t\\)-test"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#another-toy-example",
    "href": "lectures/09-compare-distr-power.html#another-toy-example",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Another toy example",
    "text": "Another toy example\nConsider two samples:\n\\[(X_1,\\dots,X_n) \\sim N(0, 1)\\] \\[(Y_1,\\dots,Y_n) \\sim N(0, 1.5)\\]\nLet’s consider three ways to test differences:\n\nt.test(x, y)\nbartlett.test(list(x, y))\nks.test(x,y)\n\nWe’ll simulate \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) 1000 times for samples sizes \\(n = 10, 20, \\dots, 1000\\)\nWhat do you think the power curves will look like for these methods?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#comparison-of-power-for-the-different-tests",
    "href": "lectures/09-compare-distr-power.html#comparison-of-power-for-the-different-tests",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Comparison of power for the different tests",
    "text": "Comparison of power for the different tests"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#recap-and-next-steps",
    "href": "lectures/09-compare-distr-power.html#recap-and-next-steps",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nGraphics should be paired with statistical analyses to determine if what you see is a true effect versus noise\nEven if there is a true effect, you may have limited power to detect it (some effects are easier to detect than others)\nRemember: Power is the probability you reject when the null is false. Things that increase statistical power:\n\nIncrease sample size\nReduce variance/error\nIncrease differences / effects\nChoose appropriate tests!\n\n\n\n\n\nHW3 is due TONIGHT and you have Lab 5 on Friday\nNext time: 2D Quantitative Data - Scatterplots and Linear Regression"
  },
  {
    "objectID": "lectures/07-density-estimation.html#announcements-previously-and-today",
    "href": "lectures/07-density-estimation.html#announcements-previously-and-today",
    "title": "Density Estimation",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW2 is due TONIGHT by 11:59 PM\nYou have Lab 4 again on Friday!\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Wednesdays @ 11 AM\n\n\n\n\n\nVisualize 1D quantitative data to inspect center, spread, and shape\nBoxplots are only a display of summary statistics (i.e., they suck)\nHistograms display shape of the distribution, but comes with tradeoffs\nDensity curves provide an easy way to visualize conditional distributions\n\n\n\n\n\nTODAY:\n\nDisplaying smooth densities\nHow does kernel density estimation work?"
  },
  {
    "objectID": "lectures/07-density-estimation.html#continuous-densities",
    "href": "lectures/07-density-estimation.html#continuous-densities",
    "title": "Density Estimation",
    "section": "Continuous Densities",
    "text": "Continuous Densities\nDistribution of any continuous random variable \\(X\\) is defined by a probability density function (PDF), typically denoted by \\(f(x)\\)\n\nProbability continuous variable \\(X\\) takes a particular value is 0, why?\n\nUse PDF to provide a relative likelihood,\n\ne.g., Normal distribution: \\(f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp(- \\frac{(x - \\mu)^2}{2\\sigma^2})\\)\n\n\n\nProperties of densities\n\nHow do we estimate densities?"
  },
  {
    "objectID": "lectures/07-density-estimation.html#normal-distribution",
    "href": "lectures/07-density-estimation.html#normal-distribution",
    "title": "Density Estimation",
    "section": "Normal distribution",
    "text": "Normal distribution"
  },
  {
    "objectID": "lectures/07-density-estimation.html#uniform-distribution",
    "href": "lectures/07-density-estimation.html#uniform-distribution",
    "title": "Density Estimation",
    "section": "Uniform distribution",
    "text": "Uniform distribution"
  },
  {
    "objectID": "lectures/07-density-estimation.html#gamma-also-exponential-and-chi-squared-distribution",
    "href": "lectures/07-density-estimation.html#gamma-also-exponential-and-chi-squared-distribution",
    "title": "Density Estimation",
    "section": "Gamma (also Exponential and Chi-squared) distribution",
    "text": "Gamma (also Exponential and Chi-squared) distribution"
  },
  {
    "objectID": "lectures/07-density-estimation.html#beta-distribution",
    "href": "lectures/07-density-estimation.html#beta-distribution",
    "title": "Density Estimation",
    "section": "Beta distribution",
    "text": "Beta distribution"
  },
  {
    "objectID": "lectures/07-density-estimation.html#normalize-histogram-frequencies-with-density",
    "href": "lectures/07-density-estimation.html#normalize-histogram-frequencies-with-density",
    "title": "Density Estimation",
    "section": "Normalize histogram frequencies with density",
    "text": "Normalize histogram frequencies with density\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_histogram(aes(y = after_stat(density), fill = species),\n                 position = \"identity\", alpha = 0.3)"
  },
  {
    "objectID": "lectures/07-density-estimation.html#can-use-density-curves-instead",
    "href": "lectures/07-density-estimation.html#can-use-density-curves-instead",
    "title": "Density Estimation",
    "section": "Can use density curves instead",
    "text": "Can use density curves instead\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(aes(color = species))"
  },
  {
    "objectID": "lectures/07-density-estimation.html#kernel-density-estimation",
    "href": "lectures/07-density-estimation.html#kernel-density-estimation",
    "title": "Density Estimation",
    "section": "Kernel density estimation",
    "text": "Kernel density estimation\nGoal: estimate PDF \\(f(x)\\) for all possible values (assuming it is continuous & smooth)\n\n\\[\n\\text{Kernel density estimate: } \\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\n\\]\n\n\n\n\n\\(n =\\) sample size, \\(x =\\) new point to estimate \\(f(x)\\) (does NOT have to be in dataset!)\n\n\n\n\n\n\n\\(h =\\) bandwidth, analogous to histogram bin width, ensures \\(\\hat{f}(x)\\) integrates to 1\n\\(x_i =\\) \\(i\\)th observation in dataset\n\n\n\n\n\n\n\\(K_h(x - x_i)\\) is the Kernel function, creates weight given distance of \\(i\\)th observation from new point\n\nas \\(|x - x_i| \\rightarrow \\infty\\) then \\(K_h(x - x_i) \\rightarrow 0\\), i.e. further apart \\(i\\)th row is from \\(x\\), smaller the weight\nas bandwidth \\(h \\uparrow\\) weights are more evenly spread out (as \\(h \\downarrow\\) more concentrated around \\(x\\))\ntypically use Gaussian / Normal kernel: \\(\\propto e^{-(x - x_i)^2 / 2h^2}\\)\n\\(K_h(x - x_i)\\) is large when \\(x_i\\) is close to \\(x\\)"
  },
  {
    "objectID": "lectures/07-density-estimation.html#wikipedia-example",
    "href": "lectures/07-density-estimation.html#wikipedia-example",
    "title": "Density Estimation",
    "section": "Wikipedia example",
    "text": "Wikipedia example"
  },
  {
    "objectID": "lectures/07-density-estimation.html#we-display-kernel-density-estimates-with-geom_density",
    "href": "lectures/07-density-estimation.html#we-display-kernel-density-estimates-with-geom_density",
    "title": "Density Estimation",
    "section": "We display kernel density estimates with geom_density()",
    "text": "We display kernel density estimates with geom_density()\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#choice-of-kernel",
    "href": "lectures/07-density-estimation.html#choice-of-kernel",
    "title": "Density Estimation",
    "section": "Choice of kernel?",
    "text": "Choice of kernel?"
  },
  {
    "objectID": "lectures/07-density-estimation.html#what-about-the-bandwidth",
    "href": "lectures/07-density-estimation.html#what-about-the-bandwidth",
    "title": "Density Estimation",
    "section": "What about the bandwidth?",
    "text": "What about the bandwidth?\nUse Gaussian reference rule (rule-of-thumb) \\(\\approx 1.06 \\cdot \\sigma \\cdot n^{-1/5}\\), where \\(\\sigma\\) is the observed standard deviation\nModify the bandwidth using the adjust argument - value to multiply default bandwidth by\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(adjust = 0.5) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#what-about-the-bandwidth-1",
    "href": "lectures/07-density-estimation.html#what-about-the-bandwidth-1",
    "title": "Density Estimation",
    "section": "What about the bandwidth?",
    "text": "What about the bandwidth?\nUse Gaussian reference rule (rule-of-thumb) \\(\\approx 1.06 \\cdot \\sigma \\cdot n^{-1/5}\\), where \\(\\sigma\\) is the observed standard deviation\nModify the bandwidth using the adjust argument - value to multiply default bandwidth by\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(adjust = 2) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#caution-dealing-with-bounded-data",
    "href": "lectures/07-density-estimation.html#caution-dealing-with-bounded-data",
    "title": "Density Estimation",
    "section": "CAUTION: dealing with bounded data…",
    "text": "CAUTION: dealing with bounded data…\n\nset.seed(101)\nbound_data &lt;- tibble(fake_x = runif(100))\n\nbound_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_density() +\n  geom_rug(alpha = 0.5) + #&lt;&lt;\n  stat_function(data = \n                  tibble(fake_x = c(0, 1)),\n                fun = dunif, color = \"red\") +\n  scale_x_continuous(limits = c(-.5, 1.5))"
  },
  {
    "objectID": "lectures/07-density-estimation.html#visualizing-conditional-distributions-violin-plots",
    "href": "lectures/07-density-estimation.html#visualizing-conditional-distributions-violin-plots",
    "title": "Density Estimation",
    "section": "Visualizing conditional distributions: violin plots",
    "text": "Visualizing conditional distributions: violin plots\n\npenguins |&gt;\n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_violin() +\n  coord_flip()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#visualizing-conditional-distributions-violin-plots-1",
    "href": "lectures/07-density-estimation.html#visualizing-conditional-distributions-violin-plots-1",
    "title": "Density Estimation",
    "section": "Visualizing conditional distributions: violin plots",
    "text": "Visualizing conditional distributions: violin plots\n\npenguins |&gt;\n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_violin() + \n  geom_boxplot(width = .2) +\n  coord_flip()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#visualizing-conditional-distributions-ggridges",
    "href": "lectures/07-density-estimation.html#visualizing-conditional-distributions-ggridges",
    "title": "Density Estimation",
    "section": "Visualizing conditional distributions: ggridges",
    "text": "Visualizing conditional distributions: ggridges\n\nlibrary(ggridges)\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = species)) +\n  geom_density_ridges(rel_min_height = 0.01)"
  },
  {
    "objectID": "lectures/07-density-estimation.html#visualizing-conditional-distributions-ggbeeswarm",
    "href": "lectures/07-density-estimation.html#visualizing-conditional-distributions-ggbeeswarm",
    "title": "Density Estimation",
    "section": "Visualizing conditional distributions: ggbeeswarm",
    "text": "Visualizing conditional distributions: ggbeeswarm\n\nlibrary(ggbeeswarm)\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = species)) +\n  geom_beeswarm(cex = 1.5) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#recap-and-next-steps",
    "href": "lectures/07-density-estimation.html#recap-and-next-steps",
    "title": "Density Estimation",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nSmoothed densities are a flexible tool for visualizing 1D distribution\nThere are two choices we need to make for kernel density estimation:\n\nBandwidth: Determines smoothness of distribution, usually data-driven choice\nKernel: Determines how much influence each observation should have on each other during estimation, usually context driven\n\nSeveral other types of density-based displays: violins, ridges, beeswarm plots\n\n\n\n\n\nHW2 is due TONIGHT and you have Lab 4 on Friday\nNext time: Graphical inference for 1D quantitative data\nRecommended reading: CW Chapter 7 Visualizing distributions: Histograms and density plots"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#announcements-previously-and-today",
    "href": "lectures/20-anim-graphics.html#announcements-previously-and-today",
    "title": "Animations, infographics, and annotations",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW7 is due TONIGHT March April 2nd by 11:59 PM ET\nYou do NOT have lab this week - CARNIVAL!\n\n\n\nLast time:\n\nFundamental characteristic of time series data: measurements are dependent over time\nPlotting moving averages is the most common way to visualize time series data\nThe things to look out for in time series data are: (1) Average trends, (2) Seasonality, (3) Noise\n\nTODAY: Animations, infographics, and annotations"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#storytelling-with-animation",
    "href": "lectures/20-anim-graphics.html#storytelling-with-animation",
    "title": "Animations, infographics, and annotations",
    "section": "Storytelling with animation…",
    "text": "Storytelling with animation…\n\nf1_data_ex |&gt;\n  ggplot(aes(x = round, y = points, group = name, color = name)) +\n  geom_line(size = 2) +\n  scale_x_continuous(breaks = seq(1, 17, 1)) +\n  labs(title = \"The race for third place in the 2020 F1 season\",\n       y = \"Accumulated points\", x = NULL) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#use-gganimate-to-add-animations",
    "href": "lectures/20-anim-graphics.html#use-gganimate-to-add-animations",
    "title": "Animations, infographics, and annotations",
    "section": "Use gganimate to add animations",
    "text": "Use gganimate to add animations\n\nlibrary(gganimate)\nf1_data_ex |&gt;\n  ggplot(aes(x = round, y = points, group = name, color = name)) +\n  geom_line(size = 2) +\n  scale_x_continuous(breaks = seq(1, 17, 1)) +\n  labs(title = \"The race for third place in the 2020 F1 season\",\n       y = \"Accumulated points\", x = NULL) +\n  theme_bw() +\n  transition_reveal(round)"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#using-animation-to-add-a-dimension",
    "href": "lectures/20-anim-graphics.html#using-animation-to-add-a-dimension",
    "title": "Animations, infographics, and annotations",
    "section": "Using animation to add a dimension",
    "text": "Using animation to add a dimension\n\ntxhousing |&gt; \n  group_by(city, year) |&gt; \n  summarize(median = mean(median, na.rm = TRUE),listings = mean(listings, na.rm = TRUE)) |&gt; \n  ggplot(aes(x = median, y = listings, color = (city == \"Houston\"), \n             size = (city == \"Houston\"))) +\n  geom_point(alpha = 0.5, show.legend = FALSE) +\n  scale_color_manual(values = c(\"black\", \"darkred\")) +\n  scale_size_manual(values = c(2, 4)) +\n  scale_x_continuous(labels = scales::dollar, name = \"Median Price\") +\n  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale())) +\n  theme_bw() +\n  labs(x = \"Median Price\", y = \"Avg. of Monthly Listings\",\n       subtitle = \"Houston in red\")"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#using-animation-to-add-a-dimension-output",
    "href": "lectures/20-anim-graphics.html#using-animation-to-add-a-dimension-output",
    "title": "Animations, infographics, and annotations",
    "section": "Using animation to add a dimension",
    "text": "Using animation to add a dimension"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#using-animation-to-add-a-dimension-1",
    "href": "lectures/20-anim-graphics.html#using-animation-to-add-a-dimension-1",
    "title": "Animations, infographics, and annotations",
    "section": "Using animation to add a dimension",
    "text": "Using animation to add a dimension\n\ntxhousing |&gt; \n  group_by(city, year) |&gt; \n  summarize(median = mean(median, na.rm = TRUE), listings = mean(listings, na.rm = TRUE)) |&gt; \n  ggplot(aes(x = median, y = listings, color = (city == \"Houston\"),\n             size = (city == \"Houston\"))) +\n  geom_point(alpha = 0.5, show.legend = FALSE) +\n  scale_color_manual(values = c(\"black\", \"darkred\")) +\n  scale_size_manual(values = c(2, 4)) +\n  scale_x_continuous(labels = scales::dollar, name = \"Median Price\") +\n  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale())) +\n  theme_bw() +\n  labs(x = \"Median Price\", y = \"Avg. of Monthly Listings\",\n       subtitle = \"Houston in red\", title = \"Year: {frame_time}\") + \n  transition_time(year)"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#using-animation-to-add-a-dimension-1-output",
    "href": "lectures/20-anim-graphics.html#using-animation-to-add-a-dimension-1-output",
    "title": "Animations, infographics, and annotations",
    "section": "Using animation to add a dimension",
    "text": "Using animation to add a dimension"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#reminders-about-animation",
    "href": "lectures/20-anim-graphics.html#reminders-about-animation",
    "title": "Animations, infographics, and annotations",
    "section": "Reminders about animation",
    "text": "Reminders about animation\nSome key points to think about before adding animation to a visualization:\n\nAlways make and describe the original / base graphic first that does NOT include animation.\n\n\n\nBefore adding animation to the graph, ask yourself: How would animation give you additional insights about the data that you would otherwise not be able to?\n\n\n\n\nNever add animation just because it’s cool!\n\n\n\n\nWhen presenting, make sure you explain exactly what is being displayed with animation and what within the animation you want to emphasize. This will help you determine if animation is actually worth including."
  },
  {
    "objectID": "lectures/20-anim-graphics.html#creating-compound-figures",
    "href": "lectures/20-anim-graphics.html#creating-compound-figures",
    "title": "Animations, infographics, and annotations",
    "section": "Creating compound figures",
    "text": "Creating compound figures\nTwo different scenarios we may face:\n\nCreating the same type of plot many times\n\n\ne.g., using facet_wrap() or facet_grid()\n\n\nCombining several distinct plots into one cohesive display\n\n\ne.g., using flexible arrangement packages like cowplot or patchwork"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#creating-the-same-type-of-plot-many-times",
    "href": "lectures/20-anim-graphics.html#creating-the-same-type-of-plot-many-times",
    "title": "Animations, infographics, and annotations",
    "section": "Creating the same type of plot many times",
    "text": "Creating the same type of plot many times\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~species) +\n  theme_light()"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#creating-the-same-type-of-plot-many-times-1",
    "href": "lectures/20-anim-graphics.html#creating-the-same-type-of-plot-many-times-1",
    "title": "Animations, infographics, and annotations",
    "section": "Creating the same type of plot many times",
    "text": "Creating the same type of plot many times\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_grid(island ~ species) +\n  theme_light()"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#creating-a-single-cohesive-display-of-multiple-plots",
    "href": "lectures/20-anim-graphics.html#creating-a-single-cohesive-display-of-multiple-plots",
    "title": "Animations, infographics, and annotations",
    "section": "Creating a single cohesive display of multiple plots",
    "text": "Creating a single cohesive display of multiple plots\n\nplot1 &lt;- penguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5)\nplot1"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#creating-a-single-cohesive-display-of-multiple-plots-1",
    "href": "lectures/20-anim-graphics.html#creating-a-single-cohesive-display-of-multiple-plots-1",
    "title": "Animations, infographics, and annotations",
    "section": "Creating a single cohesive display of multiple plots",
    "text": "Creating a single cohesive display of multiple plots\n\nplot2 &lt;- penguins |&gt;\n  ggplot(aes(x = species, y = bill_depth_mm)) +\n  geom_violin(alpha = 0.5)\nplot2"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#using-cowplot-to-arrange-plots-together",
    "href": "lectures/20-anim-graphics.html#using-cowplot-to-arrange-plots-together",
    "title": "Animations, infographics, and annotations",
    "section": "Using cowplot to arrange plots together",
    "text": "Using cowplot to arrange plots together\n\nlibrary(cowplot)\nplot_grid(plot1, plot2)"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#using-cowplot-to-arrange-plots-together-1",
    "href": "lectures/20-anim-graphics.html#using-cowplot-to-arrange-plots-together-1",
    "title": "Animations, infographics, and annotations",
    "section": "Using cowplot to arrange plots together",
    "text": "Using cowplot to arrange plots together\n\nlibrary(cowplot)\nplot_grid(plot1, plot2, labels = c('A', 'B'), label_size = 12)"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#using-patchwork-to-arrange-plots-together",
    "href": "lectures/20-anim-graphics.html#using-patchwork-to-arrange-plots-together",
    "title": "Animations, infographics, and annotations",
    "section": "Using patchwork to arrange plots together",
    "text": "Using patchwork to arrange plots together\n\nlibrary(patchwork)\nplot1 + plot2"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#using-patchwork-to-arrange-plots-together-1",
    "href": "lectures/20-anim-graphics.html#using-patchwork-to-arrange-plots-together-1",
    "title": "Animations, infographics, and annotations",
    "section": "Using patchwork to arrange plots together",
    "text": "Using patchwork to arrange plots together\n\nplot1 / plot2"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#using-patchwork-to-arrange-plots-together-2",
    "href": "lectures/20-anim-graphics.html#using-patchwork-to-arrange-plots-together-2",
    "title": "Animations, infographics, and annotations",
    "section": "Using patchwork to arrange plots together",
    "text": "Using patchwork to arrange plots together\n\nplot1 / plot2 + plot_annotation(tag_levels = \"A\")"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#using-patchwork-to-arrange-plots-together-3",
    "href": "lectures/20-anim-graphics.html#using-patchwork-to-arrange-plots-together-3",
    "title": "Animations, infographics, and annotations",
    "section": "Using patchwork to arrange plots together",
    "text": "Using patchwork to arrange plots together\n\nplot3 &lt;- penguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm,\n             color = species)) +\n  geom_point(alpha = 0.5)\nplot4 &lt;- penguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = body_mass_g,\n             color = species)) +\n  geom_point(alpha = 0.5)\n(plot1 + plot2) / (plot3 + plot4) + plot_layout(guides = 'collect')"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#using-patchwork-to-arrange-plots-together-3-output",
    "href": "lectures/20-anim-graphics.html#using-patchwork-to-arrange-plots-together-3-output",
    "title": "Animations, infographics, and annotations",
    "section": "Using patchwork to arrange plots together",
    "text": "Using patchwork to arrange plots together"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#using-patchwork-to-arrange-plots-together-4",
    "href": "lectures/20-anim-graphics.html#using-patchwork-to-arrange-plots-together-4",
    "title": "Animations, infographics, and annotations",
    "section": "Using patchwork to arrange plots together",
    "text": "Using patchwork to arrange plots together\n\n(plot1 + plot2) / (plot3 + plot4) + plot_layout(guides = 'collect') +\n  plot_annotation(tag_levels = \"A\")"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#using-patchwork-to-arrange-plots-together-5",
    "href": "lectures/20-anim-graphics.html#using-patchwork-to-arrange-plots-together-5",
    "title": "Animations, infographics, and annotations",
    "section": "Using patchwork to arrange plots together",
    "text": "Using patchwork to arrange plots together\n\n(plot1 + plot2) / (plot3 + plot4) + plot_layout(guides = 'collect') +\n  plot_annotation(tag_levels = \"A\", title = \"A plot about penguins\",\n                  subtitle = \"With subtitle...\", caption = \"...and caption\")"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#infographics-vs-figures-in-papersreports",
    "href": "lectures/20-anim-graphics.html#infographics-vs-figures-in-papersreports",
    "title": "Animations, infographics, and annotations",
    "section": "Infographics vs figures in papers/reports",
    "text": "Infographics vs figures in papers/reports\n\nInfographics should standalone, thus they must have a title along with a relevant subtitle and caption (located within the plot)"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#infographics-vs-figures-in-papersreports-1",
    "href": "lectures/20-anim-graphics.html#infographics-vs-figures-in-papersreports-1",
    "title": "Animations, infographics, and annotations",
    "section": "Infographics vs figures in papers/reports",
    "text": "Infographics vs figures in papers/reports\n\nFigures in papers/reports will have captions containing the information from the standalone title/subtitle/caption, see example:\n\n\nFigure 1. Corruption and human development. The most developed countries experience the least corruption. Data sources: Transparency International & UN Human Development Report."
  },
  {
    "objectID": "lectures/20-anim-graphics.html#thinking-about-themes",
    "href": "lectures/20-anim-graphics.html#thinking-about-themes",
    "title": "Animations, infographics, and annotations",
    "section": "Thinking about themes…",
    "text": "Thinking about themes…\nSee posted demo walking through color scales and customizing themes\nDefault choices tend to treat each element with equal weight, e.g., axes stand out as much as the data or background elements look the same as the points of emphasis\n\nYou want to design your plot with the visual hierarchy in mind:\n\nMake elements of your plot that are more important look more important!\ni.e., customize your plot so that the data is the focus, not the axes and grid lines!\nMatch visual weight to focus of the graphic you want to communicate\n\n\n\nI tend to use theme_bw() or theme_light(), but there are other options from various packages such as ggthemes"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#using-patchwork-to-arrange-plots-together-6",
    "href": "lectures/20-anim-graphics.html#using-patchwork-to-arrange-plots-together-6",
    "title": "Animations, infographics, and annotations",
    "section": "Using patchwork to arrange plots together",
    "text": "Using patchwork to arrange plots together\n\n(plot1 + plot2) / (plot3 + plot4) + plot_layout(guides = 'collect') +\n  plot_annotation(tag_levels = \"A\", title = \"A plot about penguins\",\n                  subtitle = \"With subtitle...\", caption = \"...and caption\") & \n  theme_minimal_grid()"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#annotation",
    "href": "lectures/20-anim-graphics.html#annotation",
    "title": "Animations, infographics, and annotations",
    "section": "Annotation",
    "text": "Annotation\n\nUsing text can be a great way to highlight and explain aspects of a visualization when you’re not there to explain it\nannotate() is an easy way to add text to ggplot objects or add rectangle layers for highlighting displays\n\n\nmtcars |&gt;\n  ggplot(aes(x = wt, y = mpg)) + \n  geom_point() + \n  annotate(\"text\", x = 4, y = 25, label = \"Some text\") +\n  annotate(\"rect\", xmin = 3, xmax = 4.2, ymin = 12, ymax = 21, alpha = .2)"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#annotation-tools",
    "href": "lectures/20-anim-graphics.html#annotation-tools",
    "title": "Animations, infographics, and annotations",
    "section": "Annotation tools",
    "text": "Annotation tools\n\nWe’ve discussed gghighlight and ggrepel, but directlabels and ggforce are also useful\n\n\nlibrary(ggforce)\nggplot(iris, aes(Petal.Length, Petal.Width)) +\n  geom_mark_rect(aes(fill = Species, label = Species)) +\n  geom_point()"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#saving-plots-and-ggplot-extensions",
    "href": "lectures/20-anim-graphics.html#saving-plots-and-ggplot-extensions",
    "title": "Animations, infographics, and annotations",
    "section": "Saving plots and ggplot extensions",
    "text": "Saving plots and ggplot extensions\n\nDefault function for saving the last ggplot you created is ggsave\nI tend to use the save_plot() function from cowplot since it has easier customization for handling panels of multiple figures\nThere are a wide number of ggplot2 extension packages for various different purposes, you can check them out in this gallery"
  },
  {
    "objectID": "lectures/20-anim-graphics.html#recap-and-next-steps",
    "href": "lectures/20-anim-graphics.html#recap-and-next-steps",
    "title": "Animations, infographics, and annotations",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nDiscussed the role of animation in visualizations\nDiscussed various aspects of making high-quality graphics and relevant tools\n\n\n\nHW7 is due TONIGHT!\nENJOY CARNIVAL!\nRecommended reading: gganimate package, CW Chapter 21 Multi-panel figures, CW Chapter 23 Balance the data and the context, KH Chapter 8 Refine your plots"
  },
  {
    "objectID": "lectures/23-text-data.html#announcements-previously-and-today",
    "href": "lectures/23-text-data.html#announcements-previously-and-today",
    "title": "Visualizing Text Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nYou should be working on your final projects!\nYou must email me the slides for your presentation by 10 AM ET the day of your presentation - either a Google slides link or PDF file\nYou do have lab this week!\n\n\n\nLast time: visualizing areal data\nTODAY: Text data, starting with:\n\nBag of Words representation of text.\nWord clouds\n\nWill be able to answer the following questions:\n\nWhich words are most frequent in a set of documents?\nHow do two (or more) sets of documents compare in their word usage?\nWhich unique words occur most frequently in a set of documents?"
  },
  {
    "objectID": "lectures/23-text-data.html#working-with-raw-text-data",
    "href": "lectures/23-text-data.html#working-with-raw-text-data",
    "title": "Visualizing Text Data",
    "section": "Working with raw text data",
    "text": "Working with raw text data\n\nWe’ll work with script from the best episode of ‘The Office’: Season 4, Episode 13 - ‘Dinner Party’\nWe can access the script using the schrute package (yes this is a real thing):\n\n\nlibrary(schrute)\n# Create a table from this package just corresponding to the Dinner Party episode:\ndinner_party_table &lt;- theoffice |&gt;\n  filter(season == 4, episode == 13) |&gt;\n  # Just select columns of interest:\n  dplyr::select(index, character, text)\nhead(dinner_party_table)\n\n# A tibble: 6 × 3\n  index character text                                                          \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;                                                         \n1 16791 Stanley   This is ridiculous.                                           \n2 16792 Phyllis   Do you have any idea what time we'll get out of here?         \n3 16793 Michael   Nobody likes to work late, least of all me. Do you have plans…\n4 16794 Jim       Nope I don't, remember when you told us not to make plans 'ca…\n5 16795 Michael   Yes I remember. Mmm, this is B.S. This is B.S. Why are we her…\n6 16796 Dwight    Thank you Michael."
  },
  {
    "objectID": "lectures/23-text-data.html#reducing-the-dimensionality-of-text",
    "href": "lectures/23-text-data.html#reducing-the-dimensionality-of-text",
    "title": "Visualizing Text Data",
    "section": "Reducing the dimensionality of text",
    "text": "Reducing the dimensionality of text\nHow can we quantify text data such that it can be used in statistical models, analyses, and graphs?\n\nBag of Words representation of text is the most common representation\n\nIn a document, grammar and order of words don’t matter\nAll that matters is the number of times each word occurs\n\n\n“Do I need to be liked? Absolutely not. I like to be liked. I enjoy being liked. I have to be liked, but it’s not like this compulsive need to be liked, like my need to be praised.” - Michael Scott\n\n\n\n\nDo = 1, I = 4, need = 3, to = 5, be = 5, liked = 5\nAbsolutely = 1, not = 2, like = 3, enjoy = 1\nbeing = 1, have = 1, but = 1, it’s = 1, this = 1\ncompulsive = 1, my = 1, praised = 2"
  },
  {
    "objectID": "lectures/23-text-data.html#bag-of-words-representation-of-text",
    "href": "lectures/23-text-data.html#bag-of-words-representation-of-text",
    "title": "Visualizing Text Data",
    "section": "Bag of Words representation of text",
    "text": "Bag of Words representation of text\n\nMost common way to store text data is with a document-term matrix (DTM):\n\n\n\n\n\nWord 1\nWord 2\n\\(\\dots\\)\nWord \\(J\\)\n\n\n\n\nDocument 1\n\\(w_{11}\\)\n\\(w_{12}\\)\n\\(\\dots\\)\n\\(w_{1J}\\)\n\n\nDocument 2\n\\(w_{21}\\)\n\\(w_{22}\\)\n\\(\\dots\\)\n\\(w_{2J}\\)\n\n\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\nDocument N\n\\(w_{N1}\\)\n\\(w_{N2}\\)\n\\(\\dots\\)\n\\(w_{NJ}\\)\n\n\n\n\n\\(w_{ij}\\): count of word \\(j\\) in document \\(i\\), aka term frequencies\n\n\nTwo additional ways to reduce number of columns:\n\nStop words: remove extremely common words (e.g., of, the, a)\nStemming: Reduce all words to their “stem”\n\n\nFor example: Reducing = reduc. Reduce = reduc. Reduces = reduc."
  },
  {
    "objectID": "lectures/23-text-data.html#tokenize-text-into-long-format",
    "href": "lectures/23-text-data.html#tokenize-text-into-long-format",
    "title": "Visualizing Text Data",
    "section": "Tokenize text into long format",
    "text": "Tokenize text into long format\n\nConvert raw text into long, tidy table with one-token-per-document-per-row\n\nA token equals a unit of text - typically a word\n\n\n\nlibrary(tidytext)\ntidy_dinner_party_tokens &lt;- dinner_party_table |&gt;\n  unnest_tokens(word, text)\nhead(tidy_dinner_party_tokens)\n\n# A tibble: 6 × 3\n  index character word      \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     \n1 16791 Stanley   this      \n2 16791 Stanley   is        \n3 16791 Stanley   ridiculous\n4 16792 Phyllis   do        \n5 16792 Phyllis   you       \n6 16792 Phyllis   have      \n\n\nEasy to convert text into DTM format using tidytext package"
  },
  {
    "objectID": "lectures/23-text-data.html#remove-stop-words",
    "href": "lectures/23-text-data.html#remove-stop-words",
    "title": "Visualizing Text Data",
    "section": "Remove stop words",
    "text": "Remove stop words\n\nLoad stop_words from tidytext\n\n\ndata(stop_words)\n\ntidy_dinner_party_tokens &lt;- tidy_dinner_party_tokens |&gt;\n  filter(!(word %in% stop_words$word))\n\nhead(tidy_dinner_party_tokens)\n\n# A tibble: 6 × 3\n  index character word      \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     \n1 16791 Stanley   ridiculous\n2 16792 Phyllis   idea      \n3 16792 Phyllis   time      \n4 16793 Michael   likes     \n5 16793 Michael   late      \n6 16793 Michael   plans"
  },
  {
    "objectID": "lectures/23-text-data.html#apply-stemming",
    "href": "lectures/23-text-data.html#apply-stemming",
    "title": "Visualizing Text Data",
    "section": "Apply stemming",
    "text": "Apply stemming\n\nCan use SnowballC package to perform stemming\n\n\nlibrary(SnowballC)\n\ntidy_dinner_party_tokens &lt;- tidy_dinner_party_tokens |&gt;\n  mutate(stem = wordStem(word))\n\nhead(tidy_dinner_party_tokens)\n\n# A tibble: 6 × 4\n  index character word       stem   \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;  \n1 16791 Stanley   ridiculous ridicul\n2 16792 Phyllis   idea       idea   \n3 16792 Phyllis   time       time   \n4 16793 Michael   likes      like   \n5 16793 Michael   late       late   \n6 16793 Michael   plans      plan"
  },
  {
    "objectID": "lectures/23-text-data.html#create-word-cloud-using-term-frequencies",
    "href": "lectures/23-text-data.html#create-word-cloud-using-term-frequencies",
    "title": "Visualizing Text Data",
    "section": "Create word cloud using term frequencies",
    "text": "Create word cloud using term frequencies\nWord Cloud: Displays all words mentioned across documents, where more common words are larger\n\nTo do this, you must compute the total word counts:\n\n\\[w_{\\cdot 1} = \\sum_{i=1}^N w_{i1} \\hspace{0.1in} \\dots \\hspace{0.1in} w_{\\cdot J} = \\sum_{i=1}^N w_{iJ}\\]\n\nThen, the size of Word \\(j\\) is proportional to \\(w_{\\cdot j}\\)\n\n\nCreate word clouds in R using wordcloud package\nTakes in two main arguments to create word clouds:\n\nwords: vector of unique words\nfreq: vector of frequencies"
  },
  {
    "objectID": "lectures/23-text-data.html#create-word-cloud-using-term-frequencies-1",
    "href": "lectures/23-text-data.html#create-word-cloud-using-term-frequencies-1",
    "title": "Visualizing Text Data",
    "section": "Create word cloud using term frequencies",
    "text": "Create word cloud using term frequencies\n\ntoken_summary &lt;- tidy_dinner_party_tokens |&gt;\n  group_by(stem) |&gt;\n  count() |&gt;\n  ungroup() \n\nlibrary(wordcloud)\nwordcloud(words = token_summary$stem, \n          freq = token_summary$n, \n          random.order = FALSE, \n          max.words = 100, \n          colors = brewer.pal(8, \"Dark2\"))\n\n\n\nSet random.order = FALSE to place biggest words in center\nCan customize to display limited # words (max.words)\nOther options as well like colors"
  },
  {
    "objectID": "lectures/23-text-data.html#create-word-cloud-using-term-frequencies-1-output",
    "href": "lectures/23-text-data.html#create-word-cloud-using-term-frequencies-1-output",
    "title": "Visualizing Text Data",
    "section": "Create word cloud using term frequencies",
    "text": "Create word cloud using term frequencies"
  },
  {
    "objectID": "lectures/23-text-data.html#comparison-clouds",
    "href": "lectures/23-text-data.html#comparison-clouds",
    "title": "Visualizing Text Data",
    "section": "Comparison clouds",
    "text": "Comparison clouds\nImagine we have two different collections of documents, \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\), that we wish to visually compare.\n\nImagine we create the word clouds for the two collections of documents. Then this means we constructed vectors of total words for each collection:\n\n\\(\\mathbf{w}^{\\mathcal{A}} = (w_{\\cdot 1}^{\\mathcal{A}}, \\dots, w_{\\cdot J}^{\\mathcal{A}})\\)\n\\(\\mathbf{w}^{\\mathcal{B}} = (w_{\\cdot 1}^{\\mathcal{B}}, \\dots, w_{\\cdot J}^{\\mathcal{B}})\\)\n\nConsider the \\(j\\)th word, let’s pretend it’s “dinner”:\n\nIf \\(w_{\\cdot j}^{\\mathcal{A}}\\) is large, then “dinner” is large in the word cloud for \\(\\mathcal{A}\\).\nIf \\(w_{\\cdot j}^{\\mathcal{B}}\\) is large, then “dinner” is large in the word cloud for \\(\\mathcal{B}\\).\nBut if both are large, this doesn’t tell us whether \\(w_{\\cdot j}^{\\mathcal{A}}\\) or \\(w_{\\cdot j}^{\\mathcal{B}}\\) is bigger."
  },
  {
    "objectID": "lectures/23-text-data.html#comparison-clouds-1",
    "href": "lectures/23-text-data.html#comparison-clouds-1",
    "title": "Visualizing Text Data",
    "section": "Comparison clouds",
    "text": "Comparison clouds\nThis motivates the construction of comparison word clouds:\n\nFor word \\(j\\), compute \\(\\bar{w}_{\\cdot j} = \\text{average}(w_{\\cdot j}^{\\mathcal{A}}, w_{\\cdot j}^{\\mathcal{B}})\\)\nCompute \\(w_{\\cdot j}^{\\mathcal{A}} - \\bar{w}_{\\cdot j}\\) and \\(w_{\\cdot j}^{\\mathcal{B}} - \\bar{w}_{\\cdot j}\\)\nIf \\(w_{\\cdot j}^{\\mathcal{A}} - \\bar{w}_{\\cdot j}\\) is very positive, make it large for the \\(\\mathcal{A}\\) word cloud. If \\(w_{\\cdot j}^{\\mathcal{B}} - \\bar{w}_{\\cdot j}\\) is very positive, make it large for the \\(\\mathcal{B}\\) word cloud."
  },
  {
    "objectID": "lectures/23-text-data.html#comparison-clouds-2",
    "href": "lectures/23-text-data.html#comparison-clouds-2",
    "title": "Visualizing Text Data",
    "section": "Comparison clouds",
    "text": "Comparison clouds"
  },
  {
    "objectID": "lectures/23-text-data.html#tf-idf-weighting",
    "href": "lectures/23-text-data.html#tf-idf-weighting",
    "title": "Visualizing Text Data",
    "section": "TF-IDF weighting",
    "text": "TF-IDF weighting\n\nWe saw that michael was the largest word, but what if I’m interested in comparing text across characters (i.e., documents)?\n\n\n\nIt’s arguably of more interest to understand which words are frequently used in one set of texts but not the other, i.e., which words are unique?\nMany text analytics methods will down-weight words that occur frequently across all documents\n\n\n\n\nInverse document frequency (IDF): for word \\(j\\) we compute \\(\\text{idf}_j = \\log \\frac{N}{N_j}\\)\n\nwhere \\(N\\) is number of documents, \\(N_j\\) is number of documents with word \\(j\\)\n\nCompute TF-IDF \\(= w_{ij} \\times \\text{idf}_j\\)"
  },
  {
    "objectID": "lectures/23-text-data.html#tf-idf-example-with-characters",
    "href": "lectures/23-text-data.html#tf-idf-example-with-characters",
    "title": "Visualizing Text Data",
    "section": "TF-IDF example with characters",
    "text": "TF-IDF example with characters\nCompute and join TF-IDF using bind_tf_idf():\n\ncharacter_token_summary &lt;- tidy_dinner_party_tokens |&gt;\n  group_by(character, stem) |&gt; \n  count() |&gt;\n  ungroup() \n\ncharacter_token_summary &lt;- character_token_summary |&gt;\n  bind_tf_idf(stem, character, n) \ncharacter_token_summary\n\n# A tibble: 597 × 6\n   character stem        n     tf   idf tf_idf\n   &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 All       cheer       1 1      2.77  2.77  \n 2 Andy      anim        1 0.0476 2.77  0.132 \n 3 Andy      bet         1 0.0476 2.08  0.0990\n 4 Andy      capit       1 0.0476 2.77  0.132 \n 5 Andy      dinner      1 0.0476 0.981 0.0467\n 6 Andy      flower      2 0.0952 2.77  0.264 \n 7 Andy      hei         1 0.0476 1.39  0.0660\n 8 Andy      helena      1 0.0476 2.77  0.132 \n 9 Andy      hump        2 0.0952 2.77  0.264 \n10 Andy      michael     1 0.0476 0.981 0.0467\n# ℹ 587 more rows"
  },
  {
    "objectID": "lectures/23-text-data.html#top-10-words-by-tf-idf-for-each-character",
    "href": "lectures/23-text-data.html#top-10-words-by-tf-idf-for-each-character",
    "title": "Visualizing Text Data",
    "section": "Top 10 words by TF-IDF for each character",
    "text": "Top 10 words by TF-IDF for each character\n\ncharacter_token_summary |&gt;\n  filter(character %in% c(\"Michael\", \"Jan\", \"Jim\", \"Pam\")) |&gt;\n  group_by(character) |&gt;\n  slice_max(tf_idf, n = 10, with_ties = FALSE) |&gt;\n  ungroup() |&gt;\n  mutate(stem = reorder_within(stem, tf_idf, character)) |&gt;\n  ggplot(aes(y = tf_idf, x = stem),\n         fill = \"darkblue\", alpha = 0.5) +\n  geom_col() +\n  coord_flip() +\n  scale_x_reordered() +\n  facet_wrap(~ character, ncol = 2, scales = \"free\") +\n  labs(y = \"TF-IDF\", x = NULL)"
  },
  {
    "objectID": "lectures/23-text-data.html#top-10-words-by-tf-idf-for-each-character-output",
    "href": "lectures/23-text-data.html#top-10-words-by-tf-idf-for-each-character-output",
    "title": "Visualizing Text Data",
    "section": "Top 10 words by TF-IDF for each character",
    "text": "Top 10 words by TF-IDF for each character"
  },
  {
    "objectID": "lectures/23-text-data.html#other-functions-of-text",
    "href": "lectures/23-text-data.html#other-functions-of-text",
    "title": "Visualizing Text Data",
    "section": "Other functions of text",
    "text": "Other functions of text\n\nWe’ve just focused on word counts - but there are many functions of text\nFor example: number of unique words is often used to measure vocabulary"
  },
  {
    "objectID": "lectures/23-text-data.html#recap-and-next-steps",
    "href": "lectures/23-text-data.html#recap-and-next-steps",
    "title": "Visualizing Text Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nMost common representation: Bag of words and term frequencies (possibly weighted by TF-IDF)\nWord clouds are the most common way to visualize the most frequent words in a set of documents\nTF-IDF weighting allows you to detect words that are uniquely used in certain documents\n\n\n\nYou have lab on Friday!\nNext time: Sentiment analysis and topics models\nRecommended Reading: Text Mining With R, Supervised Machine Learning for Text Analysis in R"
  },
  {
    "objectID": "lectures/02-1dcat.html#announcements-previously-and-today",
    "href": "lectures/02-1dcat.html#announcements-previously-and-today",
    "title": "1D Categorical Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nComplete HW0 by tonight! Confirms you have everything installed and can render .qmd files to PDF via tinytex\nOffice hours will be announced soon… (I’ll be in my office BH 132D today at 2:30 PM)\n\n\n\nDiscussed the importance of data visualization in your role as a statistician / data scientist\nIntroduced the Grammar of Graphics as a framework for building visualizations\nDiscussed historical examples and principles of visualization to keep in mind\n\n\n\nTODAY: 1D Categorical Data\n\nBriefly talk about variable types\nWalk through different graphs for visualizing 1D categorical data"
  },
  {
    "objectID": "lectures/02-1dcat.html#reminder-tidy-data-structure",
    "href": "lectures/02-1dcat.html#reminder-tidy-data-structure",
    "title": "1D Categorical Data",
    "section": "Reminder: tidy data structure",
    "text": "Reminder: tidy data structure\nData are often stored in tabular (or matrix) form:\n\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nEach row == unit of observation, e.g., penguins\nEach column == variable/measurement about each observation, e.g., flipper_length_mm\nKnown as a data.frame in base R and tibble in the tidyverse\nTwo main variable types: quantitative and categorical"
  },
  {
    "objectID": "lectures/02-1dcat.html#variable-types",
    "href": "lectures/02-1dcat.html#variable-types",
    "title": "1D Categorical Data",
    "section": "Variable Types",
    "text": "Variable Types\n\nMost visualizations are about understanding the distribution of different variables (which are stored in columns of tabular/matrix data)\nThe variable type often dictates the type of graphs you should make\nThere are two main types of variables:"
  },
  {
    "objectID": "lectures/02-1dcat.html#d-categorical-data",
    "href": "lectures/02-1dcat.html#d-categorical-data",
    "title": "1D Categorical Data",
    "section": "1D Categorical Data",
    "text": "1D Categorical Data\nTwo different versions of categorical:\n\nNominal: coded with arbitrary numbers, i.e., no real order\n\n\nExamples: race, gender, species, text\n\n\n\nOrdinal: levels with a meaningful order\n\n\nExamples: education level, grades, ranks\n\n\n\nNOTE: R and ggplot considers a categorical variable to be factor\n\nR will always treat categorical variables as ordinal! Defaults to alphabetical…\nWe will need to manually define the factor levels"
  },
  {
    "objectID": "lectures/02-1dcat.html#d-categorical-data-structure",
    "href": "lectures/02-1dcat.html#d-categorical-data-structure",
    "title": "1D Categorical Data",
    "section": "1D categorical data structure",
    "text": "1D categorical data structure\n\nObservations are collected into a vector \\((x_1, \\dots, x_n)\\), where \\(n\\) is number of observations\nEach observed value \\(x_i\\) can only belong to one category level \\(\\{ C_1, C_2, \\dots \\}\\)\n\n\nLook at penguins data from the palmerpenguins package, focusing on species:\n\nlibrary(palmerpenguins)\nhead(penguins$species)\n\n[1] Adelie Adelie Adelie Adelie Adelie Adelie\nLevels: Adelie Chinstrap Gentoo\n\n\nHow could we summarize these data? What information would you report?\n\n\n\ntable(penguins$species)\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124"
  },
  {
    "objectID": "lectures/02-1dcat.html#area-plots",
    "href": "lectures/02-1dcat.html#area-plots",
    "title": "1D Categorical Data",
    "section": "Area plots",
    "text": "Area plots\n\n\nEach area corresponds to one categorical level\nArea is proportional to counts/frequencies/percentages\nDifferences between areas correspond to differences between counts/frequencies/percentages"
  },
  {
    "objectID": "lectures/02-1dcat.html#bar-charts",
    "href": "lectures/02-1dcat.html#bar-charts",
    "title": "1D Categorical Data",
    "section": "Bar charts",
    "text": "Bar charts\n\nlibrary(tidyverse)\npenguins |&gt;\n  ggplot(aes(x = species)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/02-1dcat.html#behind-the-scenes-statistical-summaries",
    "href": "lectures/02-1dcat.html#behind-the-scenes-statistical-summaries",
    "title": "1D Categorical Data",
    "section": "Behind the scenes: statistical summaries",
    "text": "Behind the scenes: statistical summaries\n\nFrom Chapter 3 of R for Data Science"
  },
  {
    "objectID": "lectures/02-1dcat.html#spine-charts---height-version",
    "href": "lectures/02-1dcat.html#spine-charts---height-version",
    "title": "1D Categorical Data",
    "section": "Spine charts - height version",
    "text": "Spine charts - height version\n\npenguins |&gt;\n  ggplot(aes(fill = species, x = \"\")) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/02-1dcat.html#spine-charts---width-version",
    "href": "lectures/02-1dcat.html#spine-charts---width-version",
    "title": "1D Categorical Data",
    "section": "Spine charts - width version",
    "text": "Spine charts - width version\n\npenguins |&gt;\n  ggplot(aes(fill = species, x = \"\")) +\n  geom_bar() +\n  coord_flip()"
  },
  {
    "objectID": "lectures/02-1dcat.html#so-you-want-to-make-pie-charts",
    "href": "lectures/02-1dcat.html#so-you-want-to-make-pie-charts",
    "title": "1D Categorical Data",
    "section": "So you want to make pie charts…",
    "text": "So you want to make pie charts…\n\npenguins |&gt; \n  ggplot(aes(fill = species, x = \"\")) + \n  geom_bar(aes(y = after_stat(count))) +\n  coord_polar(theta = \"y\") +\n  theme_void()"
  },
  {
    "objectID": "lectures/02-1dcat.html#friends-dont-let-friends-make-pie-charts",
    "href": "lectures/02-1dcat.html#friends-dont-let-friends-make-pie-charts",
    "title": "1D Categorical Data",
    "section": "Friends Don’t Let Friends Make Pie Charts",
    "text": "Friends Don’t Let Friends Make Pie Charts"
  },
  {
    "objectID": "lectures/02-1dcat.html#waffle-charts-are-cooler-anyway",
    "href": "lectures/02-1dcat.html#waffle-charts-are-cooler-anyway",
    "title": "1D Categorical Data",
    "section": "Waffle charts are cooler anyway…",
    "text": "Waffle charts are cooler anyway…\n\nlibrary(waffle)\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  ggplot(aes(fill = species, values = count)) +\n  geom_waffle(n_rows = 20, color = \"white\", flip = TRUE) +\n  coord_equal() +\n  theme_void()"
  },
  {
    "objectID": "lectures/02-1dcat.html#florence-nightingales-rose-diagram",
    "href": "lectures/02-1dcat.html#florence-nightingales-rose-diagram",
    "title": "1D Categorical Data",
    "section": "Florence Nightingale’s Rose Diagram",
    "text": "Florence Nightingale’s Rose Diagram"
  },
  {
    "objectID": "lectures/02-1dcat.html#rose-diagrams",
    "href": "lectures/02-1dcat.html#rose-diagrams",
    "title": "1D Categorical Data",
    "section": "Rose diagrams",
    "text": "Rose diagrams\n\npenguins |&gt; \n  ggplot(aes(x = species)) + \n  geom_bar(fill = \"darkblue\") +\n  coord_polar() +\n  scale_y_sqrt()"
  },
  {
    "objectID": "lectures/02-1dcat.html#recap-and-next-steps",
    "href": "lectures/02-1dcat.html#recap-and-next-steps",
    "title": "1D Categorical Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\n1D Categorical Data: look at counts, frequencies, percentages\nArea plots, where area \\(\\propto\\) counts/frequencies/percentages:\n\nBar charts (you should pretty much always just make a bar chart)\nSpine charts (will be more useful with more variables)\nPie charts (DON’T DO IT)\nRose diagrams (temporal or directional context can justify usage)\n\n\n\n\n\n\nComplete HW0 by TONIGHT! Confirms you have everything installed and can render .qmd files to PDF via tinytex\nHW1 is due in two weeks, no class on Monday\n\n\n\n\n\n\nNext time: quantify and display uncertainty for 1D categorical data\nRecommended reading:\n\nCW Chapter 10 Visualizing proportions, CW Chapter 16.2 Visualizing the uncertainty of point estimates, CW Chapter 11 Visualizing nested proportions"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#announcements-previously-and-today",
    "href": "lectures/14-contours-heatmaps.html#announcements-previously-and-today",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW5 is due next Wednesday March 19th by 11:59 PM ET\nAs part of Homework 5, you’ll form groups for final projects\n\nTeams should be 3-4 students, you can pick your teammates or be randomized to a team\nGoal of the project: create and interpet hiqh-quality visualizations for a dataset of your choice\nProject requirements and rubrics are available on Canvas\nEDA reports are graded as as group, while presentations are graded individually\nHW5 is short so you have time to form teams and explore datasets\n\nYou do NOT have lab this week\n\n\n\nLast time:\n\nLOESS: bunch of little linear regressions glued together\nPairs plots: convenient wrapper to creating several visualizations at once\n\nTODAY: Contour Plots and Heat Maps"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#d-quantitative-data",
    "href": "lectures/14-contours-heatmaps.html#d-quantitative-data",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "2D quantitative data",
    "text": "2D quantitative data\n\nWe’re working with two variables: \\((X, Y) \\in \\mathbb{R}^2\\), i.e., dataset with \\(n\\) rows and 2 columns\nGoals:\n\ndescribing the relationships between two variables\ndescribing the conditional distribution \\(Y | X\\) via regression analysis\nTODAY: describing the joint distribution \\(X,Y\\) via contours, heatmaps, etc.\n\nFew big picture ideas to keep in mind:\n\nscatterplots are by far the most common visual\nregression analysis is by far the most popular analysis (you have a whole class on this…)\nrelationships may vary across other variables, e.g., categorical variables"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#what-about-focusing-on-the-joint-distribution",
    "href": "lectures/14-contours-heatmaps.html#what-about-focusing-on-the-joint-distribution",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "What about focusing on the joint distribution?",
    "text": "What about focusing on the joint distribution?\nExample dataset of pitches thrown by baseball superstar Shohei Ohtani\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  geom_point(alpha = 0.2) +\n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#going-from-1d-to-2d-density-estimation",
    "href": "lectures/14-contours-heatmaps.html#going-from-1d-to-2d-density-estimation",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Going from 1D to 2D density estimation",
    "text": "Going from 1D to 2D density estimation\nIn 1D: estimate density \\(f(x)\\), assuming that \\(f(x)\\) is smooth:\n\\[\n\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\n\\]\n\nIn 2D: estimate joint density \\(f(x_1, x_2)\\)\n\\[\\hat{f}(x_1, x_2) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h_1h_2} K(\\frac{x_1 - x_{i1}}{h_1}) K(\\frac{x_2 - x_{i2}}{h_2})\\]\n\n\nIn 1D there was one bandwidth, now we have two bandwidths\n\n\\(h_1\\): controls smoothness as \\(X_1\\) changes, holding \\(X_2\\) fixed\n\\(h_2\\): controls smoothness as \\(X_2\\) changes, holding \\(X_1\\) fixed\n\nAgain Gaussian kernels are the most popular…"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#so-how-do-we-display-densities-for-2d-data",
    "href": "lectures/14-contours-heatmaps.html#so-how-do-we-display-densities-for-2d-data",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "So how do we display densities for 2D data?",
    "text": "So how do we display densities for 2D data?"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#how-to-read-contour-plots",
    "href": "lectures/14-contours-heatmaps.html#how-to-read-contour-plots",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "How to read contour plots?",
    "text": "How to read contour plots?\nBest known in topology: outlines (contours) denote levels of elevation"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#display-2d-contour-plot",
    "href": "lectures/14-contours-heatmaps.html#display-2d-contour-plot",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Display 2D contour plot",
    "text": "Display 2D contour plot\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  geom_point(alpha = 0.2) +\n  geom_density2d() +\n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-1",
    "href": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-1",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Display 2D contour plot",
    "text": "Display 2D contour plot\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  geom_density2d() +\n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-2",
    "href": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-2",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Display 2D contour plot",
    "text": "Display 2D contour plot\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  stat_density2d(aes(fill = after_stat(level)), geom = \"polygon\") +\n  coord_fixed() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#visualizing-grid-heat-maps",
    "href": "lectures/14-contours-heatmaps.html#visualizing-grid-heat-maps",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Visualizing grid heat maps",
    "text": "Visualizing grid heat maps\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  stat_density2d(aes(fill = after_stat(density)), \n                 geom = \"tile\", contour = FALSE) + \n  coord_fixed() +\n  scale_fill_gradient(low = \"white\", high = \"red\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#alternative-idea-hexagonal-binning",
    "href": "lectures/14-contours-heatmaps.html#alternative-idea-hexagonal-binning",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Alternative idea: hexagonal binning",
    "text": "Alternative idea: hexagonal binning\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  geom_hex() +\n  coord_fixed() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#lebron-james-shots-from-hoopr",
    "href": "lectures/14-contours-heatmaps.html#lebron-james-shots-from-hoopr",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "LeBron James’ shots from hoopR",
    "text": "LeBron James’ shots from hoopR\n\nlebron_shots &lt;- read_csv(\"https://raw.githubusercontent.com/ryurko/DataViz-Class-Data/main/lebron_shots.csv\")\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = 0.4) +\n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-3",
    "href": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-3",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Display 2D contour plot",
    "text": "Display 2D contour plot\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = 0.4) +\n  geom_density2d(binwidth = 0.0001) + \n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#alternative-idea-hexagonal-binning-1",
    "href": "lectures/14-contours-heatmaps.html#alternative-idea-hexagonal-binning-1",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Alternative idea: hexagonal binning",
    "text": "Alternative idea: hexagonal binning\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_hex() +\n  coord_fixed() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#what-about-high-dimensional-data",
    "href": "lectures/14-contours-heatmaps.html#what-about-high-dimensional-data",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "What about high-dimensional data?",
    "text": "What about high-dimensional data?\nConsider this dataset containing nutritional information about Starbucks drinks:\n\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))\nstarbucks |&gt; slice(1)\n\n# A tibble: 1 × 15\n  product_name              size   milk  whip serv_size_m_l calories total_fat_g\n  &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 brewed coffee - dark roa… short     0     0           236        3         0.1\n# ℹ 8 more variables: saturated_fat_g &lt;dbl&gt;, trans_fat_g &lt;dbl&gt;,\n#   cholesterol_mg &lt;dbl&gt;, sodium_mg &lt;dbl&gt;, total_carbs_g &lt;dbl&gt;, fiber_g &lt;dbl&gt;,\n#   sugar_g &lt;dbl&gt;, caffeine_mg &lt;dbl&gt;\n\n\nHow do we visualize this dataset? \n\nTedious task: make a series of pairs plots (one giant pairs plot would overwhelming)"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#what-about-high-dimensional-data-1",
    "href": "lectures/14-contours-heatmaps.html#what-about-high-dimensional-data-1",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "What about high-dimensional data?",
    "text": "What about high-dimensional data?\n\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))\nstarbucks |&gt; slice(1)\n\n# A tibble: 1 × 15\n  product_name              size   milk  whip serv_size_m_l calories total_fat_g\n  &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 brewed coffee - dark roa… short     0     0           236        3         0.1\n# ℹ 8 more variables: saturated_fat_g &lt;dbl&gt;, trans_fat_g &lt;dbl&gt;,\n#   cholesterol_mg &lt;dbl&gt;, sodium_mg &lt;dbl&gt;, total_carbs_g &lt;dbl&gt;, fiber_g &lt;dbl&gt;,\n#   sugar_g &lt;dbl&gt;, caffeine_mg &lt;dbl&gt;\n\n\nGoals to keep in mind with visualizing high-dimensional data:\n\nVisualize structure among observations based on distances and projections (next lecture)\nVisualize structure among variables using correlation as “distance”"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#correlogram-to-visualize-correlation-matrix",
    "href": "lectures/14-contours-heatmaps.html#correlogram-to-visualize-correlation-matrix",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Correlogram to visualize correlation matrix",
    "text": "Correlogram to visualize correlation matrix\nUse the ggcorrplot package:\n\nstarbucks_quant_cor &lt;- cor(dplyr::select(starbucks, serv_size_m_l:caffeine_mg))\n\nlibrary(ggcorrplot)\nggcorrplot(starbucks_quant_cor)"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#options-to-customize-correlogram",
    "href": "lectures/14-contours-heatmaps.html#options-to-customize-correlogram",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Options to customize correlogram",
    "text": "Options to customize correlogram\n\nggcorrplot(starbucks_quant_cor,\n           type = \"lower\", method = \"circle\")"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#reorder-variables-based-on-correlation",
    "href": "lectures/14-contours-heatmaps.html#reorder-variables-based-on-correlation",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Reorder variables based on correlation",
    "text": "Reorder variables based on correlation\n\nggcorrplot(starbucks_quant_cor,\n           type = \"lower\", method = \"circle\",\n           hc.order = TRUE)"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#heatmap-displays-of-observations",
    "href": "lectures/14-contours-heatmaps.html#heatmap-displays-of-observations",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Heatmap displays of observations",
    "text": "Heatmap displays of observations\n\nheatmap(as.matrix(dplyr::select(starbucks, serv_size_m_l:caffeine_mg)),\n        scale = \"column\", \n        labRow = starbucks$product_name,\n        cexRow = .5, cexCol = .75,\n        Rowv = NA, Colv = NA)"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps",
    "href": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Manual version of heatmaps",
    "text": "Manual version of heatmaps\n\nstarbucks |&gt;\n  dplyr::select(product_name, serv_size_m_l:caffeine_mg) |&gt;\n  pivot_longer(serv_size_m_l:caffeine_mg,\n               names_to = \"variable\",\n               values_to = \"raw_value\") |&gt;\n  group_by(variable) |&gt;\n  mutate(std_value = (raw_value - mean(raw_value)) / sd(raw_value)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(y = variable, x = product_name, fill = std_value)) +\n  geom_tile() +\n  theme_light() +\n  theme(axis.text.x = element_text(size = 1, angle = 45),\n        legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-output",
    "href": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-output",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Manual version of heatmaps",
    "text": "Manual version of heatmaps"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-1",
    "href": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-1",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Manual version of heatmaps",
    "text": "Manual version of heatmaps\n\nstarbucks |&gt;\n  dplyr::select(product_name, serv_size_m_l:caffeine_mg) |&gt;\n  mutate(product_name = fct_reorder(product_name, calories)) |&gt;\n  pivot_longer(serv_size_m_l:caffeine_mg,\n               names_to = \"variable\",\n               values_to = \"raw_value\") |&gt;\n  group_by(variable) |&gt;\n  mutate(std_value = (raw_value - mean(raw_value)) / sd(raw_value)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(y = variable, x = product_name, fill = std_value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") +\n  theme_light() +\n  theme(axis.text.x = element_text(size = 1, angle = 45),\n        legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-1-output",
    "href": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-1-output",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Manual version of heatmaps",
    "text": "Manual version of heatmaps"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#parallel-coordinates-plot-with-ggparcoord",
    "href": "lectures/14-contours-heatmaps.html#parallel-coordinates-plot-with-ggparcoord",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Parallel coordinates plot with ggparcoord",
    "text": "Parallel coordinates plot with ggparcoord\n\nlibrary(GGally)\nstarbucks |&gt;\n  ggparcoord(columns = 5:15, alphaLines = .1) +\n  theme(axis.text.x = element_text(angle = 90))"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#easier-example-with-penguins",
    "href": "lectures/14-contours-heatmaps.html#easier-example-with-penguins",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Easier example with penguins…",
    "text": "Easier example with penguins…\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2, groupColumn = \"species\",\n             order = c(6, 5, 3, 4))"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#recap-and-next-steps",
    "href": "lectures/14-contours-heatmaps.html#recap-and-next-steps",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nWe can extend kernel density estimation from 1 to \\(p\\)-dimensions (don’t say easily though…)\nContour plots: Common way to visualize two-dimensional densities\nHeat maps: divide the space into a grid, and then color the grid according to high/low densities\nHexagonal bins: creating histograms in 2D\nCorrelograms and Parallel Coordinates Plots are helpful tools for visualizing high-dimensional data\n\n\n\n\nHW5 is due Wednesday March 19th and you do NOT have lab this Friday!\nNext time: Visualizing Distances and MDS"
  },
  {
    "objectID": "lectures/19-time-series.html#announcements-previously-and-today",
    "href": "lectures/19-time-series.html#announcements-previously-and-today",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW7 is due Wednesday March April 2nd by 11:59 PM ET\nGraphics Critique 2 is due TONIGHT!\nYou do NOT have lab this week - CARNIVAL!\n\n\n\nLast time:\n\nDiscussed various aspects of visualizing trends\nWhen visualizing many lines, often useful to consider highlighting a small subset\n\nTODAY: Time series, autocorrelation, and seasonal decomposition"
  },
  {
    "objectID": "lectures/19-time-series.html#things-of-interest-for-time-series-data",
    "href": "lectures/19-time-series.html#things-of-interest-for-time-series-data",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Things of interest for time series data",
    "text": "Things of interest for time series data\nTime series can be characterized by three features:\n\nTrends: Does the variable increase or decrease over time, on average?\nSeasonality: Are there changes in the variable that regularly happen (e.g., every winter, every hour, etc.)? Sometimes called periodicity.\nNoise: Variation in the variable beyond average trends and seasonality.\n\nMoving averages are a starting point for visualizing how a trend changes over time"
  },
  {
    "objectID": "lectures/19-time-series.html#be-responsible-with-your-axes",
    "href": "lectures/19-time-series.html#be-responsible-with-your-axes",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Be responsible with your axes!",
    "text": "Be responsible with your axes!"
  },
  {
    "objectID": "lectures/19-time-series.html#be-responsible-with-your-axes-1",
    "href": "lectures/19-time-series.html#be-responsible-with-your-axes-1",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Be responsible with your axes!",
    "text": "Be responsible with your axes!"
  },
  {
    "objectID": "lectures/19-time-series.html#moving-average-plots",
    "href": "lectures/19-time-series.html#moving-average-plots",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Moving Average Plots",
    "text": "Moving Average Plots\nThe Financial Times COVID-19 plots displayed a moving average (sometimes called a rolling average)\nIntuition\n\nDivide your data into small subsets (“windows”)\nCompute the average within each window\nConnect the averages together to make a trend line\n\n\nSometimes called a simple moving average\nThis is exactly what we did with LOESS… we called this a sliding window, but it’s the same thing"
  },
  {
    "objectID": "lectures/19-time-series.html#how-are-moving-averages-computed",
    "href": "lectures/19-time-series.html#how-are-moving-averages-computed",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "How are moving averages computed?",
    "text": "How are moving averages computed?\nIntuition\n\nDivide your data into small subsets (windows)\nCompute the average within each window\nConnect the averages together to make a trend line\n\n\nMathematically, a moving average can be written as the following:\n\\[\\mu_k = \\frac{\\sum_{t=k - h + 1}^k X_t}{h}\\]\n\nLarge \\(h\\): Smooth line; captures global trends\nSmall \\(h\\): Jagged/volatile line; captures local trends"
  },
  {
    "objectID": "lectures/19-time-series.html#working-with-time-series",
    "href": "lectures/19-time-series.html#working-with-time-series",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Working with Time Series",
    "text": "Working with Time Series\nco2: Mauna Loa Atmospheric CO2 Concentration dataset (monthly \\(\\text{CO}^2\\) concentration 1959 to 1997)\n\nco2_tbl |&gt;\n  ggplot(aes(x = obs_i, y = co2_val)) + \n  geom_line() + \n  labs(x = \"Time index\", y = \"CO2 (ppm)\")"
  },
  {
    "objectID": "lectures/19-time-series.html#formatting-dates",
    "href": "lectures/19-time-series.html#formatting-dates",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Formatting Dates",
    "text": "Formatting Dates\nCan use as.Date() to create time indexes.\n\nDefault format is Year/Month/Day. For something else, need to specify format in as.Date() (e.g., format = \"%m/%d/%Y\")"
  },
  {
    "objectID": "lectures/19-time-series.html#use-scale_x_date-to-create-interpretable-axis-labels",
    "href": "lectures/19-time-series.html#use-scale_x_date-to-create-interpretable-axis-labels",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Use scale_x_date() to create interpretable axis labels",
    "text": "Use scale_x_date() to create interpretable axis labels"
  },
  {
    "objectID": "lectures/19-time-series.html#other-moving-averages",
    "href": "lectures/19-time-series.html#other-moving-averages",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Other Moving Averages",
    "text": "Other Moving Averages\nTwo other common averages: Cumulative moving averages and weighted moving averages.\n\nCumulative moving average: The average at time \\(k\\) is the average of all points at and before \\(k\\). Mathematically:\n\n\\[\\mu_k^{(CMA)} = \\frac{\\sum_{t=1}^k X_t}{k}\\]\n\n\nWeighted moving average: Same as simple moving average, but different measurements get different weights for the average.\n\n\\[\\mu_k^{(WMA)} = \\frac{\\sum_{t=k - h + 1}^k X_t \\cdot w_t}{ \\sum_{t=k - h + 1}^k w_t}\\]"
  },
  {
    "objectID": "lectures/19-time-series.html#working-with-lags",
    "href": "lectures/19-time-series.html#working-with-lags",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Working with lags",
    "text": "Working with lags\nTime series data is fundamentally different from other data problems we’ve worked with because measurements are not independent\nObvious example: The temperature today is correlated with temperature yesterday. (Maybe not in Pittsburgh?)\n\nImportant term: lags. Used to determine if one time point influences future time points.\nLag 1: Comparing time series at time \\(t\\) with time series at time \\(t - 1\\).\nLag 2: Comparing time series at time \\(t\\) with time series at time \\(t - 2\\).\nAnd so on…\n\n\nLet’s say we have time measurements \\((X_1, X_2, X_3, X_4, X_5)\\).\nThe \\(\\ell = 1\\) lag is \\((X_2, X_3, X_4, X_5)\\) vs \\((X_1, X_2, X_3, X_4)\\).\n\n\nThe \\(\\ell = 2\\) lag is \\((X_3, X_4, X_5)\\) vs \\((X_1, X_2, X_3)\\).\nConsider: Are previous outcomes (lags) predictive of future outcomes?"
  },
  {
    "objectID": "lectures/19-time-series.html#autocorrelation",
    "href": "lectures/19-time-series.html#autocorrelation",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nAutocorrelation: Correlation between a time series and a lagged version of itself.\nDefine \\(r_{\\ell}\\) as the correlation between a time series and Lag \\(\\ell\\) of that time series.\n\nLag 1: \\(r_1\\) is correlation between \\((X_2, X_3, X_4, X_5)\\) and \\((X_1,X_2,X_3,X_4)\\)\nLag 2: \\(r_2\\) is correlation between \\((X_3, X_4, X_5)\\) and \\((X_1,X_2,X_3)\\)\nAnd so on…\n\n\nCommon diagnostic: Plot \\(\\ell\\) on x-axis, \\(r_{\\ell}\\) on y-axis.\nTells us if correlations are “significantly large” or “significantly small” for certain lags\nTo make an autocorrelation plot, we use the acf() function; the ggplot version uses autoplot()"
  },
  {
    "objectID": "lectures/19-time-series.html#autocorrelation-plots",
    "href": "lectures/19-time-series.html#autocorrelation-plots",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Autocorrelation plots",
    "text": "Autocorrelation plots\n\nlibrary(ggfortify)\nauto_corr &lt;- acf(co2_tbl$co2_val, plot = FALSE)\nautoplot(auto_corr)"
  },
  {
    "objectID": "lectures/19-time-series.html#autocorrelation-plots-and-seasonality",
    "href": "lectures/19-time-series.html#autocorrelation-plots-and-seasonality",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Autocorrelation Plots and Seasonality",
    "text": "Autocorrelation Plots and Seasonality\nWith strong global trends, autocorrelations will be very positive.\nHelpful: Visualize autocorrelations after removing the global trend (compute moving average with rollapply())"
  },
  {
    "objectID": "lectures/19-time-series.html#autocorrelation-plots-and-seasonality-1",
    "href": "lectures/19-time-series.html#autocorrelation-plots-and-seasonality-1",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Autocorrelation Plots and Seasonality",
    "text": "Autocorrelation Plots and Seasonality"
  },
  {
    "objectID": "lectures/19-time-series.html#seasonality-decomposition",
    "href": "lectures/19-time-series.html#seasonality-decomposition",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Seasonality Decomposition",
    "text": "Seasonality Decomposition\nRemember that there are three main components to a time series:\n\nAverage trends\nSeasonality\nNoise\n\n\nUse ggsdc() (from ggseas) to decompose a time series into these three components\n\nPlots the observed time series.\nPlots a loess curve as the global trend.\nPlots another loess curve on (observed - trend) as the seasonality.\nPlots the noise (observed - trend - seasonality)."
  },
  {
    "objectID": "lectures/19-time-series.html#seasonality-decomposition-1",
    "href": "lectures/19-time-series.html#seasonality-decomposition-1",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Seasonality Decomposition",
    "text": "Seasonality Decomposition\n\nco2_tbl |&gt;\n  ggsdc(aes(obs_date, co2_val), frequency = 12, method = \"stl\", s.window = 12) +\n  geom_line() + labs(x = \"Year\", y = \"CO2 (ppm)\")"
  },
  {
    "objectID": "lectures/19-time-series.html#recap-and-next-steps",
    "href": "lectures/19-time-series.html#recap-and-next-steps",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nFundamental characteristic of time series data: measurements are dependent over time\nThe things to look out for in time series data are: (1) Average trends, (2) Seasonality, (3) Noise\nAutocorrelation plots are also useful for assessing average trends and seasonality.\n\n\n\nHW7 is due Wednesday!\nGraphics Critique 2 is due TONIGHT!\n\n\n\n\nNext time: Animations, infographics, and annotations\nRecommended reading: CW CH 13 Visualizing time series and other functions of an independent variable, CW CH 14 Visualizing trends"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#announcements-previously-and-today",
    "href": "lectures/03-1dcat-infer.html#announcements-previously-and-today",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW1 is posted and due Wednesday Jan 29\nYou have Lab 2 again on Friday\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM\nPerry (zoom): Wednesdays @ 11 AM\n\n\n\n\n\nDiscussed 1D categorical data, basic summaries with counts and proportions\nIntroduced different area plots for visualizing 1D categorical data\nWe make bar charts for categorical data (I told you repeatedly that pie charts suck)\n\n\n\n\n\nTODAY: quantify and display uncertainty for 1D categorical data\n\nAdd confidence intervals to bar charts\nReview the Chi-Squared Test\nDiscuss connections between visualizations and statistical significance"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#crimes-against-bar-charts",
    "href": "lectures/03-1dcat-infer.html#crimes-against-bar-charts",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Crimes against bar charts",
    "text": "Crimes against bar charts"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#crimes-against-bar-charts-1",
    "href": "lectures/03-1dcat-infer.html#crimes-against-bar-charts-1",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Crimes against bar charts",
    "text": "Crimes against bar charts"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#what-does-a-bar-chart-show",
    "href": "lectures/03-1dcat-infer.html#what-does-a-bar-chart-show",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "What does a bar chart show?",
    "text": "What does a bar chart show?\nMarginal Distribution\n\nAssume categorical variable \\(X\\) has \\(K\\) categories: \\(C_1, \\dots, C_K\\)\nTrue marginal distribution of \\(X\\):\n\n\\[\nP(X = C_j) = p_j,\\ j \\in \\{ 1, \\dots, K \\}\n\\]\n\nWe have access to the Empirical Marginal Distribution\n\nObserved distribution of \\(X\\), our best estimate (MLE) of the marginal distribution of \\(X\\): \\(\\hat{p}_1\\), \\(\\hat{p}_2\\), \\(\\dots\\), \\(\\hat{p}_K\\)\n\n\ntable(penguins$species) / nrow(penguins)\n\n\n   Adelie Chinstrap    Gentoo \n0.4418605 0.1976744 0.3604651"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#bar-charts-with-proportions",
    "href": "lectures/03-1dcat-infer.html#bar-charts-with-proportions",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Bar charts with proportions",
    "text": "Bar charts with proportions\n\nafter_stat() indicates the aesthetic mapping is performed after statistical transformation\nUse after_stat(count) to access the stat_count() called by geom_bar()\n\n\npenguins |&gt;\n  ggplot(aes(x = species)) +\n  geom_bar(aes(y = after_stat(count) / sum(after_stat(count)))) + \n  labs(y = \"Proportion\")"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#compute-and-display-the-proportions-directly",
    "href": "lectures/03-1dcat-infer.html#compute-and-display-the-proportions-directly",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Compute and display the proportions directly",
    "text": "Compute and display the proportions directly\n\nUse group_by(), summarize(), and mutate() in a pipeline to compute then display the proportions directly\nNeed to indicate we are displaying the y axis as given, i.e., the identity function\n\n\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  mutate(total = sum(count), \n         prop = count / total) |&gt; \n  ggplot(aes(x = species)) +\n  geom_bar(aes(y = prop), stat = \"identity\")"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#compute-and-display-the-proportions-directly-output",
    "href": "lectures/03-1dcat-infer.html#compute-and-display-the-proportions-directly-output",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Compute and display the proportions directly",
    "text": "Compute and display the proportions directly"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#what-about-uncertainty",
    "href": "lectures/03-1dcat-infer.html#what-about-uncertainty",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "What about uncertainty?",
    "text": "What about uncertainty?\n\nQuantify uncertainty for our estimate \\(\\hat{p}_j = \\frac{n_j}{n}\\) with the standard error:\n\n\\[\nSE(\\hat{p}_j) = \\sqrt{\\frac{\\hat{p}_j(1 - \\hat{p}_j)}{n}}\n\\]\n\n\nCompute \\(\\alpha\\)-level confidence interval (CI) as \\(\\hat{p}_j \\pm z_{1 - \\alpha / 2} \\cdot SE(\\hat{p}_j)\\)\nGood rule-of-thumb: construct 95% CI using \\(\\hat{p}_j \\pm 2 \\cdot SE(\\hat{p}_j)\\)\nApproximation justified by CLT, so CI could include values outside of [0,1]"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#add-standard-errors-to-bars",
    "href": "lectures/03-1dcat-infer.html#add-standard-errors-to-bars",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Add standard errors to bars",
    "text": "Add standard errors to bars\n\nNeed to remember each CI is for each \\(\\hat{p}_j\\) marginally, not jointly\nHave to be careful with multiple testing\n\n\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  mutate(total = sum(count), \n         prop = count / total,\n         se = sqrt(prop * (1 - prop) / total), \n         lower = prop - 2 * se, \n         upper = prop + 2 * se) |&gt; \n  ggplot(aes(x = species)) +\n  geom_bar(aes(y = prop), stat = \"identity\") +\n  geom_errorbar(aes(ymin = lower, ymax = upper), \n                color = \"red\")"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#add-standard-errors-to-bars-output",
    "href": "lectures/03-1dcat-infer.html#add-standard-errors-to-bars-output",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Add standard errors to bars",
    "text": "Add standard errors to bars"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#useful-to-order-categories-by-frequency-with-forcats",
    "href": "lectures/03-1dcat-infer.html#useful-to-order-categories-by-frequency-with-forcats",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Useful to order categories by frequency with forcats",
    "text": "Useful to order categories by frequency with forcats\n\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  mutate(total = sum(count), \n         prop = count / total,\n         se = sqrt(prop * (1 - prop) / total), \n         lower = prop - 2 * se, \n         upper = prop + 2 * se,\n         species = fct_reorder(species, prop)) |&gt;\n  ggplot(aes(x = species)) +\n  geom_bar(aes(y = prop), stat = \"identity\") +\n  geom_errorbar(aes(ymin = lower, ymax = upper), \n                color = \"red\")"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#useful-to-order-categories-by-frequency-with-forcats-output",
    "href": "lectures/03-1dcat-infer.html#useful-to-order-categories-by-frequency-with-forcats-output",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Useful to order categories by frequency with forcats",
    "text": "Useful to order categories by frequency with forcats"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#dont-do-this",
    "href": "lectures/03-1dcat-infer.html#dont-do-this",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Don’t do this…",
    "text": "Don’t do this…"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#chi-squared-test-for-1d-categorical-data",
    "href": "lectures/03-1dcat-infer.html#chi-squared-test-for-1d-categorical-data",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Chi-squared test for 1D categorical data:",
    "text": "Chi-squared test for 1D categorical data:\n\n\nNull hypothesis \\(H_0\\): \\(p_1 = p_2 = \\dots = p_K\\), compute the test statistic:\n\n\\[\n\\chi^2 = \\sum_{j=1}^K \\frac{(O_j - E_j)^2}{E_j}\n\\]\n\n\\(O_j\\): observed counts in category \\(j\\)\n\\(E_j\\): expected counts under \\(H_0\\), i.e., each category is equally to occur \\(n / K = p_1 = p_2 = \\dots = p_K\\)\n\n\n\n\nchisq.test(table(penguins$species))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(penguins$species)\nX-squared = 31.907, df = 2, p-value = 1.179e-07"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#hypothesis-testing-review",
    "href": "lectures/03-1dcat-infer.html#hypothesis-testing-review",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Hypothesis testing review",
    "text": "Hypothesis testing review\n\n\n\nComputing \\(p\\)-values works like this:\n\nChoose a test statistic.\nCompute the test statistic in your dataset.\nIs test statistic “unusual” compared to what I would expect under \\(H_0\\)?\nCompare \\(p\\)-value to target error rate \\(\\alpha\\) (typically referred to as target level \\(\\alpha\\) )\nTypically choose \\(\\alpha = 0.05\\)\n\ni.e., if we reject null hypothesis at \\(\\alpha = 0.05\\) then, assuming \\(H_0\\) is true, there is a 5% chance it is a false positive (aka Type 1 error)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#chi-squared-test-for-1d-categorical-data-1",
    "href": "lectures/03-1dcat-infer.html#chi-squared-test-for-1d-categorical-data-1",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Chi-squared test for 1D categorical data:",
    "text": "Chi-squared test for 1D categorical data:\n\n\nNull hypothesis \\(H_0\\): \\(p_1 = p_2 = \\dots = p_K\\), compute the test statistic:\n\n\\[\n\\chi^2 = \\sum_{j=1}^K \\frac{(O_j - E_j)^2}{E_j}\n\\]\n\n\\(O_j\\): observed counts in category \\(j\\)\n\\(E_j\\): expected counts under \\(H_0\\), i.e., each category is equally to occur \\(n / K = p_1 = p_2 = \\dots = p_K\\)\n\n\n\nchisq.test(table(penguins$species))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(penguins$species)\nX-squared = 31.907, df = 2, p-value = 1.179e-07"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#graphics-versus-statistical-inference",
    "href": "lectures/03-1dcat-infer.html#graphics-versus-statistical-inference",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Graphics versus Statistical Inference",
    "text": "Graphics versus Statistical Inference\n\nReminder Anscombe’s Quartet: where statistical inference was the same but the graphics were very different\n\n\n\nThe opposite can be true! Graphics are the same, but statistical inference is very different…"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14",
    "href": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)",
    "text": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-1",
    "href": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-1",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)",
    "text": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-2",
    "href": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-2",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)",
    "text": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-3",
    "href": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-3",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)",
    "text": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#power-under-this-scenario-2n4-n4-n4",
    "href": "lectures/03-1dcat-infer.html#power-under-this-scenario-2n4-n4-n4",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Power under this scenario: (2n/4, n/4, n/4)",
    "text": "Power under this scenario: (2n/4, n/4, n/4)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#how-do-we-combine-graphs-with-inference",
    "href": "lectures/03-1dcat-infer.html#how-do-we-combine-graphs-with-inference",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "How do we combine graphs with inference?",
    "text": "How do we combine graphs with inference?\n\nSimply add \\(p\\)-values (or other info) to graph via text\nAdd confidence intervals to the graph\n\n\nNeed to remember what each CI is for!\nOur CIs on previous slides are for each \\(\\hat{p}_j\\) marginally, NOT jointly\nHave to be careful with multiple testing…"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#cis-will-visually-capture-uncertainty-in-estimates",
    "href": "lectures/03-1dcat-infer.html#cis-will-visually-capture-uncertainty-in-estimates",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "CIs will visually capture uncertainty in estimates",
    "text": "CIs will visually capture uncertainty in estimates"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#rough-rules-of-thumb-for-comparing-cis-on-bar-charts",
    "href": "lectures/03-1dcat-infer.html#rough-rules-of-thumb-for-comparing-cis-on-bar-charts",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "(Rough) Rules-of-thumb for comparing CIs on bar charts",
    "text": "(Rough) Rules-of-thumb for comparing CIs on bar charts\n\n\nComparing overlap of two CIs is NOT exactly the same as directly testing for a significant difference…\n\nReally you want CI( \\(\\hat{p}_1 - \\hat{p}_2\\) ), not CI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) )\nCI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) ) not overlapping implies \\(0 \\notin\\) CI( \\(\\hat{p}_1 - \\hat{p}_2\\) )\nHowever CI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) ) overlapping DOES NOT imply \\(0 \\in\\) CI( \\(\\hat{p}_1 - \\hat{p}_2\\) )\n\n\nRoughly speaking:\n\nIf CIs don’t overlap \\(\\rightarrow\\) significant difference\nIf CIs overlap a little \\(\\rightarrow\\) ambiguous\nIf CIs overlap a lot \\(\\rightarrow\\) no significant difference\n\n\n\n\nBut if we’re comparing more than two CIs simultaneously, we need to account for multiple testing!\n\nWhen you look for all non-overlapping CIs: implicitly making \\(\\binom{K}{2} = \\frac{K!}{2!(K-2)!}\\) pairwise tests in your head!"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#corrections-for-multiple-testing",
    "href": "lectures/03-1dcat-infer.html#corrections-for-multiple-testing",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Corrections for multiple testing",
    "text": "Corrections for multiple testing\n\n\nIn those bar plots, when we determine whether CIs overlap we make 3 comparisons:\n\nA vs B\nA vs C\nB vs C\n\n\nThis is a multiple testing issue\n\n\n\n\nIn short: we will make Type 1 errors (chance of false rejecting) more than 5% of the time!\nReminder: Type 1 error = Rejecting \\(H_0\\) when \\(H_0\\) is true\ne.g., CIs don’t overlap but actually \\(H_0: p_A = p_B\\) is true\nIf only interested in A vs B and nothing else, then just construct 95% CI for A vs B and control error rate at 5%\nHowever, if we construct several CIs, where A vs B is just one comparison we make, our Type 1 error rate &gt; 5%!"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#corrections-for-multiple-testing-1",
    "href": "lectures/03-1dcat-infer.html#corrections-for-multiple-testing-1",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Corrections for multiple testing",
    "text": "Corrections for multiple testing\n\nVast literature on corrections for multiple testing (beyond the scope of this class… but in my thesis!)\nBut you should understand the following:\n\nCorrections for multiple testing inflate \\(p\\)-values (i.e., make them bigger)\nEquivalently, they inflate CIs (i.e., make them wider)\nPurpose of these corrections is to control Type 1 error rate \\(\\leq 5\\%\\)\n\n\n\n\nWe’ll focus on the Bonferroni correction, which inflates \\(p\\)-values the most but is easy to implement and very popular:\n\nWe usually reject null hypothesis when \\(p\\)-value \\(\\leq .05\\)\nBonferroni: if making \\(K\\) comparisons, reject only if \\(p\\)-value \\(\\leq .05/K\\)\nFor CIs: instead of plotting 95% CIs, we plot (1 - \\(0.05/K\\))% CIs\n\ne.g., for \\(K = 3\\) then plot 98.3% CIs"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#impact-of-bonferroni-correction-on-cis",
    "href": "lectures/03-1dcat-infer.html#impact-of-bonferroni-correction-on-cis",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Impact of Bonferroni correction on CIs…",
    "text": "Impact of Bonferroni correction on CIs…"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#recap-and-next-steps",
    "href": "lectures/03-1dcat-infer.html#recap-and-next-steps",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nBar charts display the empirical distribution of the categorical variable ( \\(\\hat{p}_1, \\dots, \\hat{p}_K\\) )\nChi-squared test is a global test for 1D categorical data, testing \\(H_0 : p_1 = \\cdot \\cdot \\cdot = p_K\\)\n\nDoes not tell us which probabilities differ!\n\nCan visualize CIs for each \\(\\hat{p}_1\\), \\(\\dots\\), \\(\\hat{p}_K\\), but need to deal with multiple testing\nGraphs with the same trends can display very different statistical significance (largely due to sample size)\n\n\n\n\n\nHW1 is due next week and you have Lab 2 on Friday!\nNext time: 2D categorical data\nRecommended reading:\n\nCW Chapter 16.2 Visualizing the uncertainty of point estimates"
  },
  {
    "objectID": "lectures/01-intro.html#who-am-i",
    "href": "lectures/01-intro.html#who-am-i",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Who am I?",
    "text": "Who am I?\n\n\n\n\nAssistant Teaching Professor\nFinished Phd in Statistics @ CMU in May 2022\nPreviously BS in Statistics @ CMU in 2015\nResearch interests: sports analytics, natural language processing, clustering, selective inference\n\n\n\n\n\nIndustry experience: finance before returning to grad school and also as data scientist in professional sports"
  },
  {
    "objectID": "lectures/01-intro.html#why-do-we-visualize-data",
    "href": "lectures/01-intro.html#why-do-we-visualize-data",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Why do we visualize data?",
    "text": "Why do we visualize data?"
  },
  {
    "objectID": "lectures/01-intro.html#course-structure",
    "href": "lectures/01-intro.html#course-structure",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Course Structure",
    "text": "Course Structure\n\n\n\nLectures on Mondays/Wednesdays\n\nAll slides and demos posted on https://ryurko.github.io/statds-36315-spring25/\nParticipate and ask questions!\n\nWeekly homeworks due Wednesdays by 11:59 PM ET\nWeekly Friday labs due Saturdays by 11:59 AM ET\n\n\n\n\nTwo Graphics Critiques of Data Viz in the Wild (due Feb 28 and Mar 31)\nTake-home exam on Wednesday, Feb 26th\nFinal project with individual and group grade\n\nWork in teams on dataset you choose\nIn-class presentations during final week of class\nPublic facing HTML report due during finals week"
  },
  {
    "objectID": "lectures/01-intro.html#course-logistics",
    "href": "lectures/01-intro.html#course-logistics",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Course logistics",
    "text": "Course logistics\n\n\nAll homework/lab assignments will be in Quarto. You’ll generate a PDF, which you’ll submit on Gradescope\nMake sure R and RStudio are installed on your computer!\nHW0 due Wednesday Jan 15 at 11:59 PM ET: install R/RStudio, install/load tidyverse, render to PDF, and post to Gradescope\n\nHave any installation issues? Post to the course Piazza!\n\nPiazza: all questions about course material, HWs, exam, and projects\n\nDo NOT share code on Piazza\n\nOnly email for address administrative/logistic issues\nLab attendance on Friday is mandatory - submit lab assignment but don’t attend YOU LOSE 20PTS!\n\nQuestions about lab assignments will only be answered during lab\nIf you need to miss a lab due to illness, interviews, emergencies, etc., email me 48 hours in advance"
  },
  {
    "objectID": "lectures/01-intro.html#important-hw0-is-due-wednesday-night",
    "href": "lectures/01-intro.html#important-hw0-is-due-wednesday-night",
    "title": "Introduction and the Grammar of Graphics",
    "section": "IMPORTANT: HW0 is due Wednesday night",
    "text": "IMPORTANT: HW0 is due Wednesday night\nAs seen in today’s Canvas announcement - you must submit HW0 by Wednesday night!\n\nThis is just to make sure you have everything installed correctly and can render .qmd files to PDF\n\nRead through all of the directions in HW0 carefully!\nYou will stop saving your workspace upon exiting RStudio!\nYou will need to be set-up for the first lab on Friday"
  },
  {
    "objectID": "lectures/01-intro.html#course-objectives-read-the-syllabus",
    "href": "lectures/01-intro.html#course-objectives-read-the-syllabus",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Course Objectives (read the syllabus)",
    "text": "Course Objectives (read the syllabus)\nLearn useful principles for making appropriate statistical graphics.\nCritique existing graphs and remake better ones.\nVisualize statistical analyses to facilitate communication.\nPinpoint the statistical claims you can/cannot make from graphics.\nWrite and speak publicly about statistical graphics.\nPractice tidy data manipulation in R using the tidyverse\nPractice reproducible workflows with Quarto"
  },
  {
    "objectID": "lectures/01-intro.html#what-do-i-mean-by-tidy-data",
    "href": "lectures/01-intro.html#what-do-i-mean-by-tidy-data",
    "title": "Introduction and the Grammar of Graphics",
    "section": "What do I mean by tidy data?",
    "text": "What do I mean by tidy data?\nData are often stored in tabular (or matrix) form:\n\nlibrary(palmerpenguins)\npenguins |&gt; slice(1:5)\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "lectures/01-intro.html#the-grammar-of-graphics",
    "href": "lectures/01-intro.html#the-grammar-of-graphics",
    "title": "Introduction and the Grammar of Graphics",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\n\n\nOriginally defined by Leland Wilkinson\n\n\ndata\ngeometries: type of geometric objects to represent data, e.g., points, lines\naesthetics: visual characteristics of geometric objects to represent data, e.g., position, size\nscales: how each aesthetic is converted into values on the graph, e.g., color scales\nstats: statistical transformations to summarize data, e.g., counts, means, regression lines\nfacets: split data and view as multiple graphs\ncoordinate system: 2D space the data are projected onto, e.g., Cartesian coordinates\n\n\n\n\nHadley Wickham created ggplot2\n\n\ndata\ngeom\naes: mappings of columns to geometric objects\nscale: one scale for each aes variable\nstat\nfacet\ncoord\nlabs: labels/guides for each variable and other parts of the plot, e.g., title, subtitle, caption\ntheme: customization of plot layout"
  },
  {
    "objectID": "lectures/01-intro.html#start-with-the-data",
    "href": "lectures/01-intro.html#start-with-the-data",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Start with the data",
    "text": "Start with the data\n\n\nAccess ggplot2 from the tidyverse:\n\nlibrary(tidyverse)\nggplot(data = penguins)\n\n\nOr equivalently using |&gt;:\n\npenguins |&gt;\n  ggplot()"
  },
  {
    "objectID": "lectures/01-intro.html#need-to-add-geometric-objects",
    "href": "lectures/01-intro.html#need-to-add-geometric-objects",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Need to add geometric objects!",
    "text": "Need to add geometric objects!\n\n\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, \n             y = bill_depth_mm)) + \n  geom_point()\n\n\n\npenguins %&gt;%\n  ggplot(mapping = aes(x = bill_length_mm,\n                       y = bill_depth_mm)) + \n  geom_point()"
  },
  {
    "objectID": "lectures/01-intro.html#modify-scale-add-statistical-summary-and-so-on",
    "href": "lectures/01-intro.html#modify-scale-add-statistical-summary-and-so-on",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Modify scale, add statistical summary, and so on…",
    "text": "Modify scale, add statistical summary, and so on…\n\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm,\n             y = bill_depth_mm)) + \n  # Adjust alpha of points\n  geom_point(alpha = 0.5) +\n  # Add smooth regression line\n  stat_smooth(method = \"lm\") + \n  # Flip the x-axis scale\n  scale_x_reverse() + \n  # Change title & axes labels \n  labs(x = \"Bill length (mm)\", \n       y = \"Bill depth (mm)\", \n       title = \"Clustering of penguins bills\") + \n  # Change the theme:\n  theme_bw() +\n  # Update font size of text:\n  theme(axis.title = element_text(size = 12),\n        plot.title = element_text(size = 16))"
  },
  {
    "objectID": "lectures/01-intro.html#modify-scale-add-statistical-summary-and-so-on-output",
    "href": "lectures/01-intro.html#modify-scale-add-statistical-summary-and-so-on-output",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Modify scale, add statistical summary, and so on…",
    "text": "Modify scale, add statistical summary, and so on…"
  },
  {
    "objectID": "lectures/01-intro.html#in-the-beginning",
    "href": "lectures/01-intro.html#in-the-beginning",
    "title": "Introduction and the Grammar of Graphics",
    "section": "In the beginning…",
    "text": "In the beginning…\n\nMichael Florent van Langren published the first (known) statistical graphic in 1644\n\n\n\n\n\n\nPlots different estimates of the longitudinal distance between Toledo, Spain and Rome, Italy\ni.e., visualization of collected data to aid in estimation of parameter"
  },
  {
    "objectID": "lectures/01-intro.html#john-snow-knows-something-about-cholera",
    "href": "lectures/01-intro.html#john-snow-knows-something-about-cholera",
    "title": "Introduction and the Grammar of Graphics",
    "section": "John Snow Knows Something About Cholera",
    "text": "John Snow Knows Something About Cholera"
  },
  {
    "objectID": "lectures/01-intro.html#charles-minards-map-of-napoleons-russian-disaster",
    "href": "lectures/01-intro.html#charles-minards-map-of-napoleons-russian-disaster",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Charles Minard’s Map of Napoleon’s Russian Disaster",
    "text": "Charles Minard’s Map of Napoleon’s Russian Disaster"
  },
  {
    "objectID": "lectures/01-intro.html#florence-nightingales-rose-diagram",
    "href": "lectures/01-intro.html#florence-nightingales-rose-diagram",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Florence Nightingale’s Rose Diagram",
    "text": "Florence Nightingale’s Rose Diagram"
  },
  {
    "objectID": "lectures/01-intro.html#milestones-in-data-visualization-history",
    "href": "lectures/01-intro.html#milestones-in-data-visualization-history",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Milestones in Data Visualization History",
    "text": "Milestones in Data Visualization History"
  },
  {
    "objectID": "lectures/01-intro.html#edward-tuftes-principles-of-data-visualization",
    "href": "lectures/01-intro.html#edward-tuftes-principles-of-data-visualization",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Edward Tufte’s Principles of Data Visualization",
    "text": "Edward Tufte’s Principles of Data Visualization\nGraphics: visually display measured quantities by combining points, lines, coordinate systems, numbers, symbols, words, shading, color\nOften our goal is to show data and/or communicate a story\n\n\nInduce viewer to think about substance, not graphical methodology\nMake large, complex datasets more coherent\nEncourage comparison of different pieces of data\nDescribe, explore, and identify relationships\nAvoid data distortion and data decoration\nUse consistent graph design\n\n\n\nAvoid graphs that lead to misleading conclusions!"
  },
  {
    "objectID": "lectures/01-intro.html#how-to-fail-this-class",
    "href": "lectures/01-intro.html#how-to-fail-this-class",
    "title": "Introduction and the Grammar of Graphics",
    "section": "How to Fail this Class:",
    "text": "How to Fail this Class:"
  },
  {
    "objectID": "lectures/01-intro.html#what-about-this-spiral",
    "href": "lectures/01-intro.html#what-about-this-spiral",
    "title": "Introduction and the Grammar of Graphics",
    "section": "What about this spiral?",
    "text": "What about this spiral?\n\n\nRequires distortion"
  },
  {
    "objectID": "lectures/01-intro.html#recap-and-next-steps",
    "href": "lectures/01-intro.html#recap-and-next-steps",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nDiscussed the importance of data visualization in your role as a statistician / data scientist\nWalked through course logistics (READ THE SYLLABUS)\nIntroduced the Grammar of Graphics and ggplot2 basics\nDiscussed data visualization principles and the role of infographics\n\n\n\nComplete HW0 by Wednesday night! Confirms you have everything installed and can render .qmd files to PDF via tinytex\n\n\n\n\nNext time: 1D Categorical Data\nRecommended reading:\n\nCW Chapter 2 Visualizing data: Mapping data onto aesthetics, CW Chapter 17 The principle of proportional ink\nKH Chapter 1 Look at data, KH Chapter 3 Make a plot"
  },
  {
    "objectID": "lectures/15-distance-mds.html#announcements-previously-and-today",
    "href": "lectures/15-distance-mds.html#announcements-previously-and-today",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW5 is due Wednesday March 19th by 11:59 PM ET\nAs part of Homework 5, you’ll form groups for final projects\n\nTeams should be 3-4 students, you can pick your teammates or be randomized to a team\nGoal of the project: create and interpet hiqh-quality visualizations for a dataset of your choice\nProject requirements and rubrics are available on Canvas\nEDA reports are graded as as group, while presentations are graded individually\nHW5 is short so you have time to form teams and explore datasets\n\nYou have Lab 7 this Friday\n\n\n\nLast time: Contour plots, heat maps, and diving into high-dimensional data\nTODAY: How do we visualize structure of high-dimensional data?\n\nExample: What if I give you a dataset with 50 variables, and ask you to make one visualization that best represents the data? What do you do?\nDo NOT panic and make \\(\\binom{50}{2} = 1225\\) pairs of plots!\nIntuition: Take high-dimensional data and represent it in 2-3 dimensions, then visualize those dimensions"
  },
  {
    "objectID": "lectures/15-distance-mds.html#what-about-high-dimensional-data",
    "href": "lectures/15-distance-mds.html#what-about-high-dimensional-data",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "What about high-dimensional data?",
    "text": "What about high-dimensional data?\nConsider this dataset about Starbucks drinks:\n\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))\nstarbucks |&gt; slice(1)\n\n# A tibble: 1 × 15\n  product_name              size   milk  whip serv_size_m_l calories total_fat_g\n  &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 brewed coffee - dark roa… short     0     0           236        3         0.1\n# ℹ 8 more variables: saturated_fat_g &lt;dbl&gt;, trans_fat_g &lt;dbl&gt;,\n#   cholesterol_mg &lt;dbl&gt;, sodium_mg &lt;dbl&gt;, total_carbs_g &lt;dbl&gt;, fiber_g &lt;dbl&gt;,\n#   sugar_g &lt;dbl&gt;, caffeine_mg &lt;dbl&gt;\n\n\nToday: Visualize structure among observations using distances matrices"
  },
  {
    "objectID": "lectures/15-distance-mds.html#thinking-about-distance",
    "href": "lectures/15-distance-mds.html#thinking-about-distance",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Thinking about distance…",
    "text": "Thinking about distance…\n\nWhen describing visuals, we’ve implicitly “clustered” observations together\n\ne.g., where are the mode(s) in the data?\n\nThese types of task require characterizing the distance between observations\n\nClusters: groups of observations that are “close” together\n\n\n\n\nThis is easy to do for 2 quantitative variables: just make a scatterplot (possibly with contours or heatmap)\nBut how do we define “distance” for high-dimensional data?\nLet \\(\\boldsymbol{x}_i = (x_{i1}, \\dots, x_{ip})\\) be a vector of \\(p\\) features for observation \\(i\\)\nQuestion of interest: How “far away” is \\(\\boldsymbol{x}_i\\) from \\(\\boldsymbol{x}_j\\)?\n\n\n\n\nWhen looking at a scatterplot, you’re using Euclidean distance (length of the line in \\(p\\)-dimensional space):\n\\[d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\sqrt{(x_{i1} - x_{j1})^2 + \\dots + (x_{ip} - x_{jp})^2}\\]"
  },
  {
    "objectID": "lectures/15-distance-mds.html#distances-in-general",
    "href": "lectures/15-distance-mds.html#distances-in-general",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Distances in general",
    "text": "Distances in general\nThere’s a variety of different types of distance metrics: Manhattan, Mahalanobis, Cosine, Kullback-Leiber Divergence, Wasserstein, but we’re just going to focus on Euclidean distance\n\\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\) measures pairwise distance between two observations \\(i,j\\) and has the following properties:\n\nIdentity: \\(\\boldsymbol{x}_i = \\boldsymbol{x}_j \\iff d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = 0\\)\nNon-Negativity: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\geq 0\\)\nSymmetry: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = d(\\boldsymbol{x}_j, \\boldsymbol{x}_i)\\)\nTriangle Inequality: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\leq d(\\boldsymbol{x}_i, \\boldsymbol{x}_k) + d(\\boldsymbol{x}_k, \\boldsymbol{x}_j)\\)\n\n\nDistance Matrix: matrix \\(D\\) of all pairwise distances\n\n\\(D_{ij} = d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\)\nwhere \\(D_{ii} = 0\\) and \\(D_{ij} = D_{ji}\\)"
  },
  {
    "objectID": "lectures/15-distance-mds.html#what-could-go-wrong-with-euclidean-distance",
    "href": "lectures/15-distance-mds.html#what-could-go-wrong-with-euclidean-distance",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "What could go wrong with Euclidean distance?",
    "text": "What could go wrong with Euclidean distance?"
  },
  {
    "objectID": "lectures/15-distance-mds.html#multi-dimensional-scaling-mds",
    "href": "lectures/15-distance-mds.html#multi-dimensional-scaling-mds",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Multi-dimensional scaling (MDS)",
    "text": "Multi-dimensional scaling (MDS)\n\nGeneral approach for visualizing distance matrices\nPuts \\(n\\) observations in a \\(k\\)-dimensional space such that the distances are preserved as much as possible\n\nwhere \\(k &lt;&lt; p\\) typically choose \\(k = 2\\)\n\n\n\n\nMDS attempts to create new point \\(\\boldsymbol{y}_i = (y_{i1}, y_{i2})\\) for each unit such that:\n\\[\\sqrt{(y_{i1} - y_{j1})^2 + (y_{i2} - y_{j2})^2} \\approx D_{ij}\\]\n\ni.e., distance in 2D MDS world is approximately equal to the actual distance\n\n\n\n\n\nThen plot the new \\(\\boldsymbol{y}\\)s on a scatterplot\n\nUse the scale() function to ensure variables are comparable\nMake a distance matrix for this dataset\nVisualize it with MDS"
  },
  {
    "objectID": "lectures/15-distance-mds.html#mds-workflow-example-with-starbucks-drinks",
    "href": "lectures/15-distance-mds.html#mds-workflow-example-with-starbucks-drinks",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "MDS workflow example with Starbucks drinks",
    "text": "MDS workflow example with Starbucks drinks\n\nstarbucks_quant_data &lt;- starbucks |&gt; \n  dplyr::select(serv_size_m_l:caffeine_mg)\n\nstarbucks_scaled_quant_data &lt;- \n  scale(starbucks_quant_data, center = FALSE, \n        scale = apply(starbucks_quant_data, 2, sd, na.rm = TRUE))\n\ndist_euc &lt;- dist(starbucks_scaled_quant_data)\nstarbucks_mds &lt;- cmdscale(d = dist_euc, k = 2)\n\nstarbucks &lt;- starbucks |&gt; \n  mutate(mds1 = starbucks_mds[,1], mds2 = starbucks_mds[,2])\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2)) + \n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/15-distance-mds.html#mds-workflow-example-with-starbucks-drinks-output",
    "href": "lectures/15-distance-mds.html#mds-workflow-example-with-starbucks-drinks-output",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "MDS workflow example with Starbucks drinks",
    "text": "MDS workflow example with Starbucks drinks"
  },
  {
    "objectID": "lectures/15-distance-mds.html#what-does-dist-return",
    "href": "lectures/15-distance-mds.html#what-does-dist-return",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "What does dist() return?",
    "text": "What does dist() return?\n\ndist(starbucks_scaled_quant_data[1:8,])\n\n          1         2         3         4         5         6         7\n2 1.0597905                                                            \n3 2.1605009 1.1015883                                                  \n4 3.3885615 2.3316426 1.2323454                                        \n5 1.4722992 2.3802999 3.4258192 4.6439965                              \n6 1.5671249 2.2148502 3.1494033 4.3218904 0.6904406                    \n7 1.9247890 2.2591636 3.0086115 4.0906365 1.3835219 0.6941308          \n8 2.4275777 2.4999068 3.0232984 3.9688066 2.0714599 1.3824217 0.6883099\n\n\nDefault distance calculation is Euclidean"
  },
  {
    "objectID": "lectures/15-distance-mds.html#what-does-cmdscale-do",
    "href": "lectures/15-distance-mds.html#what-does-cmdscale-do",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "What does cmdscale do?",
    "text": "What does cmdscale do?\n\nstarbucks_mds &lt;- cmdscale(d = dist_euc, k = 2)\n\ncmdscale() is the function we use to run MDS and it has two inputs:\n\nd: distance matrix, e.g., dist_euc\nk: number of dimensions we want, e.g., usually 2 for visualization purposes\n\nInput is \\(N \\times N\\) matrix, and the output is \\(N \\times 2\\)\nTo grab the output, we just grab the two columns of starbucks_mds and then can make a scatterplot of these two new dimensions\n\nstarbucks &lt;- starbucks |&gt; \n  mutate(mds1 = starbucks_mds[,1],  mds2 = starbucks_mds[,2])"
  },
  {
    "objectID": "lectures/15-distance-mds.html#interpreting-the-2d-projection",
    "href": "lectures/15-distance-mds.html#interpreting-the-2d-projection",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Interpreting the 2D projection",
    "text": "Interpreting the 2D projection\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/15-distance-mds.html#view-structure-with-additional-variables---size",
    "href": "lectures/15-distance-mds.html#view-structure-with-additional-variables---size",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "View structure with additional variables - size",
    "text": "View structure with additional variables - size\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2, color = size)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/15-distance-mds.html#view-structure-with-additional-variables---sugar_g",
    "href": "lectures/15-distance-mds.html#view-structure-with-additional-variables---sugar_g",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "View structure with additional variables - sugar_g",
    "text": "View structure with additional variables - sugar_g\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2, color = sugar_g)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\") +\n  scale_color_gradient(low = \"darkblue\", high = \"darkorange\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/15-distance-mds.html#recap-and-next-steps",
    "href": "lectures/15-distance-mds.html#recap-and-next-steps",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nWhen data is high dimensional, it’s impossible to visualize every dimension in the data, so instead:\nWe reduce the data to a small number of dimensions, and then plot those dimensions\n\nCompute a distance matrix: reduces the data to a single distance between points\nRun Multi-Dimensional Scaling (MDS): summarizes the distance matrix in 2 or 3 dimensions\nPlot the dimensions provided by MDS\n\nAdding other dimensions (e.g., via color) when plotting MDS can be a great way to see structure (such as clusters) in the data\n\n\n\n\nHW5 is due Wednesday March 19th and you have lab this Friday!\nNext time: Dendrograms to visualize distances and clusters\nReview more code in lecture demos!"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#announcements-previously-and-today",
    "href": "lectures/12-midsemester-review.html#announcements-previously-and-today",
    "title": "Midsemester Review",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nTake-home exam is Wednesday Feb 26th!\nHere’s how the exam will work:\n\nI’ll post the exam tonight, and it’s due Wednesday by 11:59 PM EDT (Feb 26th)\nExam will cover material from HW 1-4 and Labs 1-6\nQuestions will be similar to homeworks but more open-ended, e.g, instead of “make a side-by-side violin plot…” I’ll ask “Make a plot that compares the conditional distributions…”\nWe do NOT have class on Wednesday Feb 26th\n\n\n\n\nUse graphs to assess linear regression assumptions, i.e., residual diagnostic plots\nDiscussed interpretation of coefficients, the role of categorical variables, and interactions\n\n\n\n\nTODAY: Wrapping up regression and midsemester review!"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#understanding-interactions-categorical-example",
    "href": "lectures/12-midsemester-review.html#understanding-interactions-categorical-example",
    "title": "Midsemester Review",
    "section": "Understanding Interactions (Categorical Example)",
    "text": "Understanding Interactions (Categorical Example)\n\nSay we also have a quantitative variable \\(X\\) (bill length). Consider two statistical models:\n\n\\(Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_X X + \\beta_C I_C + \\beta_G I_G, \\sigma^2)\\)\n\\(Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_X X + \\beta_C I_C + \\beta_G I_G + \\beta_{CX} I_C X + \\beta_{GX} I_G X, \\sigma^2)\\)\n\n\n\n\nFor Model 1…\n\nThe intercept for Adelie is \\(\\beta_0\\); for Chinstrap it is \\(\\beta_0 + \\beta_C\\); for Gentoo it is \\(\\beta_0 + \\beta_G\\)\nThe slope for all species is \\(\\beta_X\\).\n\n\n\n\n\nFor Model 2…\n\nThe intercept for Adelie is \\(\\beta_0\\); for Chinstrap it is \\(\\beta_0 + \\beta_C\\); for Gentoo it is \\(\\beta_0 + \\beta_G\\).\nThe slope for Adelie is \\(\\beta_X\\); for Chinstrap it is \\(\\beta_X + \\beta_{CX}\\); for Gentoo it is \\(\\beta_X + \\beta_{GX}\\)\n\n\n\n\n\nSignificant coefficient for categorical variables by themselves? Significantly different intercepts\nSignificant coefficient for interactions with categorical variables? Significantly different slopes"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#model-2-bill_depth_mm-bill_length_mm-species",
    "href": "lectures/12-midsemester-review.html#model-2-bill_depth_mm-bill_length_mm-species",
    "title": "Midsemester Review",
    "section": "Model 2: bill_depth_mm ~ bill_length_mm + species",
    "text": "Model 2: bill_depth_mm ~ bill_length_mm + species"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#model-3-bill_depth_mm-bill_length_mm-species-bill_length_mm-times-species",
    "href": "lectures/12-midsemester-review.html#model-3-bill_depth_mm-bill_length_mm-species-bill_length_mm-times-species",
    "title": "Midsemester Review",
    "section": "Model 3: bill_depth_mm ~ bill_length_mm + species + bill_length_mm \\(\\times\\) species",
    "text": "Model 3: bill_depth_mm ~ bill_length_mm + species + bill_length_mm \\(\\times\\) species"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#a-few-linear-regression-warnings",
    "href": "lectures/12-midsemester-review.html#a-few-linear-regression-warnings",
    "title": "Midsemester Review",
    "section": "A Few Linear Regression Warnings",
    "text": "A Few Linear Regression Warnings\n\nSimpson’s Paradox\n\nThere is a negative linear relationship between two variables but a positive linear relationship within every subpopulation\nIn these cases, subgroup analysis is especially important\n\n\n\n\nIs the intercept meaningful?\n\nThink about whether \\(X = 0\\) makes scientific sense for a particular variable before you interpret the intercept\n\n\n\n\n\nInterpolation versus Extrapolation\n\nInterpolation is defined as prediction within the range of a variable\nExtrapolation is defined as prediction outside the range of a variable\nGenerally speaking, interpolation is more reliable than extrapolation (less sensitive to model misspecification)"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#extrapolation-example",
    "href": "lectures/12-midsemester-review.html#extrapolation-example",
    "title": "Midsemester Review",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#extrapolation-example-1",
    "href": "lectures/12-midsemester-review.html#extrapolation-example-1",
    "title": "Midsemester Review",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#extrapolation-example-2",
    "href": "lectures/12-midsemester-review.html#extrapolation-example-2",
    "title": "Midsemester Review",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#take-home-exam-logistics",
    "href": "lectures/12-midsemester-review.html#take-home-exam-logistics",
    "title": "Midsemester Review",
    "section": "Take-home exam logistics",
    "text": "Take-home exam logistics\nI will post it today, due Wednesday Feb 26th by 11:59 PM ET on Gradescope\nWhile the exam is in progress…\n\nYou can NOT talk to anyone else about 36-315\nYou can NOT post on Piazza\nYou can use any materials that are available to you from class (lectures, labs, homeworks, R demos)\n\nBest way to prepare:\n\nLook over lecture notes, R demos, homework/lab solutions"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#main-skill-ive-wanted-you-to-learn",
    "href": "lectures/12-midsemester-review.html#main-skill-ive-wanted-you-to-learn",
    "title": "Midsemester Review",
    "section": "Main skill I’ve wanted you to learn…",
    "text": "Main skill I’ve wanted you to learn…\n\nPick graph types that are most appropriate for a particular dataset\n\nRequires a working knowledge of different graph types and need to appropriately distinguish categorical vs quantitative variables\nFor any graph, need to know what information is visible vs hidden\n\n\n\n\nCharacterizing distributions (visually and quantitatively)\n\nNeed a “distributional vocabulary” (center/mode, spread, skewness) and need to choose graphs that showcase distributional quantities\nNeed to choose graph specifications that showcase distribution quantities (e.g., binwidth/bandwidth)\n\n\n\n\n\nConduct statistical inference to complement graphs\n\nFor most differences you spot in a graph, should be able to follow-up with an analysis to determine if that difference is significant\nRequires a working knowledge of different statistical tests\nNeed to know how to interpret the output from statistical tests (knowing the null/alternative hypotheses is key!)"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#variable-types",
    "href": "lectures/12-midsemester-review.html#variable-types",
    "title": "Midsemester Review",
    "section": "Variable Types",
    "text": "Variable Types\n\nFirst thing to do when looking at a dataset is determine what the variable types are.\nCategorical: May have order (ordinal) or no order (nominal).\n\nOften represented as a factor in R\nMay be coded with numbers!\nIf only 3-5 values, probably appropriate to treat as categorical.\n\nQuantitative: Represented numerically. Always has order.\n\nRepresented as numeric or integer in R.\n\nHow to determine if a variable is quantitative or categorical?\n\nOften obvious, but not always.\nSubtraction test: Does \\(X_1 - X_2\\) lead to a sensible value? If so, it’s quantitative.\nIf a variable is used in scatterplots/regression, it shouldn’t have a super strict range. 1-to-5 Likert scale variables fail this."
  },
  {
    "objectID": "lectures/12-midsemester-review.html#variable-type-situations",
    "href": "lectures/12-midsemester-review.html#variable-type-situations",
    "title": "Midsemester Review",
    "section": "Variable Type Situations",
    "text": "Variable Type Situations"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#variable-type-situations-1",
    "href": "lectures/12-midsemester-review.html#variable-type-situations-1",
    "title": "Midsemester Review",
    "section": "Variable Type Situations",
    "text": "Variable Type Situations"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#statistical-testsanalyses",
    "href": "lectures/12-midsemester-review.html#statistical-testsanalyses",
    "title": "Midsemester Review",
    "section": "Statistical Tests/Analyses",
    "text": "Statistical Tests/Analyses\n\nChi-square test for equal proportions: \\(H_0: p_1 = \\cdots = p_K\\).\nChi-square test for independence: \\(H_0:\\) Variables are independent.\n\nDependence: \\(P(A | B) \\neq P(A)\\)\n\n\n\n\nOne-sample KS test: \\(H_0\\): Variable follows a distribution.\nt-test/ANOVA: \\(H_0\\): Group means equal.\nBartlett’s test: \\(H_0\\): Group variances equal.\nTwo-Sample KS Test: \\(H_0\\): Variables follow the same distribution.\n\n\n\n\nLinear Regression: \\(H_0: \\beta = 0\\)\n\nNeed to distinguish between intercepts and slopes!\n\nRemember: Different tests have different power (chance of rejecting \\(H_0\\) when you should)"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#distribution-terminology",
    "href": "lectures/12-midsemester-review.html#distribution-terminology",
    "title": "Midsemester Review",
    "section": "Distribution Terminology",
    "text": "Distribution Terminology\n\nMarginal Distributions: \\(P(A)\\) - plot a graph of a single variable \\(A\\).\n\nPerhaps compare confidence intervals for different categories of \\(A\\).\n\n\n\n\nConditional Distributions: \\(P(A | B)\\) - in English: Distribution of \\(A\\) given a particular value of \\(B\\).\n\nGoal: Compare \\(P(A | B = b)\\) for different \\(b\\) when \\(A\\) is quantitative and \\(B\\) categorical\nA univariate graph (histograms, densities, violins) for each category.\nWhen \\(A\\) and \\(B\\) are categorical, can visualize with stacked bar plots or mosaic plots.\nNote: Linear regression estimates \\(\\mathbb{E}[Y | X]\\)\n\n\n\n\n\nJoint Distribution: \\(P(A, B)\\)\n\nUse mosaic plots when \\(A\\) and \\(B\\) are categorical.\n\\(P(A | B) P(B) = P(A, B)\\)\nScatterplots display joint distribution for continuous."
  },
  {
    "objectID": "lectures/12-midsemester-review.html#good-luck",
    "href": "lectures/12-midsemester-review.html#good-luck",
    "title": "Midsemester Review",
    "section": "Good luck!",
    "text": "Good luck!"
  }
]
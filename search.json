[
  {
    "objectID": "lectures/12-midsemester-review.html#announcements-previously-and-today",
    "href": "lectures/12-midsemester-review.html#announcements-previously-and-today",
    "title": "Midsemester Review",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nTake-home exam is Wednesday Feb 26th!\nHere’s how the exam will work:\n\nI’ll post the exam tonight, and it’s due Wednesday by 11:59 PM EDT (Feb 26th)\nExam will cover material from HW 1-4 and Labs 1-6\nQuestions will be similar to homeworks but more open-ended, e.g, instead of “make a side-by-side violin plot…” I’ll ask “Make a plot that compares the conditional distributions…”\nWe do NOT have class on Wednesday Feb 26th\n\n\n\n\nUse graphs to assess linear regression assumptions, i.e., residual diagnostic plots\nDiscussed interpretation of coefficients, the role of categorical variables, and interactions\n\n\n\n\nTODAY: Wrapping up regression and midsemester review!"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#understanding-interactions-categorical-example",
    "href": "lectures/12-midsemester-review.html#understanding-interactions-categorical-example",
    "title": "Midsemester Review",
    "section": "Understanding Interactions (Categorical Example)",
    "text": "Understanding Interactions (Categorical Example)\n\nSay we also have a quantitative variable \\(X\\) (bill length). Consider two statistical models:\n\n\\(Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_X X + \\beta_C I_C + \\beta_G I_G, \\sigma^2)\\)\n\\(Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_X X + \\beta_C I_C + \\beta_G I_G + \\beta_{CX} I_C X + \\beta_{GX} I_G X, \\sigma^2)\\)\n\n\n\n\nFor Model 1…\n\nThe intercept for Adelie is \\(\\beta_0\\); for Chinstrap it is \\(\\beta_0 + \\beta_C\\); for Gentoo it is \\(\\beta_0 + \\beta_G\\)\nThe slope for all species is \\(\\beta_X\\).\n\n\n\n\n\nFor Model 2…\n\nThe intercept for Adelie is \\(\\beta_0\\); for Chinstrap it is \\(\\beta_0 + \\beta_C\\); for Gentoo it is \\(\\beta_0 + \\beta_G\\).\nThe slope for Adelie is \\(\\beta_X\\); for Chinstrap it is \\(\\beta_X + \\beta_{CX}\\); for Gentoo it is \\(\\beta_X + \\beta_{GX}\\)\n\n\n\n\n\nSignificant coefficient for categorical variables by themselves? Significantly different intercepts\nSignificant coefficient for interactions with categorical variables? Significantly different slopes"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#model-2-bill_depth_mm-bill_length_mm-species",
    "href": "lectures/12-midsemester-review.html#model-2-bill_depth_mm-bill_length_mm-species",
    "title": "Midsemester Review",
    "section": "Model 2: bill_depth_mm ~ bill_length_mm + species",
    "text": "Model 2: bill_depth_mm ~ bill_length_mm + species"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#model-3-bill_depth_mm-bill_length_mm-species-bill_length_mm-times-species",
    "href": "lectures/12-midsemester-review.html#model-3-bill_depth_mm-bill_length_mm-species-bill_length_mm-times-species",
    "title": "Midsemester Review",
    "section": "Model 3: bill_depth_mm ~ bill_length_mm + species + bill_length_mm \\(\\times\\) species",
    "text": "Model 3: bill_depth_mm ~ bill_length_mm + species + bill_length_mm \\(\\times\\) species"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#a-few-linear-regression-warnings",
    "href": "lectures/12-midsemester-review.html#a-few-linear-regression-warnings",
    "title": "Midsemester Review",
    "section": "A Few Linear Regression Warnings",
    "text": "A Few Linear Regression Warnings\n\nSimpson’s Paradox\n\nThere is a negative linear relationship between two variables but a positive linear relationship within every subpopulation\nIn these cases, subgroup analysis is especially important\n\n\n\n\nIs the intercept meaningful?\n\nThink about whether \\(X = 0\\) makes scientific sense for a particular variable before you interpret the intercept\n\n\n\n\n\nInterpolation versus Extrapolation\n\nInterpolation is defined as prediction within the range of a variable\nExtrapolation is defined as prediction outside the range of a variable\nGenerally speaking, interpolation is more reliable than extrapolation (less sensitive to model misspecification)"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#extrapolation-example",
    "href": "lectures/12-midsemester-review.html#extrapolation-example",
    "title": "Midsemester Review",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#extrapolation-example-1",
    "href": "lectures/12-midsemester-review.html#extrapolation-example-1",
    "title": "Midsemester Review",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#extrapolation-example-2",
    "href": "lectures/12-midsemester-review.html#extrapolation-example-2",
    "title": "Midsemester Review",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#take-home-exam-logistics",
    "href": "lectures/12-midsemester-review.html#take-home-exam-logistics",
    "title": "Midsemester Review",
    "section": "Take-home exam logistics",
    "text": "Take-home exam logistics\nI will post it today, due Wednesday Feb 26th by 11:59 PM ET on Gradescope\nWhile the exam is in progress…\n\nYou can NOT talk to anyone else about 36-315\nYou can NOT post on Piazza\nYou can use any materials that are available to you from class (lectures, labs, homeworks, R demos)\n\nBest way to prepare:\n\nLook over lecture notes, R demos, homework/lab solutions"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#main-skill-ive-wanted-you-to-learn",
    "href": "lectures/12-midsemester-review.html#main-skill-ive-wanted-you-to-learn",
    "title": "Midsemester Review",
    "section": "Main skill I’ve wanted you to learn…",
    "text": "Main skill I’ve wanted you to learn…\n\nPick graph types that are most appropriate for a particular dataset\n\nRequires a working knowledge of different graph types and need to appropriately distinguish categorical vs quantitative variables\nFor any graph, need to know what information is visible vs hidden\n\n\n\n\nCharacterizing distributions (visually and quantitatively)\n\nNeed a “distributional vocabulary” (center/mode, spread, skewness) and need to choose graphs that showcase distributional quantities\nNeed to choose graph specifications that showcase distribution quantities (e.g., binwidth/bandwidth)\n\n\n\n\n\nConduct statistical inference to complement graphs\n\nFor most differences you spot in a graph, should be able to follow-up with an analysis to determine if that difference is significant\nRequires a working knowledge of different statistical tests\nNeed to know how to interpret the output from statistical tests (knowing the null/alternative hypotheses is key!)"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#variable-types",
    "href": "lectures/12-midsemester-review.html#variable-types",
    "title": "Midsemester Review",
    "section": "Variable Types",
    "text": "Variable Types\n\nFirst thing to do when looking at a dataset is determine what the variable types are.\nCategorical: May have order (ordinal) or no order (nominal).\n\nOften represented as a factor in R\nMay be coded with numbers!\nIf only 3-5 values, probably appropriate to treat as categorical.\n\nQuantitative: Represented numerically. Always has order.\n\nRepresented as numeric or integer in R.\n\nHow to determine if a variable is quantitative or categorical?\n\nOften obvious, but not always.\nSubtraction test: Does \\(X_1 - X_2\\) lead to a sensible value? If so, it’s quantitative.\nIf a variable is used in scatterplots/regression, it shouldn’t have a super strict range. 1-to-5 Likert scale variables fail this."
  },
  {
    "objectID": "lectures/12-midsemester-review.html#variable-type-situations",
    "href": "lectures/12-midsemester-review.html#variable-type-situations",
    "title": "Midsemester Review",
    "section": "Variable Type Situations",
    "text": "Variable Type Situations"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#variable-type-situations-1",
    "href": "lectures/12-midsemester-review.html#variable-type-situations-1",
    "title": "Midsemester Review",
    "section": "Variable Type Situations",
    "text": "Variable Type Situations"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#statistical-testsanalyses",
    "href": "lectures/12-midsemester-review.html#statistical-testsanalyses",
    "title": "Midsemester Review",
    "section": "Statistical Tests/Analyses",
    "text": "Statistical Tests/Analyses\n\nChi-square test for equal proportions: \\(H_0: p_1 = \\cdots = p_K\\).\nChi-square test for independence: \\(H_0:\\) Variables are independent.\n\nDependence: \\(P(A | B) \\neq P(A)\\)\n\n\n\n\nOne-sample KS test: \\(H_0\\): Variable follows a distribution.\nt-test/ANOVA: \\(H_0\\): Group means equal.\nBartlett’s test: \\(H_0\\): Group variances equal.\nTwo-Sample KS Test: \\(H_0\\): Variables follow the same distribution.\n\n\n\n\nLinear Regression: \\(H_0: \\beta = 0\\)\n\nNeed to distinguish between intercepts and slopes!\n\nRemember: Different tests have different power (chance of rejecting \\(H_0\\) when you should)"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#distribution-terminology",
    "href": "lectures/12-midsemester-review.html#distribution-terminology",
    "title": "Midsemester Review",
    "section": "Distribution Terminology",
    "text": "Distribution Terminology\n\nMarginal Distributions: \\(P(A)\\) - plot a graph of a single variable \\(A\\).\n\nPerhaps compare confidence intervals for different categories of \\(A\\).\n\n\n\n\nConditional Distributions: \\(P(A | B)\\) - in English: Distribution of \\(A\\) given a particular value of \\(B\\).\n\nGoal: Compare \\(P(A | B = b)\\) for different \\(b\\) when \\(A\\) is quantitative and \\(B\\) categorical\nA univariate graph (histograms, densities, violins) for each category.\nWhen \\(A\\) and \\(B\\) are categorical, can visualize with stacked bar plots or mosaic plots.\nNote: Linear regression estimates \\(\\mathbb{E}[Y | X]\\)\n\n\n\n\n\nJoint Distribution: \\(P(A, B)\\)\n\nUse mosaic plots when \\(A\\) and \\(B\\) are categorical.\n\\(P(A | B) P(B) = P(A, B)\\)\nScatterplots display joint distribution for continuous."
  },
  {
    "objectID": "lectures/12-midsemester-review.html#good-luck",
    "href": "lectures/12-midsemester-review.html#good-luck",
    "title": "Midsemester Review",
    "section": "Good luck!",
    "text": "Good luck!"
  },
  {
    "objectID": "lectures/01-intro.html#who-am-i",
    "href": "lectures/01-intro.html#who-am-i",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Who am I?",
    "text": "Who am I?\n\n\n\n\nAssistant Teaching Professor\nFinished Phd in Statistics @ CMU in May 2022\nPreviously BS in Statistics @ CMU in 2015\nResearch interests: sports analytics, natural language processing, clustering, selective inference\n\n\n\n\n\nIndustry experience: finance before returning to grad school and also as data scientist in professional sports"
  },
  {
    "objectID": "lectures/01-intro.html#why-do-we-visualize-data",
    "href": "lectures/01-intro.html#why-do-we-visualize-data",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Why do we visualize data?",
    "text": "Why do we visualize data?"
  },
  {
    "objectID": "lectures/01-intro.html#course-structure",
    "href": "lectures/01-intro.html#course-structure",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Course Structure",
    "text": "Course Structure\n\n\n\nLectures on Mondays/Wednesdays\n\nAll slides and demos posted on https://ryurko.github.io/statds-36315-spring25/\nParticipate and ask questions!\n\nWeekly homeworks due Wednesdays by 11:59 PM ET\nWeekly Friday labs due Saturdays by 11:59 AM ET\n\n\n\n\nTwo Graphics Critiques of Data Viz in the Wild (due Feb 28 and Mar 31)\nTake-home exam on Wednesday, Feb 26th\nFinal project with individual and group grade\n\nWork in teams on dataset you choose\nIn-class presentations during final week of class\nPublic facing HTML report due during finals week"
  },
  {
    "objectID": "lectures/01-intro.html#course-logistics",
    "href": "lectures/01-intro.html#course-logistics",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Course logistics",
    "text": "Course logistics\n\n\nAll homework/lab assignments will be in Quarto. You’ll generate a PDF, which you’ll submit on Gradescope\nMake sure R and RStudio are installed on your computer!\nHW0 due Wednesday Jan 15 at 11:59 PM ET: install R/RStudio, install/load tidyverse, render to PDF, and post to Gradescope\n\nHave any installation issues? Post to the course Piazza!\n\nPiazza: all questions about course material, HWs, exam, and projects\n\nDo NOT share code on Piazza\n\nOnly email for address administrative/logistic issues\nLab attendance on Friday is mandatory - submit lab assignment but don’t attend YOU LOSE 20PTS!\n\nQuestions about lab assignments will only be answered during lab\nIf you need to miss a lab due to illness, interviews, emergencies, etc., email me 48 hours in advance"
  },
  {
    "objectID": "lectures/01-intro.html#important-hw0-is-due-wednesday-night",
    "href": "lectures/01-intro.html#important-hw0-is-due-wednesday-night",
    "title": "Introduction and the Grammar of Graphics",
    "section": "IMPORTANT: HW0 is due Wednesday night",
    "text": "IMPORTANT: HW0 is due Wednesday night\nAs seen in today’s Canvas announcement - you must submit HW0 by Wednesday night!\n\nThis is just to make sure you have everything installed correctly and can render .qmd files to PDF\n\nRead through all of the directions in HW0 carefully!\nYou will stop saving your workspace upon exiting RStudio!\nYou will need to be set-up for the first lab on Friday"
  },
  {
    "objectID": "lectures/01-intro.html#course-objectives-read-the-syllabus",
    "href": "lectures/01-intro.html#course-objectives-read-the-syllabus",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Course Objectives (read the syllabus)",
    "text": "Course Objectives (read the syllabus)\nLearn useful principles for making appropriate statistical graphics.\nCritique existing graphs and remake better ones.\nVisualize statistical analyses to facilitate communication.\nPinpoint the statistical claims you can/cannot make from graphics.\nWrite and speak publicly about statistical graphics.\nPractice tidy data manipulation in R using the tidyverse\nPractice reproducible workflows with Quarto"
  },
  {
    "objectID": "lectures/01-intro.html#what-do-i-mean-by-tidy-data",
    "href": "lectures/01-intro.html#what-do-i-mean-by-tidy-data",
    "title": "Introduction and the Grammar of Graphics",
    "section": "What do I mean by tidy data?",
    "text": "What do I mean by tidy data?\nData are often stored in tabular (or matrix) form:\n\nlibrary(palmerpenguins)\npenguins |&gt; slice(1:5)\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "lectures/01-intro.html#the-grammar-of-graphics",
    "href": "lectures/01-intro.html#the-grammar-of-graphics",
    "title": "Introduction and the Grammar of Graphics",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\n\n\nOriginally defined by Leland Wilkinson\n\n\ndata\ngeometries: type of geometric objects to represent data, e.g., points, lines\naesthetics: visual characteristics of geometric objects to represent data, e.g., position, size\nscales: how each aesthetic is converted into values on the graph, e.g., color scales\nstats: statistical transformations to summarize data, e.g., counts, means, regression lines\nfacets: split data and view as multiple graphs\ncoordinate system: 2D space the data are projected onto, e.g., Cartesian coordinates\n\n\n\n\nHadley Wickham created ggplot2\n\n\ndata\ngeom\naes: mappings of columns to geometric objects\nscale: one scale for each aes variable\nstat\nfacet\ncoord\nlabs: labels/guides for each variable and other parts of the plot, e.g., title, subtitle, caption\ntheme: customization of plot layout"
  },
  {
    "objectID": "lectures/01-intro.html#start-with-the-data",
    "href": "lectures/01-intro.html#start-with-the-data",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Start with the data",
    "text": "Start with the data\n\n\nAccess ggplot2 from the tidyverse:\n\nlibrary(tidyverse)\nggplot(data = penguins)\n\n\nOr equivalently using |&gt;:\n\npenguins |&gt;\n  ggplot()"
  },
  {
    "objectID": "lectures/01-intro.html#need-to-add-geometric-objects",
    "href": "lectures/01-intro.html#need-to-add-geometric-objects",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Need to add geometric objects!",
    "text": "Need to add geometric objects!\n\n\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, \n             y = bill_depth_mm)) + \n  geom_point()\n\n\n\npenguins %&gt;%\n  ggplot(mapping = aes(x = bill_length_mm,\n                       y = bill_depth_mm)) + \n  geom_point()"
  },
  {
    "objectID": "lectures/01-intro.html#modify-scale-add-statistical-summary-and-so-on",
    "href": "lectures/01-intro.html#modify-scale-add-statistical-summary-and-so-on",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Modify scale, add statistical summary, and so on…",
    "text": "Modify scale, add statistical summary, and so on…\n\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm,\n             y = bill_depth_mm)) + \n  # Adjust alpha of points\n  geom_point(alpha = 0.5) +\n  # Add smooth regression line\n  stat_smooth(method = \"lm\") + \n  # Flip the x-axis scale\n  scale_x_reverse() + \n  # Change title & axes labels \n  labs(x = \"Bill length (mm)\", \n       y = \"Bill depth (mm)\", \n       title = \"Clustering of penguins bills\") + \n  # Change the theme:\n  theme_bw() +\n  # Update font size of text:\n  theme(axis.title = element_text(size = 12),\n        plot.title = element_text(size = 16))"
  },
  {
    "objectID": "lectures/01-intro.html#modify-scale-add-statistical-summary-and-so-on-output",
    "href": "lectures/01-intro.html#modify-scale-add-statistical-summary-and-so-on-output",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Modify scale, add statistical summary, and so on…",
    "text": "Modify scale, add statistical summary, and so on…"
  },
  {
    "objectID": "lectures/01-intro.html#in-the-beginning",
    "href": "lectures/01-intro.html#in-the-beginning",
    "title": "Introduction and the Grammar of Graphics",
    "section": "In the beginning…",
    "text": "In the beginning…\n\nMichael Florent van Langren published the first (known) statistical graphic in 1644\n\n\n\n\n\n\nPlots different estimates of the longitudinal distance between Toledo, Spain and Rome, Italy\ni.e., visualization of collected data to aid in estimation of parameter"
  },
  {
    "objectID": "lectures/01-intro.html#john-snow-knows-something-about-cholera",
    "href": "lectures/01-intro.html#john-snow-knows-something-about-cholera",
    "title": "Introduction and the Grammar of Graphics",
    "section": "John Snow Knows Something About Cholera",
    "text": "John Snow Knows Something About Cholera"
  },
  {
    "objectID": "lectures/01-intro.html#charles-minards-map-of-napoleons-russian-disaster",
    "href": "lectures/01-intro.html#charles-minards-map-of-napoleons-russian-disaster",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Charles Minard’s Map of Napoleon’s Russian Disaster",
    "text": "Charles Minard’s Map of Napoleon’s Russian Disaster"
  },
  {
    "objectID": "lectures/01-intro.html#florence-nightingales-rose-diagram",
    "href": "lectures/01-intro.html#florence-nightingales-rose-diagram",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Florence Nightingale’s Rose Diagram",
    "text": "Florence Nightingale’s Rose Diagram"
  },
  {
    "objectID": "lectures/01-intro.html#milestones-in-data-visualization-history",
    "href": "lectures/01-intro.html#milestones-in-data-visualization-history",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Milestones in Data Visualization History",
    "text": "Milestones in Data Visualization History"
  },
  {
    "objectID": "lectures/01-intro.html#edward-tuftes-principles-of-data-visualization",
    "href": "lectures/01-intro.html#edward-tuftes-principles-of-data-visualization",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Edward Tufte’s Principles of Data Visualization",
    "text": "Edward Tufte’s Principles of Data Visualization\nGraphics: visually display measured quantities by combining points, lines, coordinate systems, numbers, symbols, words, shading, color\nOften our goal is to show data and/or communicate a story\n\n\nInduce viewer to think about substance, not graphical methodology\nMake large, complex datasets more coherent\nEncourage comparison of different pieces of data\nDescribe, explore, and identify relationships\nAvoid data distortion and data decoration\nUse consistent graph design\n\n\n\nAvoid graphs that lead to misleading conclusions!"
  },
  {
    "objectID": "lectures/01-intro.html#how-to-fail-this-class",
    "href": "lectures/01-intro.html#how-to-fail-this-class",
    "title": "Introduction and the Grammar of Graphics",
    "section": "How to Fail this Class:",
    "text": "How to Fail this Class:"
  },
  {
    "objectID": "lectures/01-intro.html#what-about-this-spiral",
    "href": "lectures/01-intro.html#what-about-this-spiral",
    "title": "Introduction and the Grammar of Graphics",
    "section": "What about this spiral?",
    "text": "What about this spiral?\n\n\nRequires distortion"
  },
  {
    "objectID": "lectures/01-intro.html#recap-and-next-steps",
    "href": "lectures/01-intro.html#recap-and-next-steps",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nDiscussed the importance of data visualization in your role as a statistician / data scientist\nWalked through course logistics (READ THE SYLLABUS)\nIntroduced the Grammar of Graphics and ggplot2 basics\nDiscussed data visualization principles and the role of infographics\n\n\n\nComplete HW0 by Wednesday night! Confirms you have everything installed and can render .qmd files to PDF via tinytex\n\n\n\n\nNext time: 1D Categorical Data\nRecommended reading:\n\nCW Chapter 2 Visualizing data: Mapping data onto aesthetics, CW Chapter 17 The principle of proportional ink\nKH Chapter 1 Look at data, KH Chapter 3 Make a plot"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#announcements-previously-and-today",
    "href": "lectures/03-1dcat-infer.html#announcements-previously-and-today",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW1 is posted and due Wednesday Jan 29\nYou have Lab 2 again on Friday\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM\nPerry (zoom): Wednesdays @ 11 AM\n\n\n\n\n\nDiscussed 1D categorical data, basic summaries with counts and proportions\nIntroduced different area plots for visualizing 1D categorical data\nWe make bar charts for categorical data (I told you repeatedly that pie charts suck)\n\n\n\n\n\nTODAY: quantify and display uncertainty for 1D categorical data\n\nAdd confidence intervals to bar charts\nReview the Chi-Squared Test\nDiscuss connections between visualizations and statistical significance"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#crimes-against-bar-charts",
    "href": "lectures/03-1dcat-infer.html#crimes-against-bar-charts",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Crimes against bar charts",
    "text": "Crimes against bar charts"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#crimes-against-bar-charts-1",
    "href": "lectures/03-1dcat-infer.html#crimes-against-bar-charts-1",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Crimes against bar charts",
    "text": "Crimes against bar charts"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#what-does-a-bar-chart-show",
    "href": "lectures/03-1dcat-infer.html#what-does-a-bar-chart-show",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "What does a bar chart show?",
    "text": "What does a bar chart show?\nMarginal Distribution\n\nAssume categorical variable \\(X\\) has \\(K\\) categories: \\(C_1, \\dots, C_K\\)\nTrue marginal distribution of \\(X\\):\n\n\\[\nP(X = C_j) = p_j,\\ j \\in \\{ 1, \\dots, K \\}\n\\]\n\nWe have access to the Empirical Marginal Distribution\n\nObserved distribution of \\(X\\), our best estimate (MLE) of the marginal distribution of \\(X\\): \\(\\hat{p}_1\\), \\(\\hat{p}_2\\), \\(\\dots\\), \\(\\hat{p}_K\\)\n\n\ntable(penguins$species) / nrow(penguins)\n\n\n   Adelie Chinstrap    Gentoo \n0.4418605 0.1976744 0.3604651"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#bar-charts-with-proportions",
    "href": "lectures/03-1dcat-infer.html#bar-charts-with-proportions",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Bar charts with proportions",
    "text": "Bar charts with proportions\n\nafter_stat() indicates the aesthetic mapping is performed after statistical transformation\nUse after_stat(count) to access the stat_count() called by geom_bar()\n\n\npenguins |&gt;\n  ggplot(aes(x = species)) +\n  geom_bar(aes(y = after_stat(count) / sum(after_stat(count)))) + \n  labs(y = \"Proportion\")"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#compute-and-display-the-proportions-directly",
    "href": "lectures/03-1dcat-infer.html#compute-and-display-the-proportions-directly",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Compute and display the proportions directly",
    "text": "Compute and display the proportions directly\n\nUse group_by(), summarize(), and mutate() in a pipeline to compute then display the proportions directly\nNeed to indicate we are displaying the y axis as given, i.e., the identity function\n\n\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  mutate(total = sum(count), \n         prop = count / total) |&gt; \n  ggplot(aes(x = species)) +\n  geom_bar(aes(y = prop), stat = \"identity\")"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#compute-and-display-the-proportions-directly-output",
    "href": "lectures/03-1dcat-infer.html#compute-and-display-the-proportions-directly-output",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Compute and display the proportions directly",
    "text": "Compute and display the proportions directly"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#what-about-uncertainty",
    "href": "lectures/03-1dcat-infer.html#what-about-uncertainty",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "What about uncertainty?",
    "text": "What about uncertainty?\n\nQuantify uncertainty for our estimate \\(\\hat{p}_j = \\frac{n_j}{n}\\) with the standard error:\n\n\\[\nSE(\\hat{p}_j) = \\sqrt{\\frac{\\hat{p}_j(1 - \\hat{p}_j)}{n}}\n\\]\n\n\nCompute \\(\\alpha\\)-level confidence interval (CI) as \\(\\hat{p}_j \\pm z_{1 - \\alpha / 2} \\cdot SE(\\hat{p}_j)\\)\nGood rule-of-thumb: construct 95% CI using \\(\\hat{p}_j \\pm 2 \\cdot SE(\\hat{p}_j)\\)\nApproximation justified by CLT, so CI could include values outside of [0,1]"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#add-standard-errors-to-bars",
    "href": "lectures/03-1dcat-infer.html#add-standard-errors-to-bars",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Add standard errors to bars",
    "text": "Add standard errors to bars\n\nNeed to remember each CI is for each \\(\\hat{p}_j\\) marginally, not jointly\nHave to be careful with multiple testing\n\n\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  mutate(total = sum(count), \n         prop = count / total,\n         se = sqrt(prop * (1 - prop) / total), \n         lower = prop - 2 * se, \n         upper = prop + 2 * se) |&gt; \n  ggplot(aes(x = species)) +\n  geom_bar(aes(y = prop), stat = \"identity\") +\n  geom_errorbar(aes(ymin = lower, ymax = upper), \n                color = \"red\")"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#add-standard-errors-to-bars-output",
    "href": "lectures/03-1dcat-infer.html#add-standard-errors-to-bars-output",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Add standard errors to bars",
    "text": "Add standard errors to bars"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#useful-to-order-categories-by-frequency-with-forcats",
    "href": "lectures/03-1dcat-infer.html#useful-to-order-categories-by-frequency-with-forcats",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Useful to order categories by frequency with forcats",
    "text": "Useful to order categories by frequency with forcats\n\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  mutate(total = sum(count), \n         prop = count / total,\n         se = sqrt(prop * (1 - prop) / total), \n         lower = prop - 2 * se, \n         upper = prop + 2 * se,\n         species = fct_reorder(species, prop)) |&gt;\n  ggplot(aes(x = species)) +\n  geom_bar(aes(y = prop), stat = \"identity\") +\n  geom_errorbar(aes(ymin = lower, ymax = upper), \n                color = \"red\")"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#useful-to-order-categories-by-frequency-with-forcats-output",
    "href": "lectures/03-1dcat-infer.html#useful-to-order-categories-by-frequency-with-forcats-output",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Useful to order categories by frequency with forcats",
    "text": "Useful to order categories by frequency with forcats"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#dont-do-this",
    "href": "lectures/03-1dcat-infer.html#dont-do-this",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Don’t do this…",
    "text": "Don’t do this…"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#chi-squared-test-for-1d-categorical-data",
    "href": "lectures/03-1dcat-infer.html#chi-squared-test-for-1d-categorical-data",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Chi-squared test for 1D categorical data:",
    "text": "Chi-squared test for 1D categorical data:\n\n\nNull hypothesis \\(H_0\\): \\(p_1 = p_2 = \\dots = p_K\\), compute the test statistic:\n\n\\[\n\\chi^2 = \\sum_{j=1}^K \\frac{(O_j - E_j)^2}{E_j}\n\\]\n\n\\(O_j\\): observed counts in category \\(j\\)\n\\(E_j\\): expected counts under \\(H_0\\), i.e., each category is equally to occur \\(n / K = p_1 = p_2 = \\dots = p_K\\)\n\n\n\n\nchisq.test(table(penguins$species))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(penguins$species)\nX-squared = 31.907, df = 2, p-value = 1.179e-07"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#hypothesis-testing-review",
    "href": "lectures/03-1dcat-infer.html#hypothesis-testing-review",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Hypothesis testing review",
    "text": "Hypothesis testing review\n\n\n\nComputing \\(p\\)-values works like this:\n\nChoose a test statistic.\nCompute the test statistic in your dataset.\nIs test statistic “unusual” compared to what I would expect under \\(H_0\\)?\nCompare \\(p\\)-value to target error rate \\(\\alpha\\) (typically referred to as target level \\(\\alpha\\) )\nTypically choose \\(\\alpha = 0.05\\)\n\ni.e., if we reject null hypothesis at \\(\\alpha = 0.05\\) then, assuming \\(H_0\\) is true, there is a 5% chance it is a false positive (aka Type 1 error)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#chi-squared-test-for-1d-categorical-data-1",
    "href": "lectures/03-1dcat-infer.html#chi-squared-test-for-1d-categorical-data-1",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Chi-squared test for 1D categorical data:",
    "text": "Chi-squared test for 1D categorical data:\n\n\nNull hypothesis \\(H_0\\): \\(p_1 = p_2 = \\dots = p_K\\), compute the test statistic:\n\n\\[\n\\chi^2 = \\sum_{j=1}^K \\frac{(O_j - E_j)^2}{E_j}\n\\]\n\n\\(O_j\\): observed counts in category \\(j\\)\n\\(E_j\\): expected counts under \\(H_0\\), i.e., each category is equally to occur \\(n / K = p_1 = p_2 = \\dots = p_K\\)\n\n\n\nchisq.test(table(penguins$species))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(penguins$species)\nX-squared = 31.907, df = 2, p-value = 1.179e-07"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#graphics-versus-statistical-inference",
    "href": "lectures/03-1dcat-infer.html#graphics-versus-statistical-inference",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Graphics versus Statistical Inference",
    "text": "Graphics versus Statistical Inference\n\nReminder Anscombe’s Quartet: where statistical inference was the same but the graphics were very different\n\n\n\nThe opposite can be true! Graphics are the same, but statistical inference is very different…"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14",
    "href": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)",
    "text": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-1",
    "href": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-1",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)",
    "text": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-2",
    "href": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-2",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)",
    "text": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-3",
    "href": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-3",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)",
    "text": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#power-under-this-scenario-2n4-n4-n4",
    "href": "lectures/03-1dcat-infer.html#power-under-this-scenario-2n4-n4-n4",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Power under this scenario: (2n/4, n/4, n/4)",
    "text": "Power under this scenario: (2n/4, n/4, n/4)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#how-do-we-combine-graphs-with-inference",
    "href": "lectures/03-1dcat-infer.html#how-do-we-combine-graphs-with-inference",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "How do we combine graphs with inference?",
    "text": "How do we combine graphs with inference?\n\nSimply add \\(p\\)-values (or other info) to graph via text\nAdd confidence intervals to the graph\n\n\nNeed to remember what each CI is for!\nOur CIs on previous slides are for each \\(\\hat{p}_j\\) marginally, NOT jointly\nHave to be careful with multiple testing…"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#cis-will-visually-capture-uncertainty-in-estimates",
    "href": "lectures/03-1dcat-infer.html#cis-will-visually-capture-uncertainty-in-estimates",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "CIs will visually capture uncertainty in estimates",
    "text": "CIs will visually capture uncertainty in estimates"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#rough-rules-of-thumb-for-comparing-cis-on-bar-charts",
    "href": "lectures/03-1dcat-infer.html#rough-rules-of-thumb-for-comparing-cis-on-bar-charts",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "(Rough) Rules-of-thumb for comparing CIs on bar charts",
    "text": "(Rough) Rules-of-thumb for comparing CIs on bar charts\n\n\nComparing overlap of two CIs is NOT exactly the same as directly testing for a significant difference…\n\nReally you want CI( \\(\\hat{p}_1 - \\hat{p}_2\\) ), not CI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) )\nCI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) ) not overlapping implies \\(0 \\notin\\) CI( \\(\\hat{p}_1 - \\hat{p}_2\\) )\nHowever CI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) ) overlapping DOES NOT imply \\(0 \\in\\) CI( \\(\\hat{p}_1 - \\hat{p}_2\\) )\n\n\nRoughly speaking:\n\nIf CIs don’t overlap \\(\\rightarrow\\) significant difference\nIf CIs overlap a little \\(\\rightarrow\\) ambiguous\nIf CIs overlap a lot \\(\\rightarrow\\) no significant difference\n\n\n\n\nBut if we’re comparing more than two CIs simultaneously, we need to account for multiple testing!\n\nWhen you look for all non-overlapping CIs: implicitly making \\(\\binom{K}{2} = \\frac{K!}{2!(K-2)!}\\) pairwise tests in your head!"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#corrections-for-multiple-testing",
    "href": "lectures/03-1dcat-infer.html#corrections-for-multiple-testing",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Corrections for multiple testing",
    "text": "Corrections for multiple testing\n\n\nIn those bar plots, when we determine whether CIs overlap we make 3 comparisons:\n\nA vs B\nA vs C\nB vs C\n\n\nThis is a multiple testing issue\n\n\n\n\nIn short: we will make Type 1 errors (chance of false rejecting) more than 5% of the time!\nReminder: Type 1 error = Rejecting \\(H_0\\) when \\(H_0\\) is true\ne.g., CIs don’t overlap but actually \\(H_0: p_A = p_B\\) is true\nIf only interested in A vs B and nothing else, then just construct 95% CI for A vs B and control error rate at 5%\nHowever, if we construct several CIs, where A vs B is just one comparison we make, our Type 1 error rate &gt; 5%!"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#corrections-for-multiple-testing-1",
    "href": "lectures/03-1dcat-infer.html#corrections-for-multiple-testing-1",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Corrections for multiple testing",
    "text": "Corrections for multiple testing\n\nVast literature on corrections for multiple testing (beyond the scope of this class… but in my thesis!)\nBut you should understand the following:\n\nCorrections for multiple testing inflate \\(p\\)-values (i.e., make them bigger)\nEquivalently, they inflate CIs (i.e., make them wider)\nPurpose of these corrections is to control Type 1 error rate \\(\\leq 5\\%\\)\n\n\n\n\nWe’ll focus on the Bonferroni correction, which inflates \\(p\\)-values the most but is easy to implement and very popular:\n\nWe usually reject null hypothesis when \\(p\\)-value \\(\\leq .05\\)\nBonferroni: if making \\(K\\) comparisons, reject only if \\(p\\)-value \\(\\leq .05/K\\)\nFor CIs: instead of plotting 95% CIs, we plot (1 - \\(0.05/K\\))% CIs\n\ne.g., for \\(K = 3\\) then plot 98.3% CIs"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#impact-of-bonferroni-correction-on-cis",
    "href": "lectures/03-1dcat-infer.html#impact-of-bonferroni-correction-on-cis",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Impact of Bonferroni correction on CIs…",
    "text": "Impact of Bonferroni correction on CIs…"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#recap-and-next-steps",
    "href": "lectures/03-1dcat-infer.html#recap-and-next-steps",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nBar charts display the empirical distribution of the categorical variable ( \\(\\hat{p}_1, \\dots, \\hat{p}_K\\) )\nChi-squared test is a global test for 1D categorical data, testing \\(H_0 : p_1 = \\cdot \\cdot \\cdot = p_K\\)\n\nDoes not tell us which probabilities differ!\n\nCan visualize CIs for each \\(\\hat{p}_1\\), \\(\\dots\\), \\(\\hat{p}_K\\), but need to deal with multiple testing\nGraphs with the same trends can display very different statistical significance (largely due to sample size)\n\n\n\n\n\nHW1 is due next week and you have Lab 2 on Friday!\nNext time: 2D categorical data\nRecommended reading:\n\nCW Chapter 16.2 Visualizing the uncertainty of point estimates"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#announcements-previously-and-today",
    "href": "lectures/14-contours-heatmaps.html#announcements-previously-and-today",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW5 is due next Wednesday March 19th by 11:59 PM ET\nAs part of Homework 5, you’ll form groups for final projects\n\nTeams should be 3-4 students, you can pick your teammates or be randomized to a team\nGoal of the project: create and interpet hiqh-quality visualizations for a dataset of your choice\nProject requirements and rubrics are available on Canvas\nEDA reports are graded as as group, while presentations are graded individually\nHW5 is short so you have time to form teams and explore datasets\n\nYou do NOT have lab this week\n\n\n\nLast time:\n\nLOESS: bunch of little linear regressions glued together\nPairs plots: convenient wrapper to creating several visualizations at once\n\nTODAY: Contour Plots and Heat Maps"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#d-quantitative-data",
    "href": "lectures/14-contours-heatmaps.html#d-quantitative-data",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "2D quantitative data",
    "text": "2D quantitative data\n\nWe’re working with two variables: \\((X, Y) \\in \\mathbb{R}^2\\), i.e., dataset with \\(n\\) rows and 2 columns\nGoals:\n\ndescribing the relationships between two variables\ndescribing the conditional distribution \\(Y | X\\) via regression analysis\nTODAY: describing the joint distribution \\(X,Y\\) via contours, heatmaps, etc.\n\nFew big picture ideas to keep in mind:\n\nscatterplots are by far the most common visual\nregression analysis is by far the most popular analysis (you have a whole class on this…)\nrelationships may vary across other variables, e.g., categorical variables"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#what-about-focusing-on-the-joint-distribution",
    "href": "lectures/14-contours-heatmaps.html#what-about-focusing-on-the-joint-distribution",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "What about focusing on the joint distribution?",
    "text": "What about focusing on the joint distribution?\nExample dataset of pitches thrown by baseball superstar Shohei Ohtani\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  geom_point(alpha = 0.2) +\n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#going-from-1d-to-2d-density-estimation",
    "href": "lectures/14-contours-heatmaps.html#going-from-1d-to-2d-density-estimation",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Going from 1D to 2D density estimation",
    "text": "Going from 1D to 2D density estimation\nIn 1D: estimate density \\(f(x)\\), assuming that \\(f(x)\\) is smooth:\n\\[\n\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\n\\]\n\nIn 2D: estimate joint density \\(f(x_1, x_2)\\)\n\\[\\hat{f}(x_1, x_2) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h_1h_2} K(\\frac{x_1 - x_{i1}}{h_1}) K(\\frac{x_2 - x_{i2}}{h_2})\\]\n\n\nIn 1D there was one bandwidth, now we have two bandwidths\n\n\\(h_1\\): controls smoothness as \\(X_1\\) changes, holding \\(X_2\\) fixed\n\\(h_2\\): controls smoothness as \\(X_2\\) changes, holding \\(X_1\\) fixed\n\nAgain Gaussian kernels are the most popular…"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#so-how-do-we-display-densities-for-2d-data",
    "href": "lectures/14-contours-heatmaps.html#so-how-do-we-display-densities-for-2d-data",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "So how do we display densities for 2D data?",
    "text": "So how do we display densities for 2D data?"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#how-to-read-contour-plots",
    "href": "lectures/14-contours-heatmaps.html#how-to-read-contour-plots",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "How to read contour plots?",
    "text": "How to read contour plots?\nBest known in topology: outlines (contours) denote levels of elevation"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#display-2d-contour-plot",
    "href": "lectures/14-contours-heatmaps.html#display-2d-contour-plot",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Display 2D contour plot",
    "text": "Display 2D contour plot\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  geom_point(alpha = 0.2) +\n  geom_density2d() +\n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-1",
    "href": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-1",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Display 2D contour plot",
    "text": "Display 2D contour plot\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  geom_density2d() +\n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-2",
    "href": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-2",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Display 2D contour plot",
    "text": "Display 2D contour plot\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  stat_density2d(aes(fill = after_stat(level)), geom = \"polygon\") +\n  coord_fixed() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#visualizing-grid-heat-maps",
    "href": "lectures/14-contours-heatmaps.html#visualizing-grid-heat-maps",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Visualizing grid heat maps",
    "text": "Visualizing grid heat maps\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  stat_density2d(aes(fill = after_stat(density)), \n                 geom = \"tile\", contour = FALSE) + \n  coord_fixed() +\n  scale_fill_gradient(low = \"white\", high = \"red\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#alternative-idea-hexagonal-binning",
    "href": "lectures/14-contours-heatmaps.html#alternative-idea-hexagonal-binning",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Alternative idea: hexagonal binning",
    "text": "Alternative idea: hexagonal binning\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  geom_hex() +\n  coord_fixed() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#lebron-james-shots-from-hoopr",
    "href": "lectures/14-contours-heatmaps.html#lebron-james-shots-from-hoopr",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "LeBron James’ shots from hoopR",
    "text": "LeBron James’ shots from hoopR\n\nlebron_shots &lt;- read_csv(\"https://raw.githubusercontent.com/ryurko/DataViz-Class-Data/main/lebron_shots.csv\")\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = 0.4) +\n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-3",
    "href": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-3",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Display 2D contour plot",
    "text": "Display 2D contour plot\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = 0.4) +\n  geom_density2d(binwidth = 0.0001) + \n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#alternative-idea-hexagonal-binning-1",
    "href": "lectures/14-contours-heatmaps.html#alternative-idea-hexagonal-binning-1",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Alternative idea: hexagonal binning",
    "text": "Alternative idea: hexagonal binning\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_hex() +\n  coord_fixed() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#what-about-high-dimensional-data",
    "href": "lectures/14-contours-heatmaps.html#what-about-high-dimensional-data",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "What about high-dimensional data?",
    "text": "What about high-dimensional data?\nConsider this dataset containing nutritional information about Starbucks drinks:\n\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))\nstarbucks |&gt; slice(1)\n\n# A tibble: 1 × 15\n  product_name              size   milk  whip serv_size_m_l calories total_fat_g\n  &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 brewed coffee - dark roa… short     0     0           236        3         0.1\n# ℹ 8 more variables: saturated_fat_g &lt;dbl&gt;, trans_fat_g &lt;dbl&gt;,\n#   cholesterol_mg &lt;dbl&gt;, sodium_mg &lt;dbl&gt;, total_carbs_g &lt;dbl&gt;, fiber_g &lt;dbl&gt;,\n#   sugar_g &lt;dbl&gt;, caffeine_mg &lt;dbl&gt;\n\n\nHow do we visualize this dataset? \n\nTedious task: make a series of pairs plots (one giant pairs plot would overwhelming)"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#what-about-high-dimensional-data-1",
    "href": "lectures/14-contours-heatmaps.html#what-about-high-dimensional-data-1",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "What about high-dimensional data?",
    "text": "What about high-dimensional data?\n\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))\nstarbucks |&gt; slice(1)\n\n# A tibble: 1 × 15\n  product_name              size   milk  whip serv_size_m_l calories total_fat_g\n  &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 brewed coffee - dark roa… short     0     0           236        3         0.1\n# ℹ 8 more variables: saturated_fat_g &lt;dbl&gt;, trans_fat_g &lt;dbl&gt;,\n#   cholesterol_mg &lt;dbl&gt;, sodium_mg &lt;dbl&gt;, total_carbs_g &lt;dbl&gt;, fiber_g &lt;dbl&gt;,\n#   sugar_g &lt;dbl&gt;, caffeine_mg &lt;dbl&gt;\n\n\nGoals to keep in mind with visualizing high-dimensional data:\n\nVisualize structure among observations based on distances and projections (next lecture)\nVisualize structure among variables using correlation as “distance”"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#correlogram-to-visualize-correlation-matrix",
    "href": "lectures/14-contours-heatmaps.html#correlogram-to-visualize-correlation-matrix",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Correlogram to visualize correlation matrix",
    "text": "Correlogram to visualize correlation matrix\nUse the ggcorrplot package:\n\nstarbucks_quant_cor &lt;- cor(dplyr::select(starbucks, serv_size_m_l:caffeine_mg))\n\nlibrary(ggcorrplot)\nggcorrplot(starbucks_quant_cor)"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#options-to-customize-correlogram",
    "href": "lectures/14-contours-heatmaps.html#options-to-customize-correlogram",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Options to customize correlogram",
    "text": "Options to customize correlogram\n\nggcorrplot(starbucks_quant_cor,\n           type = \"lower\", method = \"circle\")"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#reorder-variables-based-on-correlation",
    "href": "lectures/14-contours-heatmaps.html#reorder-variables-based-on-correlation",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Reorder variables based on correlation",
    "text": "Reorder variables based on correlation\n\nggcorrplot(starbucks_quant_cor,\n           type = \"lower\", method = \"circle\",\n           hc.order = TRUE)"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#heatmap-displays-of-observations",
    "href": "lectures/14-contours-heatmaps.html#heatmap-displays-of-observations",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Heatmap displays of observations",
    "text": "Heatmap displays of observations\n\nheatmap(as.matrix(dplyr::select(starbucks, serv_size_m_l:caffeine_mg)),\n        scale = \"column\", \n        labRow = starbucks$product_name,\n        cexRow = .5, cexCol = .75,\n        Rowv = NA, Colv = NA)"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps",
    "href": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Manual version of heatmaps",
    "text": "Manual version of heatmaps\n\nstarbucks |&gt;\n  dplyr::select(product_name, serv_size_m_l:caffeine_mg) |&gt;\n  pivot_longer(serv_size_m_l:caffeine_mg,\n               names_to = \"variable\",\n               values_to = \"raw_value\") |&gt;\n  group_by(variable) |&gt;\n  mutate(std_value = (raw_value - mean(raw_value)) / sd(raw_value)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(y = variable, x = product_name, fill = std_value)) +\n  geom_tile() +\n  theme_light() +\n  theme(axis.text.x = element_text(size = 1, angle = 45),\n        legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-output",
    "href": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-output",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Manual version of heatmaps",
    "text": "Manual version of heatmaps"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-1",
    "href": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-1",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Manual version of heatmaps",
    "text": "Manual version of heatmaps\n\nstarbucks |&gt;\n  dplyr::select(product_name, serv_size_m_l:caffeine_mg) |&gt;\n  mutate(product_name = fct_reorder(product_name, calories)) |&gt;\n  pivot_longer(serv_size_m_l:caffeine_mg,\n               names_to = \"variable\",\n               values_to = \"raw_value\") |&gt;\n  group_by(variable) |&gt;\n  mutate(std_value = (raw_value - mean(raw_value)) / sd(raw_value)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(y = variable, x = product_name, fill = std_value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") +\n  theme_light() +\n  theme(axis.text.x = element_text(size = 1, angle = 45),\n        legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-1-output",
    "href": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-1-output",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Manual version of heatmaps",
    "text": "Manual version of heatmaps"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#parallel-coordinates-plot-with-ggparcoord",
    "href": "lectures/14-contours-heatmaps.html#parallel-coordinates-plot-with-ggparcoord",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Parallel coordinates plot with ggparcoord",
    "text": "Parallel coordinates plot with ggparcoord\n\nlibrary(GGally)\nstarbucks |&gt;\n  ggparcoord(columns = 5:15, alphaLines = .1) +\n  theme(axis.text.x = element_text(angle = 90))"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#easier-example-with-penguins",
    "href": "lectures/14-contours-heatmaps.html#easier-example-with-penguins",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Easier example with penguins…",
    "text": "Easier example with penguins…\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2, groupColumn = \"species\",\n             order = c(6, 5, 3, 4))"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#recap-and-next-steps",
    "href": "lectures/14-contours-heatmaps.html#recap-and-next-steps",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nWe can extend kernel density estimation from 1 to \\(p\\)-dimensions (don’t say easily though…)\nContour plots: Common way to visualize two-dimensional densities\nHeat maps: divide the space into a grid, and then color the grid according to high/low densities\nHexagonal bins: creating histograms in 2D\nCorrelograms and Parallel Coordinates Plots are helpful tools for visualizing high-dimensional data\n\n\n\n\nHW5 is due Wednesday March 19th and you do NOT have lab this Friday!\nNext time: Visualizing Distances and MDS"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#announcements-previously-and-today",
    "href": "lectures/13-nonlinear-pairs.html#announcements-previously-and-today",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW5 is due next Wednesday March 19th by 11:59 PM ET\nAs part of Homework 5, you’ll form groups for final projects\n\nTeams should be 3-4 students, you can pick your teammates or be randomized to a team\nGoal of the project: create and interpet hiqh-quality visualizations for a dataset of your choice\nProject requirements and rubrics are available on Canvas\nEDA reports are graded as as group, while presentations are graded individually\nHW5 is short so you have time to form teams and explore datasets\n\nYou do NOT have lab this week\nTODAY: How does LOESS (nonlinear regression) work? And maybe pairs plots"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#beyond-linear-regression",
    "href": "lectures/13-nonlinear-pairs.html#beyond-linear-regression",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Beyond Linear Regression",
    "text": "Beyond Linear Regression\nMany kinds of regression methods - we’ll focus on local linear regression for now.\nLet’s say: Still willing to assume Normality, but not linearity where \\(f(x)\\) is some unknown function\n\\[Y_i \\stackrel{iid}{\\sim} N(\\underbrace{f(X_i)}, \\sigma^2)\\]\nIntuition: Any nonlinear function is locally linear\nWe saw this in the extrapolation example\n\nLocal linear regressions fits a bunch of, well, local linear regressions, and then glues them together\nLocal linear regression is basically weighted linear regression, where only “local units” get weight"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#weighted-linear-regression",
    "href": "lectures/13-nonlinear-pairs.html#weighted-linear-regression",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Weighted Linear Regression",
    "text": "Weighted Linear Regression\nRemember that in typical linear regression, we solve the following:\n\\[\\arg \\min_{\\beta_0,\\beta_1} \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\\]\nIn weighted linear regression, we solve the following:\n\\[\\arg \\min_{\\beta_0,\\beta_1} \\sum_{i=1}^n w_i \\cdot (Y_i - \\beta_0 - \\beta_1 X_i)^2\\]\nLocal linear regression is exactly the same, except the weights depend on which \\(x\\) we want to estimate \\(f(x)\\)."
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#local-linear-regression-via-loess",
    "href": "lectures/13-nonlinear-pairs.html#local-linear-regression-via-loess",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Local linear regression via LOESS",
    "text": "Local linear regression via LOESS\n\\(Y_i \\overset{iid}{\\sim} N(f(x), \\sigma^2)\\), where \\(f(x)\\) is some unknown function\n\nIn local linear regression, we estimate \\(f(X_i)\\):\n\\[\\text{arg }\\underset{\\beta_0, \\beta_1}{\\text{min}} \\sum_i^n w_i(x) \\cdot \\big(Y_i - \\beta_0 - \\beta_1 X_i \\big)^2\\]\n\n\ngeom_smooth() uses tri-cubic weighting:\n\\[w_i(d_i) = \\begin{cases} (1 - |d_i|^3)^3, \\text{ if } i \\in \\text{neighborhood of  } x, \\\\\n0 \\text{ if } i \\notin \\text{neighborhood of  } x \\end{cases}\\]\n\n\\(d_i\\) is the distance between \\(x\\) and \\(X_i\\) scaled to be between 0 and 1\nspan: decides proportion of observations in neighborhood (default is 0.75)"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#animation-example",
    "href": "lectures/13-nonlinear-pairs.html#animation-example",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Animation example",
    "text": "Animation example"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#animation-example---changing-the-span",
    "href": "lectures/13-nonlinear-pairs.html#animation-example---changing-the-span",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Animation example - changing the span",
    "text": "Animation example - changing the span"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess",
    "href": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Displaying trend lines: LOESS",
    "text": "Displaying trend lines: LOESS\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth()\n\n\nFor \\(n &gt; 1000\\), mgcv::gam() is used with formula = y ~ s(x, bs = \"cs\") and method = \"REML\""
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess-1",
    "href": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess-1",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Displaying trend lines: LOESS",
    "text": "Displaying trend lines: LOESS\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(span = .1)"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess-2",
    "href": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess-2",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Displaying trend lines: LOESS",
    "text": "Displaying trend lines: LOESS\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(span = 1)"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#back-to-the-penguins",
    "href": "lectures/13-nonlinear-pairs.html#back-to-the-penguins",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Back to the penguins…",
    "text": "Back to the penguins…\nPretend I give you this penguins dataset and I ask you to make a plot for every pairwise comparison…\n\npenguins |&gt; slice(1:3)\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nWe can create a pairs plot to see all pairwise relationships in one plot\nPairs plot can include the various kinds of pairwise plots we’ve seen:\n\nTwo quantitative variables: scatterplot\nOne categorical, one quantitative: side-by-side violins, stacked histograms, overlaid densities\nTwo categorical: stacked bars, side-by-side bars, mosaic plots"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#create-pairs-plots-with-ggally",
    "href": "lectures/13-nonlinear-pairs.html#create-pairs-plots-with-ggally",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Create pairs plots with GGally",
    "text": "Create pairs plots with GGally\n\nlibrary(GGally)\npenguins |&gt; ggpairs(columns = 3:6)"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#create-pairs-plots-with-ggally-1",
    "href": "lectures/13-nonlinear-pairs.html#create-pairs-plots-with-ggally-1",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Create pairs plots with GGally",
    "text": "Create pairs plots with GGally\n\npenguins |&gt; ggpairs(columns = 3:6,\n                    mapping = aes(alpha = 0.5))"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#flexibility-in-customization",
    "href": "lectures/13-nonlinear-pairs.html#flexibility-in-customization",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Flexibility in customization",
    "text": "Flexibility in customization\n\npenguins |&gt; \n  ggpairs(columns = c(\"bill_length_mm\", \"body_mass_g\", \"island\"),\n          mapping = aes(alpha = 0.5, color = species), \n          lower = list(\n            continuous = \"smooth_lm\", \n            combo = \"facetdensitystrip\"\n          ),\n          upper = list(\n            continuous = \"cor\",\n            combo = \"facethist\"\n          )\n  )"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#flexibility-in-customization-output",
    "href": "lectures/13-nonlinear-pairs.html#flexibility-in-customization-output",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Flexibility in customization",
    "text": "Flexibility in customization"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#see-demo-for-more",
    "href": "lectures/13-nonlinear-pairs.html#see-demo-for-more",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "See demo for more!",
    "text": "See demo for more!"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#recap-and-next-steps",
    "href": "lectures/13-nonlinear-pairs.html#recap-and-next-steps",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nLOESS may seem like magic, but it’s just a bunch of little linear regressions glued together\nPairs plots: Nice way to see all pairwise relationships in a dataset\n\n\n\n\nHW5 is due Wednesday March 19th and you do NOT have lab this Friday!\nNext time: Contour Plots and Heat Maps"
  },
  {
    "objectID": "lectures/11-regression-inference.html#announcements-previously-and-today",
    "href": "lectures/11-regression-inference.html#announcements-previously-and-today",
    "title": "Inference with Linear Regression",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW4 is due TONIGHT by 11:59 PM and you have Lab 6 again on Friday!\nTake-home exam is next week Wednesday Feb 26th\nHere’s how the exam will work:\n\nI’ll post the exam Monday evening, and it’s due Wednesday by 11:59 PM EDT (Feb 26th)\nExam will cover material from HW 1-4 and Labs 1-6\nQuestions will be similar to homeworks but more open-ended, e.g, instead of “make a side-by-side violin plot…” I’ll ask “Make a plot that compares the conditional distributions…”\nThere will NOT be class on Wednesday Feb 26th\nConflict Feb 26th? Let me know ASAP! Day-of accommodations will NOT be made, late submissions will NOT be accepted\n\n\n\n\nScatterplots are the most common visual for 2D quantitative variables\n\nMany ways to incorporate additional dimensions in scatterplots, e.g., color and shape\n\nLinear regression is by far the most common model for describing the relationship between 2+ quantitative variables\n\nCan also: transform the outcome, transform the covariates, do nonparametric “smoothing”\n\nUse graphs to assess linear regression assumptions, i.e., residual diagnostic plots\n\n\n\n\nTODAY: More linear regression and inference with linear regression"
  },
  {
    "objectID": "lectures/11-regression-inference.html#displaying-trend-lines-linear-regression",
    "href": "lectures/11-regression-inference.html#displaying-trend-lines-linear-regression",
    "title": "Inference with Linear Regression",
    "section": "Displaying trend lines: linear regression",
    "text": "Displaying trend lines: linear regression\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "lectures/11-regression-inference.html#assessing-assumptions-of-linear-regression",
    "href": "lectures/11-regression-inference.html#assessing-assumptions-of-linear-regression",
    "title": "Inference with Linear Regression",
    "section": "Assessing assumptions of linear regression",
    "text": "Assessing assumptions of linear regression\nLinear regression assumes \\(Y_i \\overset{iid}{\\sim} N(\\beta_0 + \\beta_1 X_i, \\sigma^2)\\)\n\nIf this is true, then \\(Y_i - \\hat{Y}_i \\overset{iid}{\\sim} N(0, \\sigma^2)\\)\n\n\nPlot residuals against \\(\\hat{Y}_i\\), residuals vs fit plot\n\nUsed to assess linearity, any divergence from mean 0\nUsed to assess equal variance, i.e., if \\(\\sigma^2\\) is homogenous across predictions/fits \\(\\hat{Y}_i\\)\n\n\n\nMore difficult to assess the independence and fixed \\(X\\) assumptions\n\nMake these assumptions based on subject-matter knowledge"
  },
  {
    "objectID": "lectures/11-regression-inference.html#residual-vs-fit-plots",
    "href": "lectures/11-regression-inference.html#residual-vs-fit-plots",
    "title": "Inference with Linear Regression",
    "section": "Residual vs fit plots",
    "text": "Residual vs fit plots\n\nlin_reg &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\n\ntibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")"
  },
  {
    "objectID": "lectures/11-regression-inference.html#residual-vs-fit-plots-1",
    "href": "lectures/11-regression-inference.html#residual-vs-fit-plots-1",
    "title": "Inference with Linear Regression",
    "section": "Residual vs fit plots",
    "text": "Residual vs fit plots\n\ntibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth()"
  },
  {
    "objectID": "lectures/11-regression-inference.html#examples-of-residual-vs-fit-plots",
    "href": "lectures/11-regression-inference.html#examples-of-residual-vs-fit-plots",
    "title": "Inference with Linear Regression",
    "section": "Examples of Residual-vs-Fit Plots",
    "text": "Examples of Residual-vs-Fit Plots"
  },
  {
    "objectID": "lectures/11-regression-inference.html#more-fun-with-penguins",
    "href": "lectures/11-regression-inference.html#more-fun-with-penguins",
    "title": "Inference with Linear Regression",
    "section": "More fun with penguins…",
    "text": "More fun with penguins…\nDemo 03: Walk through an example of plotting/running different linear regression models\n\nOutcome: bill depth (in mm)\nCovariates: bill length (in mm) and species\n\n\nLinear regression models we will consider:\n\nbill_depth_mm ~ bill_length_mm\nbill_depth_mm ~ bill_length_mm + species\nbill_depth_mm ~ bill_length_mm + species + bill_length_mm \\(\\times\\) species"
  },
  {
    "objectID": "lectures/11-regression-inference.html#model-1-bill_depth_mm-bill_length_mm",
    "href": "lectures/11-regression-inference.html#model-1-bill_depth_mm-bill_length_mm",
    "title": "Inference with Linear Regression",
    "section": "Model 1: bill_depth_mm ~ bill_length_mm",
    "text": "Model 1: bill_depth_mm ~ bill_length_mm"
  },
  {
    "objectID": "lectures/11-regression-inference.html#model-1-bill_depth_mm-bill_length_mm-1",
    "href": "lectures/11-regression-inference.html#model-1-bill_depth_mm-bill_length_mm-1",
    "title": "Inference with Linear Regression",
    "section": "Model 1: bill_depth_mm ~ bill_length_mm",
    "text": "Model 1: bill_depth_mm ~ bill_length_mm\n\nsummary(lm(bill_depth_mm ~ bill_length_mm, data = penguins))\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1381 -1.4263  0.0164  1.3841  4.5255 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    20.88547    0.84388  24.749  &lt; 2e-16 ***\nbill_length_mm -0.08502    0.01907  -4.459 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.922 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.05525,   Adjusted R-squared:  0.05247 \nF-statistic: 19.88 on 1 and 340 DF,  p-value: 1.12e-05"
  },
  {
    "objectID": "lectures/11-regression-inference.html#how-are-the-intercept-and-slope-estimated",
    "href": "lectures/11-regression-inference.html#how-are-the-intercept-and-slope-estimated",
    "title": "Inference with Linear Regression",
    "section": "How are the intercept and slope estimated?",
    "text": "How are the intercept and slope estimated?\n\nWe have data \\((X_i, Y_i)\\). Want to estimate \\(\\beta_0\\) and \\(\\beta_1\\), where \\(\\mathbb{E}[Y | X] = \\beta_0 + \\beta_1 X\\)\nIf we had \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), then \\(\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\)\nThe estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are obtained by solving\n\\[\\arg \\min_{\\beta_0,\\beta_1} \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\\]\n\nRemember that \\(\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\), so the above is saying: “Give me the \\(\\hat{Y}_i\\) such that \\((Y_i - \\hat{Y}_i)^2\\) is minimized, on average”\n\nThe estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are:\n\\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}\\]\n\\[\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X}\\]"
  },
  {
    "objectID": "lectures/11-regression-inference.html#assessing-the-fit-of-linear-regression",
    "href": "lectures/11-regression-inference.html#assessing-the-fit-of-linear-regression",
    "title": "Inference with Linear Regression",
    "section": "Assessing the Fit of Linear Regression",
    "text": "Assessing the Fit of Linear Regression\n\nIntuitively, the more \\(X\\) and \\(Y\\) are correlated, the better the fit of the linear regression\nCorrelation is defined as\n\\[\\rho = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^n (X_i - \\bar{X})^2 \\cdot \\sum_{i=1}^n (Y_i - \\bar{Y})^2}} = \\frac{\\text{Cov}(X,Y)}{ \\sqrt{\\text{Var}(X) \\cdot \\text{Var}(Y)} }\\]\n\nCorrelation is just a standardized covariance, where \\(-1 \\leq \\rho \\leq 1\\).\nMore generally, \\(R^2\\) measures the fraction of variability in the outcome accounted by the covariates:\n\n\\[R^2 = 1 - \\frac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2} = 1 - \\frac{\\text{SS}_{\\text{residuals}}}{\\text{SS}_{\\text{total}}}\\]\nThe higher \\(R^2\\), the more the association. When linear regression has one covariate, \\(R = \\rho\\)"
  },
  {
    "objectID": "lectures/11-regression-inference.html#multiple-linear-regression",
    "href": "lectures/11-regression-inference.html#multiple-linear-regression",
    "title": "Inference with Linear Regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nLet’s say we have a bunch of covariates \\(X_1,X_2,\\dots,X_p\\)\nThe statistical model for multiple linear regression is\n\\[Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_1 X_{i1} + \\cdots + \\beta_k X_{ip}, \\sigma^2), \\hspace{0.1in} \\text{for all } i=1,\\dots,n\\]\n\nCovariates can be quadratic, cubic, etc. forms of other covariates, so this is quite flexible\nHow do we know when we’ve included the “right” covariates?\nThe higher \\(R^2\\), the more the association. So, maximize \\(R^2\\)?\n\n\n\nHowever, adding more covariates always increases \\(R^2\\). Better to look at “adjusted \\(R^2\\)”, which accounts for this\nAlso common: AIC and BIC (smaller is better)"
  },
  {
    "objectID": "lectures/11-regression-inference.html#special-case---categorical-variables",
    "href": "lectures/11-regression-inference.html#special-case---categorical-variables",
    "title": "Inference with Linear Regression",
    "section": "Special Case - Categorical Variables",
    "text": "Special Case - Categorical Variables\nCan include categorical variables in multiple linear regression, but need to code them as “dummy variables” (i.e., indicator variables)\nSay a categorical variable has \\(k \\geq 2\\) levels. Need to create \\((k-1)\\) indicator variables, equal to 1 for one category and 0 otherwise\nImportant: Categorical variable may be coded numerically (e.g., Agree = 1, Disagree = -1, Not Sure = 0)\n\nIf you put this variable straight into lm(), it will fit a very different model!"
  },
  {
    "objectID": "lectures/11-regression-inference.html#understanding-the-categorical-variables-example",
    "href": "lectures/11-regression-inference.html#understanding-the-categorical-variables-example",
    "title": "Inference with Linear Regression",
    "section": "Understanding the Categorical Variables Example",
    "text": "Understanding the Categorical Variables Example\nExample: Penguins species: Adelie, Chinstrap, Gentoo. There are \\(k = 3\\) levels.\nCreate an indicator for Chinstrap and Gentoo: \\(I_C\\) and \\(I_G\\).\n\nIf \\(I_C = I_G = 0\\), then the penguin must be Adelie\n\nThe statistical model would be \\(Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_C I_C + \\beta_G I_G, \\sigma^2)\\)\n\n\\(\\beta_0\\): \n\\(\\beta_0 + \\beta_C\\): \n\\(\\beta_0 + \\beta_G\\): \nSignificant \\(\\beta_C\\) \\(\\rightarrow\\) \nSignificant \\(\\beta_G\\) \\(\\rightarrow\\) \nHow to compare Chinstrap and Gentoo?"
  },
  {
    "objectID": "lectures/11-regression-inference.html#understanding-interactions-categorical-example",
    "href": "lectures/11-regression-inference.html#understanding-interactions-categorical-example",
    "title": "Inference with Linear Regression",
    "section": "Understanding Interactions (Categorical Example)",
    "text": "Understanding Interactions (Categorical Example)\n\nSay we also have a quantitative variable \\(X\\) (bill length). Consider two statistical models:\n\n\\(Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_X X + \\beta_C I_C + \\beta_G I_G, \\sigma^2)\\)\n\\(Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_X X + \\beta_C I_C + \\beta_G I_G + \\beta_{CX} I_C X + \\beta_{GX} I_G X, \\sigma^2)\\)\n\n\n\n\nFor Model 1…\n\nThe intercept for Adelie is \\(\\beta_0\\); for Chinstrap it is \\(\\beta_0 + \\beta_C\\); for Gentoo it is \\(\\beta_0 + \\beta_G\\)\nThe slope for all species is \\(\\beta_X\\).\n\n\n\n\n\nFor Model 2…\n\nThe intercept for Adelie is \\(\\beta_0\\); for Chinstrap it is \\(\\beta_0 + \\beta_C\\); for Gentoo it is \\(\\beta_0 + \\beta_G\\).\nThe slope for Adelie is \\(\\beta_X\\); for Chinstrap it is \\(\\beta_X + \\beta_{CX}\\); for Gentoo it is \\(\\beta_X + \\beta_{GX}\\)\n\n\n\n\n\nSignificant coefficient for categorical variables by themselves? Significantly different intercepts\nSignificant coefficient for interactions with categorical variables? Significantly different slopes"
  },
  {
    "objectID": "lectures/11-regression-inference.html#model-2-bill_depth_mm-bill_length_mm-species",
    "href": "lectures/11-regression-inference.html#model-2-bill_depth_mm-bill_length_mm-species",
    "title": "Inference with Linear Regression",
    "section": "Model 2: bill_depth_mm ~ bill_length_mm + species",
    "text": "Model 2: bill_depth_mm ~ bill_length_mm + species"
  },
  {
    "objectID": "lectures/11-regression-inference.html#model-3-bill_depth_mm-bill_length_mm-species-bill_length_mm-times-species",
    "href": "lectures/11-regression-inference.html#model-3-bill_depth_mm-bill_length_mm-species-bill_length_mm-times-species",
    "title": "Inference with Linear Regression",
    "section": "Model 3: bill_depth_mm ~ bill_length_mm + species + bill_length_mm \\(\\times\\) species",
    "text": "Model 3: bill_depth_mm ~ bill_length_mm + species + bill_length_mm \\(\\times\\) species"
  },
  {
    "objectID": "lectures/11-regression-inference.html#a-few-linear-regression-warnings",
    "href": "lectures/11-regression-inference.html#a-few-linear-regression-warnings",
    "title": "Inference with Linear Regression",
    "section": "A Few Linear Regression Warnings",
    "text": "A Few Linear Regression Warnings\n\nSimpson’s Paradox\n\nThere is a negative linear relationship between two variables but a positive linear relationship within every subpopulation\nIn these cases, subgroup analysis is especially important\n\n\n\n\nIs the intercept meaningful?\n\nThink about whether \\(X = 0\\) makes scientific sense for a particular variable before you interpret the intercept\n\n\n\n\n\nInterpolation versus Extrapolation\n\nInterpolation is defined as prediction within the range of a variable\nExtrapolation is defined as prediction outside the range of a variable\nGenerally speaking, interpolation is more reliable than extrapolation (less sensitive to model misspecification)"
  },
  {
    "objectID": "lectures/11-regression-inference.html#extrapolation-example",
    "href": "lectures/11-regression-inference.html#extrapolation-example",
    "title": "Inference with Linear Regression",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/11-regression-inference.html#extrapolation-example-1",
    "href": "lectures/11-regression-inference.html#extrapolation-example-1",
    "title": "Inference with Linear Regression",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/11-regression-inference.html#extrapolation-example-2",
    "href": "lectures/11-regression-inference.html#extrapolation-example-2",
    "title": "Inference with Linear Regression",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/11-regression-inference.html#recap-and-next-steps",
    "href": "lectures/11-regression-inference.html#recap-and-next-steps",
    "title": "Inference with Linear Regression",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nUse graphs to assess linear regression assumptions, i.e., residual diagnostic plots\nDiscussed interpretation of coefficients, the role of categorical variables, and interactions\nHighlighted common problems to consider: Simpson’s Paradox, intercept meaning, and extrapolation\n\n\n\n\nHW4 is due TONIGHT and you have Lab 6 on Friday\nGraphics critique due Feb 28th!\nNext time: Midsemester Review (take-home exam on Feb 26th)\nRecommended reading: CW Chapter 12 Visualizing associations among two or more quantitative variables"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#announcements-previously-and-today",
    "href": "lectures/04-power-multiple-testing.html#announcements-previously-and-today",
    "title": "Power and multiple testing",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW1 is due this Wednesday Jan 29 by 11:59 PM\nYou have Lab 3 again on Friday\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Wednesdays @ 11 AM\n\n\n\n\n\nMain estimators: \\(\\underbrace{\\hat{p}_1,\\dots,\\hat{p}_K}_{\\text{proportions}}\\) for \\(K\\)-many categories\nChi-square test is the main statistical test for 1D categorical data, tests \\(H_0: p_1 = \\cdots = p_K\\)\nCan also make confidence intervals for \\(\\hat{p}_1,\\dots,\\hat{p}_K\\) (just multiply CIs by \\(n\\))\n\n\n\n\n\nTODAY:\n\nInterpreting CIs on graphs is tricky and have to be careful with multiple testing"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#graphics-versus-statistical-inference",
    "href": "lectures/04-power-multiple-testing.html#graphics-versus-statistical-inference",
    "title": "Power and multiple testing",
    "section": "Graphics versus Statistical Inference",
    "text": "Graphics versus Statistical Inference\n\nReminder Anscombe’s Quartet: where statistical inference was the same but the graphics were very different\n\n\n\nThe opposite can be true! Graphics are the same, but statistical inference is very different…"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14",
    "href": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14",
    "title": "Power and multiple testing",
    "section": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)",
    "text": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-1",
    "href": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-1",
    "title": "Power and multiple testing",
    "section": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)",
    "text": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-2",
    "href": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-2",
    "title": "Power and multiple testing",
    "section": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)",
    "text": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-3",
    "href": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-3",
    "title": "Power and multiple testing",
    "section": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)",
    "text": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#power-under-this-scenario-2n4-n4-n4",
    "href": "lectures/04-power-multiple-testing.html#power-under-this-scenario-2n4-n4-n4",
    "title": "Power and multiple testing",
    "section": "Power under this scenario: (2n/4, n/4, n/4)",
    "text": "Power under this scenario: (2n/4, n/4, n/4)"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#how-do-we-combine-graphs-with-inference",
    "href": "lectures/04-power-multiple-testing.html#how-do-we-combine-graphs-with-inference",
    "title": "Power and multiple testing",
    "section": "How do we combine graphs with inference?",
    "text": "How do we combine graphs with inference?\n\nSimply add \\(p\\)-values (or other info) to graph via text\nAdd confidence intervals to the graph\n\n\nNeed to remember what each CI is for!\nOur CIs on previous slides are for each \\(\\hat{p}_j\\) marginally, NOT jointly\nHave to be careful with multiple testing…"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#cis-will-visually-capture-uncertainty-in-estimates",
    "href": "lectures/04-power-multiple-testing.html#cis-will-visually-capture-uncertainty-in-estimates",
    "title": "Power and multiple testing",
    "section": "CIs will visually capture uncertainty in estimates",
    "text": "CIs will visually capture uncertainty in estimates"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#rough-rules-for-comparing-cis-on-bar-charts",
    "href": "lectures/04-power-multiple-testing.html#rough-rules-for-comparing-cis-on-bar-charts",
    "title": "Power and multiple testing",
    "section": "Rough rules for comparing CIs on bar charts",
    "text": "Rough rules for comparing CIs on bar charts\n\n\nComparing overlap of two CIs is NOT exactly the same as directly testing for a significant difference…\n\nReally you want CI( \\(\\hat{p}_1 - \\hat{p}_2\\) ), not CI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) )\nCI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) ) not overlapping implies \\(0 \\notin\\) CI( \\(\\hat{p}_1 - \\hat{p}_2\\) )\nHowever CI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) ) overlapping DOES NOT imply \\(0 \\in\\) CI( \\(\\hat{p}_1 - \\hat{p}_2\\) )\n\n\nRoughly speaking:\n\nIf CIs don’t overlap \\(\\rightarrow\\) significant difference\nIf CIs overlap a little \\(\\rightarrow\\) ambiguous\nIf CIs overlap a lot \\(\\rightarrow\\) no significant difference\n\n\n\n\nBut if we’re comparing more than two CIs simultaneously, we need to account for multiple testing!\n\nWhen you look for all non-overlapping CIs: making \\(\\binom{K}{2} = \\frac{K!}{2!(K-2)!}\\) pairwise tests in your head!"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#corrections-for-multiple-testing",
    "href": "lectures/04-power-multiple-testing.html#corrections-for-multiple-testing",
    "title": "Power and multiple testing",
    "section": "Corrections for multiple testing",
    "text": "Corrections for multiple testing\n\n\nIn those bar plots, when we determine whether CIs overlap we make 3 comparisons:\n\nA vs B\nA vs C\nB vs C\n\n\nThis is a multiple testing issue\n\n\n\n\nIn short: we will make Type 1 errors (chance of false rejecting) more than 5% of the time!\nReminder: Type 1 error = Rejecting \\(H_0\\) when \\(H_0\\) is true\ne.g., CIs don’t overlap but actually \\(H_0: p_A = p_B\\) is true\nIf only interested in A vs B and nothing else, then just construct 95% CI for A vs B and control error rate at 5%\nHowever, if we construct several CIs, where A vs B is just one comparison we make, our Type 1 error rate &gt; 5%!"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#corrections-for-multiple-testing-1",
    "href": "lectures/04-power-multiple-testing.html#corrections-for-multiple-testing-1",
    "title": "Power and multiple testing",
    "section": "Corrections for multiple testing",
    "text": "Corrections for multiple testing\n\nVast literature on corrections for multiple testing (beyond the scope of this class… but in my thesis!)\nBut you should understand the following:\n\nCorrections for multiple testing inflate \\(p\\)-values (i.e., make them bigger)\nEquivalently, they inflate CIs (i.e., make them wider)\nPurpose of these corrections is to control Type 1 error rate \\(\\leq 5\\%\\)\n\n\n\n\nWe’ll focus on the Bonferroni correction, which inflates \\(p\\)-values the most but is easy to implement and very popular:\n\nWe usually reject null hypothesis when \\(p\\)-value \\(\\leq .05\\)\nBonferroni: if making \\(K\\) comparisons, reject only if \\(p\\)-value \\(\\leq .05/K\\)\nFor CIs: instead of plotting 95% CIs, we plot (1 - \\(0.05/K\\))% CIs\n\ne.g., for \\(K = 3\\) then plot 98.3% CIs"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#impact-of-bonferroni-correction-on-cis",
    "href": "lectures/04-power-multiple-testing.html#impact-of-bonferroni-correction-on-cis",
    "title": "Power and multiple testing",
    "section": "Impact of Bonferroni correction on CIs…",
    "text": "Impact of Bonferroni correction on CIs…"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#recap-and-next-steps",
    "href": "lectures/04-power-multiple-testing.html#recap-and-next-steps",
    "title": "Power and multiple testing",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nGraphs with the same trends can display very different statistical significance (largely due to sample size)\nCan visualize CIs for each \\(\\hat{p}_1\\), \\(\\dots\\), \\(\\hat{p}_K\\), but need to deal with multiple testing\n\n\n\n\n\nHW1 is due Wednesday and you have Lab 3 on Friday!\nNext time: Visualizations and inference for 2D categorical data\nRecommended reading: CW Chapter 11 Visualizing nested proportions"
  },
  {
    "objectID": "lectures/10-2dquant.html#announcements-previously-and-today",
    "href": "lectures/10-2dquant.html#announcements-previously-and-today",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW4 is due Wednesday by 11:59 PM and you have Lab 6 again on Friday!\nTake-home exam is next week Wednesday Feb 26th\nHere’s how the exam will work:\n\nI’ll post the exam Monday evening, and it’s due Wednesday by 11:59 PM EDT (Feb 26th)\nExam will cover material from HW 1-4 and Labs 1-6\nQuestions will be similar to homeworks but more open-ended, e.g, instead of “make a side-by-side violin plot…” I’ll ask “Make a plot that compares the conditional distributions…”\nThere will NOT be class on Wednesday Feb 26th\nConflict Feb 26th? Let me know ASAP! Day-of accommodations will NOT be made, late submissions will NOT be accepted\n\n\n\n\nDiscussed power in the context of visualizations and statistical tests\nEven if there is a true effect, you may have limited power to detect it\nSeveral ways to formally compare distributions:\n\n\\(t\\)-test: Compare means\nBartlett’s test: Compare variances\nKS test: Compare distributions\n\n\n\n\n\nTODAY: 2D quantitative data, scatterplots, and linear regression"
  },
  {
    "objectID": "lectures/10-2dquant.html#d-quantitative-data",
    "href": "lectures/10-2dquant.html#d-quantitative-data",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "2D quantitative data",
    "text": "2D quantitative data\n\nWe’re working with two variables: \\((X, Y) \\in \\mathbb{R}^2\\), i.e., dataset with \\(n\\) rows and 2 columns\n\n\n\nGoals:\n\ndescribing the relationships between two variables\ndescribing the conditional distribution \\(Y | X\\) via regression analysis\ndescribing the joint distribution \\(X,Y\\) via contours, heatmaps, etc.\n\n\n\n\n\nFew big picture ideas to keep in mind:\n\nscatterplots are by far the most common visual\nregression analysis is by far the most popular analysis (you have a whole class on this…)\nrelationships may vary across other variables, e.g., categorical variables"
  },
  {
    "objectID": "lectures/10-2dquant.html#making-scatterplots-with-geom_point",
    "href": "lectures/10-2dquant.html#making-scatterplots-with-geom_point",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Making scatterplots with geom_point()",
    "text": "Making scatterplots with geom_point()\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()"
  },
  {
    "objectID": "lectures/10-2dquant.html#always-adjust-the-alpha",
    "href": "lectures/10-2dquant.html#always-adjust-the-alpha",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "ALWAYS adjust the alpha",
    "text": "ALWAYS adjust the alpha\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#map-categorical-variable-to-color",
    "href": "lectures/10-2dquant.html#map-categorical-variable-to-color",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Map categorical variable to color",
    "text": "Map categorical variable to color\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g,\n             color = species)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#map-continuous-variable-to-color",
    "href": "lectures/10-2dquant.html#map-continuous-variable-to-color",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Map continuous variable to color",
    "text": "Map continuous variable to color\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g,\n             color = body_mass_g)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#map-continuous-variable-to-color-1",
    "href": "lectures/10-2dquant.html#map-continuous-variable-to-color-1",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Map continuous variable to color",
    "text": "Map continuous variable to color\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g,\n             color = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  scale_color_gradient(low = \"darkblue\", high = \"darkorange\")"
  },
  {
    "objectID": "lectures/10-2dquant.html#map-continuous-variable-to-size",
    "href": "lectures/10-2dquant.html#map-continuous-variable-to-size",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Map continuous variable to size",
    "text": "Map continuous variable to size\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g,\n             size = body_mass_g)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#map-categorical-variable-to-shape",
    "href": "lectures/10-2dquant.html#map-categorical-variable-to-shape",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Map categorical variable to shape",
    "text": "Map categorical variable to shape\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g,\n             shape = species)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#all-at-once",
    "href": "lectures/10-2dquant.html#all-at-once",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "ALL AT ONCE!",
    "text": "ALL AT ONCE!\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm,\n             color = species, shape = island, size = body_mass_g)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#displaying-trend-lines-linear-regression",
    "href": "lectures/10-2dquant.html#displaying-trend-lines-linear-regression",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Displaying trend lines: linear regression",
    "text": "Displaying trend lines: linear regression\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "lectures/10-2dquant.html#setup-and-motivation-for-linear-regression",
    "href": "lectures/10-2dquant.html#setup-and-motivation-for-linear-regression",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Setup and motivation for linear regression",
    "text": "Setup and motivation for linear regression\n\nConsider an outcome \\(Y \\in \\mathbb{R}\\) and covariate \\(X \\in \\mathbb{R}\\)\n\nWe have \\(n\\) observations: \\((X_1, Y_1), \\dots, (X_n, Y_n)\\)\n\nPurpose of regression is to model \\(\\mathbb{E}[Y | X]\\)\nConsider the case where \\(X\\) takes on discrete values \\(c_1, \\dots, c_k\\)\nThen most straightforward way to estimate \\(\\mathbb{E}[Y | X = c_j]\\) is to use the sample mean for subgroup \\(X_i = c_j\\):\n\\[\\hat{\\mathbb{E}}[Y|X = c_j] = \\frac{1}{N_j} \\sum_{i: X_i = c_j} Y_i\\]\n\nGraphs like side-by-side violin plots, facetted histograms, and overlaid density plots essentially compare \\(\\hat{\\mathbb{E}}[Y|X = c_j]\\) for different categories\n\nBut when \\(X\\) is quantitative, what do we do?\n\nUse statistical model to “guess” \\(\\mathbb{E}[Y|X = x]\\), even when we don’t observe \\(X = x\\)"
  },
  {
    "objectID": "lectures/10-2dquant.html#statistical-model-for-linear-regression",
    "href": "lectures/10-2dquant.html#statistical-model-for-linear-regression",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Statistical Model for Linear Regression",
    "text": "Statistical Model for Linear Regression\nLinear regression assumes \\(Y_i \\overset{iid}{\\sim} N(\\beta_0 + \\beta_1 X_i, \\sigma^2)\\)\n\n\\(\\beta_0\\): intercept - population mean outcome when \\(X = 0\\); i.e., \\(\\mathbb{E}[Y | X = 0]\\)\n\\(\\beta_1\\): slope - population mean change in \\(Y\\) when \\(X\\) increases by 1\n\\(\\beta_0\\) and \\(\\beta_1\\) are parameters that must be estimated"
  },
  {
    "objectID": "lectures/10-2dquant.html#assessing-assumptions-of-linear-regression",
    "href": "lectures/10-2dquant.html#assessing-assumptions-of-linear-regression",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Assessing assumptions of linear regression",
    "text": "Assessing assumptions of linear regression\nLinear regression assumes \\(Y_i \\overset{iid}{\\sim} N(\\beta_0 + \\beta_1 X_i, \\sigma^2)\\)\n\nIf this is true, then \\(Y_i - \\hat{Y}_i \\overset{iid}{\\sim} N(0, \\sigma^2)\\)\n\n\nPlot residuals against \\(\\hat{Y}_i\\), residuals vs fit plot\n\nUsed to assess linearity, any divergence from mean 0\nUsed to assess equal variance, i.e., if \\(\\sigma^2\\) is homogenous across predictions/fits \\(\\hat{Y}_i\\)\n\n\n\nMore difficult to assess the independence and fixed \\(X\\) assumptions\n\nMake these assumptions based on subject-matter knowledge"
  },
  {
    "objectID": "lectures/10-2dquant.html#residual-vs-fit-plots",
    "href": "lectures/10-2dquant.html#residual-vs-fit-plots",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Residual vs fit plots",
    "text": "Residual vs fit plots\n\nlin_reg &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\n\ntibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")"
  },
  {
    "objectID": "lectures/10-2dquant.html#residual-vs-fit-plots-1",
    "href": "lectures/10-2dquant.html#residual-vs-fit-plots-1",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Residual vs fit plots",
    "text": "Residual vs fit plots\n\ntibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth()"
  },
  {
    "objectID": "lectures/10-2dquant.html#examples-of-residual-vs-fit-plots",
    "href": "lectures/10-2dquant.html#examples-of-residual-vs-fit-plots",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Examples of Residual-vs-Fit Plots",
    "text": "Examples of Residual-vs-Fit Plots"
  },
  {
    "objectID": "lectures/10-2dquant.html#recap-and-next-steps",
    "href": "lectures/10-2dquant.html#recap-and-next-steps",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nScatterplots are the most common visual for 2D quantitative variables\n\nMany ways to incorporate additional dimensions in scatterplots, e.g., color and shape\n\nLinear regression is by far the most common model for describing the relationship between 2+ quantitative variables\n\nCan also: transform the outcome, transform the covariates, do nonparametric “smoothing”\n\nUse graphs to assess linear regression assumptions, i.e., residual diagnostic plots\n\n\n\n\nHW4 due Wednesday and you have Lab 6 on Friday\nGraphics critique due Feb 28th!\nNext time: Inference with Linear Regression\nRecommended reading: CW Chapter 12 Visualizing associations among two or more quantitative variables"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Lecture\nDate\nTitle\nMaterials\n\n\n\n\n1\nJan 13\nIntroduction and Grammar of Graphics\nslides\n\n\n2\nJan 15\n1D Categorical Data\nslides\n\n\n3\nJan 22\nStatistical Inference for 1D Categorical Data\nslides\n\n\n4\nJan 27\nPower and Multiple Testing\nslides\n\n\n5\nJan 29\nVisualizations and Inference for 2D Categorical Data\nslides\n\n\n6\nFeb 3\nVisualizing 1D Quantitative Data\nslides\n\n\n7\nFeb 5\nDensity Estimation\nslides\n\n\n8\nFeb 10\nGraphical Inference for 1D Quantitative Data\nslides\n\n\n9\nFeb 12\nComparing Distributions and Statistical Power\nslides\n\n\n10\nFeb 17\nScatterplots and Linear Regression\nslides\n\n\n11\nFeb 19\nInference with Linear Regression\nslides\n\n\n12\nFeb 24\nMidsemester Review\nslides\n\n\n13\nMar 10\nNonlinear Regression and Pairs Plots\nslides\n\n\n14\nMar 12\nContour Plots, Heat Maps, and Into High-Dimensional Data\nslides\n\n\n15\nMar 17\nVisualizing Distances for High-Dimensional Data\nslides",
    "crumbs": [
      "Lectures"
    ]
  },
  {
    "objectID": "demos/02-plot-2dquant.html",
    "href": "demos/02-plot-2dquant.html",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs. Throughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")"
  },
  {
    "objectID": "demos/02-plot-2dquant.html#colors",
    "href": "demos/02-plot-2dquant.html#colors",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Colors",
    "text": "Colors\nWe can color by a third variable (e.g., different color for each category).\nNote that, just like the x and y aesthetics, you can put color = inside ggplot or geom_point - both display the same visualization:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm, color = species)) +\n  geom_point(alpha = 0.5)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(color = species))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nYou can also color by a quantitative variable using a color scale/gradient:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(color = body_mass_g))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe default color gradient is not the most appealing, while there are a number of possibilities - blue to orange is a good choice since these colors are opposites on the color spectrum:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(color = body_mass_g)) +\n  scale_color_gradient(low = \"darkblue\", high = \"darkorange\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#point-size-size",
    "href": "demos/02-plot-2dquant.html#point-size-size",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Point size (size)",
    "text": "Point size (size)\nWe can also map variables to other aesthetics, e.g. size:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(size = body_mass_g))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#point-type-shape",
    "href": "demos/02-plot-2dquant.html#point-type-shape",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Point type (shape)",
    "text": "Point type (shape)\nOr the type (shape) of points:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(shape = species))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#combining-aesthetics",
    "href": "demos/02-plot-2dquant.html#combining-aesthetics",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Combining aesthetics",
    "text": "Combining aesthetics\nWe can even do several of these at once:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm,\n             color = species, shape = island,\n             size = body_mass_g)) +\n  geom_point(alpha = 0.5)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe above graph may be a bit difficult to read, but it contains a lot of information in the sense that it is a 5-dimensional graphic:\n\nx = bill depth (mm)\ny = bill length (mm)\ncolor = species\nsize = body mass (g)\nshape = island\n\n\nBut be careful!\nThe more complications you add, the more difficult your graph is to explain."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#linear-regression",
    "href": "demos/02-plot-2dquant.html#linear-regression",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Linear regression",
    "text": "Linear regression\nTo do this, we can use + geom_smooth(method = lm):\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n##Linear regression (with error bars)\nAbove, I added se = FALSE so that the standard error bars do not show up in the graph. Setting this parameter to TRUE produces (by default) 95% confidence intervals.\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = TRUE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nYou can change the level of the confidence intervals using the level argument:\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = TRUE, level = 0.99)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHowever, from these graphs, it’s not clear if the linear regression is a good fit. We can “eyeball” this by looking at the fitted versus residuals, or we can make a residual-versus-fit plot.\nWhat’s a residual-versus-fit plot? In short, the “fits” are the estimated y-values from the linear regression (i.e., the y-values along the linear regression line). Meanwhile, the “residuals” are the distance between the actual y-values and the fits. A residual-versus-fit plot is itself a scatterplot, with fits on the x-axis and residuals on the y-axis.\n\nlin_reg &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\nfitted_vals &lt;- fitted(lin_reg)\nres_vals &lt;- residuals(lin_reg)\ntibble(fits = fitted_vals, \n       residuals = res_vals) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\nWe look for two things when looking at residual-versus-fit plots:\n\nIs there any trend around the 0 horizontal line? If so, that might be a violation of the linearity assumption (more on this next class).\nDo the points have equal vertical spread from left to right? If not, that might be a violation of the equal variance assumption.\n\nWe’ll talk about these assumptions more next class."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#transformations-of-the-outcome",
    "href": "demos/02-plot-2dquant.html#transformations-of-the-outcome",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Transformations of the outcome",
    "text": "Transformations of the outcome\nWe can transform variables as well – again, within the plot. First, we will focus on the outcome y; in particular, we will focus on log transformations. This can be done through the y argument…\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = log(body_mass_g))) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = TRUE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#transformations-of-covariates",
    "href": "demos/02-plot-2dquant.html#transformations-of-covariates",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Transformations of Covariates",
    "text": "Transformations of Covariates\nIt’s also possible to include transformations of the covariates instead of (or in addition to) transformations of the outcome. For example, the following plots a quadratic regression model (i.e., plots y ~ x + x^2).\nNote that the formula argument in geom_smooth requires you to write in terms of y and x, NOT the variable names!\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", \n              formula = y ~ x + I(x^2))\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nTo assess if this is a better fit, we can again make a fitted-versus-residual plot (this looks better!):\n\nquad_lin_reg &lt;- lm(body_mass_g ~ flipper_length_mm + I(flipper_length_mm^2), \n                   data = penguins)\nquad_fitted_vals &lt;- fitted(quad_lin_reg)\nquad_res_vals &lt;- residuals(quad_lin_reg)\ntibble(fits = quad_fitted_vals, \n       residuals = quad_res_vals) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")"
  },
  {
    "objectID": "demos/02-plot-2dquant.html#non-linear-trends",
    "href": "demos/02-plot-2dquant.html#non-linear-trends",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Non-Linear Trends",
    "text": "Non-Linear Trends\nWe can also do other types of modeling, e.g. local regression / loess smoothing:\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"loess\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nCheck the help documentation for geom_smooth() and stat_smooth() to see what methods are available and how to use them. The most common choices are “lm”, “glm”, “gam”, and “loess”.\nNote that if you don’t put anything in geom_smooth, it will select “auto”, which typically uses loess for small datasets and gam for large datasets. However, it uses a particular form of smoothing splines, so in practice I recommend you specify a particular statistical method (e.g., “lm”, “loess”) so you actually know what you’re plotting."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#useful-for-residual-diagnostics",
    "href": "demos/02-plot-2dquant.html#useful-for-residual-diagnostics",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Useful for residual diagnostics",
    "text": "Useful for residual diagnostics\nConvenient to add + geom_smooth() to residual plots to help display any trends:\n\ntibble(fits = fitted_vals, \n       residuals = res_vals) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nwhich appears to be alleviated with the quadratic transformation:\n\ntibble(fits = quad_fitted_vals, \n       residuals = quad_res_vals) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "demos/02-plot-2dquant.html#leave-the-points-take-the-regression-model-this-is-a-bad-idea",
    "href": "demos/02-plot-2dquant.html#leave-the-points-take-the-regression-model-this-is-a-bad-idea",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Leave The Points, take The Regression Model? (This is a bad idea…)",
    "text": "Leave The Points, take The Regression Model? (This is a bad idea…)\nWe don’t even need to plot the points to do this – you can plot the regression model by itself by simply omitting geom_point():\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_smooth(method = \"loess\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\nAs always, you can adjust some parameters (like color, alpha, etc.):\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_smooth(method = \"loess\", se = TRUE, fill = \"darkorange\", \n              color = \"darkblue\", size = 2, alpha = 0.2) +\n  theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\nHowever, in general I don’t recommend doing this, because it hides the data entirely, making it unclear which data points are influencing the regression line."
  },
  {
    "objectID": "demos/06-mds.html",
    "href": "demos/06-mds.html",
    "title": "Demo 06: Visualizing Distances for High-Dimensional Data",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs.\nThroughout this demo we use a dataset about Starbucks drinks available in the #TidyTuesday project.\nYou can read in and manipulate various columns in the dataset with the following code:\nlibrary(tidyverse)\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g),\n         fiber_g = as.numeric(fiber_g))"
  },
  {
    "objectID": "demos/06-mds.html#so-far",
    "href": "demos/06-mds.html#so-far",
    "title": "Demo 06: Visualizing Distances for High-Dimensional Data",
    "section": "So far…",
    "text": "So far…\nWe’ve been working with “tidy data” – data that has \\(n\\) rows and \\(p\\) columns, where each row is an observation, and each column is a variable describing some feature of each observation.\nNow we’ll discuss more complicated data structures."
  },
  {
    "objectID": "demos/06-mds.html#distance-matrices",
    "href": "demos/06-mds.html#distance-matrices",
    "title": "Demo 06: Visualizing Distances for High-Dimensional Data",
    "section": "Distance matrices",
    "text": "Distance matrices\nA distance matrix is a data structure that specifies the “distance” between each pair of observations in the original \\(n\\)-row, \\(p\\)-column dataset. For each pair of observations (e.g. \\(x_i, x_j\\)) in the original dataset, we compute the distance between those observations, denoted as \\(d(x_i, x_j)\\) or \\(d_{ij}\\) for short.\nA variety of approaches for calculating the distance between a pair of observations can be used. The most commonly used approach (when we have quantitative variables) is called “Euclidean Distance”. The Euclidean distance between observations \\(x_i\\) and \\(x_j\\) is defined as follows: \\(d(x_i, x_j) = \\sqrt{\\sum_{l = 1}^p (x_{i,l} - x_{j,l}) ^ 2}\\). That is, it is the square root of the sum of squared differences between each column (\\(l \\in \\{1, ..., p\\}\\)) of \\(x_i\\) and \\(x_j\\) (remember, there are \\(p\\) original columns / variables).\nNote that if some variables in our dataset have substantially higher variance than others, the high-variance variables will dominate the calculation of distance, skewing our resulting distances towards the differences in these variables. As such, it’s common to scale the original dataset before calculating the distance, so that each variable is on the same scale."
  },
  {
    "objectID": "demos/06-mds.html#starbucks-drinks-dataset",
    "href": "demos/06-mds.html#starbucks-drinks-dataset",
    "title": "Demo 06: Visualizing Distances for High-Dimensional Data",
    "section": "Starbucks drinks dataset",
    "text": "Starbucks drinks dataset\nIn this R demo we will look at Starbucks drinks (courtesy of the #TidyTuesday project). In short, this is a dataset containing nutritional information about Starbucks drinks. We’re going to consider all of the quantitative variables in this dataset, starting with fifth column serv_size_m_l to the final column caffeine_mg. You can read about the columns in the dataset here. After selecting the desired columns, the first thing we’re going to do is use the scale() function to ensure each variable on the same scale, i.e., variances are equal to 1.\n\n# Select the variables of interest:\nstarbucks_quant_data &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg)\n# Now scale each column so that the variance is 1 using the scale function:\n# We specify here to not center the data and need to follow the directions in\n# the help page of scale to ensure we are properly standardizing the variance\nstarbucks_scaled_quant_data &lt;- \n  scale(starbucks_quant_data, center = FALSE, \n        scale = apply(starbucks_quant_data,\n                      2, sd, na.rm = TRUE)) \n\n# Just for reference - this is equivalent to the following commented out code:\n# starbucks_quant_data &lt;- starbucks |&gt;\n#   dplyr::select(serv_size_m_l:caffeine_mg)\n# starbucks_scaled_quant_data &lt;- apply(starbucks_quant_data, MARGIN = 2,\n#                                      FUN = function(x) x / sd(x))\n\nThe most common way to compute distances in R is to use the dist function. This takes in a dataset and returns the distance matrix for that dataset. By default this computes the euclidean distance (method = \"euclidean\"), but other distance metrics can be used.\n\n# Calculate distance matrix.\n# As an example, we'll just look at the first five rows:\ndist(starbucks_scaled_quant_data[1:5,])\n\n         1        2        3        4\n2 1.059790                           \n3 2.160501 1.101588                  \n4 3.388562 2.331643 1.232345         \n5 1.472299 2.380300 3.425819 4.643997\n\n# You can also include the diagonal if you want:\n# (the diagonal will always be 0s)\ndist(starbucks_scaled_quant_data[1:5,], diag = T)\n\n         1        2        3        4        5\n1 0.000000                                    \n2 1.059790 0.000000                           \n3 2.160501 1.101588 0.000000                  \n4 3.388562 2.331643 1.232345 0.000000         \n5 1.472299 2.380300 3.425819 4.643997 0.000000\n\n# You can also include the \"upper triangle\" if you want:\ndist(starbucks_scaled_quant_data[1:5,], upper = T)\n\n         1        2        3        4        5\n1          1.059790 2.160501 3.388562 1.472299\n2 1.059790          1.101588 2.331643 2.380300\n3 2.160501 1.101588          1.232345 3.425819\n4 3.388562 2.331643 1.232345          4.643997\n5 1.472299 2.380300 3.425819 4.643997         \n\n# Can also include both:\n# (this is the full distance matrix)\ndist(starbucks_scaled_quant_data[1:5,], diag = T, upper = T)\n\n         1        2        3        4        5\n1 0.000000 1.059790 2.160501 3.388562 1.472299\n2 1.059790 0.000000 1.101588 2.331643 2.380300\n3 2.160501 1.101588 0.000000 1.232345 3.425819\n4 3.388562 2.331643 1.232345 0.000000 4.643997\n5 1.472299 2.380300 3.425819 4.643997 0.000000\n\n# Can also consider other distance metrics\n# The default is euclidean, as you can see below:\n# (compare to what you see above)\ndist(starbucks_scaled_quant_data[1:5,], method = \"euclidean\", diag = T, upper = T)\n\n         1        2        3        4        5\n1 0.000000 1.059790 2.160501 3.388562 1.472299\n2 1.059790 0.000000 1.101588 2.331643 2.380300\n3 2.160501 1.101588 0.000000 1.232345 3.425819\n4 3.388562 2.331643 1.232345 0.000000 4.643997\n5 1.472299 2.380300 3.425819 4.643997 0.000000\n\n# For example, can consider the Manhattan distance:\ndist(starbucks_scaled_quant_data[1:5,], method = \"manhattan\", diag = T, upper = T)\n\n         1        2        3        4        5\n1 0.000000 1.552865 3.109038 4.818573 1.472299\n2 1.552865 0.000000 1.556173 3.265708 3.025165\n3 3.109038 1.556173 0.000000 1.709535 4.581337\n4 4.818573 3.265708 1.709535 0.000000 6.290872\n5 1.472299 3.025165 4.581337 6.290872 0.000000\n\n\nFor the purposes of this class, we’ll mostly focus on the Euclidean distance, so let’s define that here:\n\ndist_euc &lt;- dist(starbucks_scaled_quant_data)"
  },
  {
    "objectID": "demos/06-mds.html#implementing-multi-dimensional-scaling",
    "href": "demos/06-mds.html#implementing-multi-dimensional-scaling",
    "title": "Demo 06: Visualizing Distances for High-Dimensional Data",
    "section": "Implementing multi-dimensional scaling",
    "text": "Implementing multi-dimensional scaling\nNow we will implement multi-dimensional scaling (MDS) in R. As a reminder, MDS tries to find the “best” \\(k\\)-dimensional projection of the original \\(p\\)-dimensional dataset (\\(k &lt; p\\)).\nAs such, MDS tries to preserve the order of the pairwise distances. That is, pairs of observations with low distances in the original \\(p\\)-column dataset will still be have low distances in the smaller \\(k\\)-column dataset. Similarly, pairs of observations with high distances in the original \\(p\\)-column dataset will still be have high distances in the smaller \\(k\\)-column dataset.\nMDS can be implemented in R using the cmdscale function. This function takes a distance matrix (not a dataset!!):\n\nstarbucks_mds &lt;- cmdscale(d = dist_euc, k = 2)\n\nNote that you can change \\(k\\) to be greater than 2 if you want, but usually we want \\(k = 2\\) so that we can plot the (projected) distances in a scatterplot; see below.\nFor the purposes of plotting, let’s add the two coordinates of mds to our original dataset:\n\nstarbucks &lt;- starbucks |&gt;\n  mutate(mds1 = starbucks_mds[,1], mds2 = starbucks_mds[,2])\n\nThen, we can make a plot with ggplot:\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\")\n\n\n\n\n\n\n\n\nIt can be helpful to add colors and/or shapes of the plot according to categorical variables. For example, here’s the plot colored by size:\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2, color = size)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\")\n\n\n\n\n\n\n\n\nTo get some insight into the contributions by the different continous variables, we could also map them to various aesthetics. For example, the following plots displays points colored by sugar_g:\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2, color = sugar_g)) +\n  geom_point(alpha = 0.5) +\n  scale_color_gradient(low = \"darkblue\", high = \"darkorange\") +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\")\n\n\n\n\n\n\n\n\nWhat do these two colored plots tell us about the data?"
  },
  {
    "objectID": "demos/05-heatmap-highdim.html",
    "href": "demos/05-heatmap-highdim.html",
    "title": "Demo 05: Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs.\nFor the first part of this demo we’ll use a dataset of shots by LeBron James. For people that are interested, this was constructed using the hoopR package.\nYou can read in the dataset with the following code:\n\nlibrary(tidyverse)\nlebron_shots &lt;- read_csv(\"https://raw.githubusercontent.com/ryurko/DataViz-Class-Data/main/lebron_shots.csv\")\n\n\n\nWe’re all experts in 1D kernel density estimation by now. Let’s move on to 2D kernel density estimation.\n\n\nHere let’s focus on plotting the joint distribution of coordinate_x and coordinate_y, which are both quantitative (i.e., observing the joint distribution of shots). We’re already very familiar with how to make scatterplots:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = .5)\n\n\n\n\n\n\n\n\nIt’s really easy to add a two-dimensional density (via contour lines) to the plot: we just use geom_density2d():\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = .5) +\n  geom_density2d()\n\n\n\n\n\n\n\n\nSimilar to the contour lines on a topological map, the inner lines denote the “peaks” of the density. Note that the contour lines won’t necessarily encapsulate every data point.\nWe can also plot the contour lines without the points if you’d like (see below), but this is a bit misleading, because it automatically throws out areas of the plot where there were points but the density was low. To see this, compare the plot below to the plot above.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_density2d()\n\n\n\n\n\n\n\n\n\n\n\nFor example, we can change the fill type, which gives two benefits: (1) It looks cooler, and (2) Now we can see what the actual density values are.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(level)), geom = \"polygon\") + \n  geom_point(alpha = .5) #+\n\n\n\n\n\n\n\n  #scale_fill_gradient(low = \"darkblue\", high = \"darkorange\")\n\nNote: To change the color, you can uncomment the code above. This uses the scale_fill_gradient() function, which we’ve seen before in previous homeworks.\nWe might also want to change the bandwidth. In 2D kernel density estimation, we must specify two bandwidths – one for the x-direction, one for the y-direction. We’ll see how to do this in homework.\n\n\n\nHeat maps: Divide the space into a grid and color the grid according to high/low values.\nTo do this with densities, include fill = after_stat(density), geom = \"tile\", contour = FALSE in your call to stat_density2d, as below:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(density)), geom = \"tile\",\n                 contour = FALSE) + \n  geom_point(alpha = .5)\n\n\n\n\n\n\n\n\nAgain, I recommend changing the default color scheme (it’s pretty awful…), as below:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(density)), geom = \"tile\",\n                 contour = FALSE) + \n  geom_point(alpha = .5) + \n  scale_fill_gradient(low = \"white\", high = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nWe make hexagonal heatmap plots using geom_hex(), can specify binwidth in both directions. This avoids limitations and issues with smoothing and challenges with multivariate density estimation. Note: You need to have the hexbin package installed prior to creating these visuals.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_hex() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nUnrelated to 2D density estimation and viewing the joint frequency of points, we can alternatively view some statistical summary within various hexagonal bins displayed on two axes of interest. For example, the following graph displays the percentage of shots made within each hexagonal bin. We do this by mapping as.numeric(scoring_play) to the z aesthetic (since scoring_play is a boolean TRUE/FALSE and as.numeric() converts it to 1/0) and using the stat_summary_hex() layer with a specified function via fun = mean.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y, \n             z = as.numeric(scoring_play))) +\n  stat_summary_hex(fun = mean) +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "demos/05-heatmap-highdim.html#d-density-estimates",
    "href": "demos/05-heatmap-highdim.html#d-density-estimates",
    "title": "Demo 05: Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "",
    "text": "We’re all experts in 1D kernel density estimation by now. Let’s move on to 2D kernel density estimation.\n\n\nHere let’s focus on plotting the joint distribution of coordinate_x and coordinate_y, which are both quantitative (i.e., observing the joint distribution of shots). We’re already very familiar with how to make scatterplots:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = .5)\n\n\n\n\n\n\n\n\nIt’s really easy to add a two-dimensional density (via contour lines) to the plot: we just use geom_density2d():\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = .5) +\n  geom_density2d()\n\n\n\n\n\n\n\n\nSimilar to the contour lines on a topological map, the inner lines denote the “peaks” of the density. Note that the contour lines won’t necessarily encapsulate every data point.\nWe can also plot the contour lines without the points if you’d like (see below), but this is a bit misleading, because it automatically throws out areas of the plot where there were points but the density was low. To see this, compare the plot below to the plot above.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_density2d()\n\n\n\n\n\n\n\n\n\n\n\nFor example, we can change the fill type, which gives two benefits: (1) It looks cooler, and (2) Now we can see what the actual density values are.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(level)), geom = \"polygon\") + \n  geom_point(alpha = .5) #+\n\n\n\n\n\n\n\n  #scale_fill_gradient(low = \"darkblue\", high = \"darkorange\")\n\nNote: To change the color, you can uncomment the code above. This uses the scale_fill_gradient() function, which we’ve seen before in previous homeworks.\nWe might also want to change the bandwidth. In 2D kernel density estimation, we must specify two bandwidths – one for the x-direction, one for the y-direction. We’ll see how to do this in homework.\n\n\n\nHeat maps: Divide the space into a grid and color the grid according to high/low values.\nTo do this with densities, include fill = after_stat(density), geom = \"tile\", contour = FALSE in your call to stat_density2d, as below:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(density)), geom = \"tile\",\n                 contour = FALSE) + \n  geom_point(alpha = .5)\n\n\n\n\n\n\n\n\nAgain, I recommend changing the default color scheme (it’s pretty awful…), as below:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(density)), geom = \"tile\",\n                 contour = FALSE) + \n  geom_point(alpha = .5) + \n  scale_fill_gradient(low = \"white\", high = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nWe make hexagonal heatmap plots using geom_hex(), can specify binwidth in both directions. This avoids limitations and issues with smoothing and challenges with multivariate density estimation. Note: You need to have the hexbin package installed prior to creating these visuals.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_hex() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "demos/05-heatmap-highdim.html#bonus-statistical-summaries-within-hexagonal-bins",
    "href": "demos/05-heatmap-highdim.html#bonus-statistical-summaries-within-hexagonal-bins",
    "title": "Demo 05: Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "",
    "text": "Unrelated to 2D density estimation and viewing the joint frequency of points, we can alternatively view some statistical summary within various hexagonal bins displayed on two axes of interest. For example, the following graph displays the percentage of shots made within each hexagonal bin. We do this by mapping as.numeric(scoring_play) to the z aesthetic (since scoring_play is a boolean TRUE/FALSE and as.numeric() converts it to 1/0) and using the stat_summary_hex() layer with a specified function via fun = mean.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y, \n             z = as.numeric(scoring_play))) +\n  stat_summary_hex(fun = mean) +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "demos/05-heatmap-highdim.html#correlograms-with-ggcorrplot",
    "href": "demos/05-heatmap-highdim.html#correlograms-with-ggcorrplot",
    "title": "Demo 05: Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Correlograms with ggcorrplot",
    "text": "Correlograms with ggcorrplot\nWe can visualize the correlation matrix for the variables in a dataset using the ggcorrplot package. You need to install the package:\n\ninstall.packages(\"ggcorrplot\")\n\nNext, we’ll load the package and create a correlogram using only the continuous variables. To do this, we first need to compute the correlation matrix for these variables:\n\npenguins_cor_matrix &lt;- penguins |&gt;\n  dplyr::select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) |&gt;\n  cor(use = \"complete.obs\")\npenguins_cor_matrix\n\n                  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\nbill_length_mm         1.0000000    -0.2350529         0.6561813   0.5951098\nbill_depth_mm         -0.2350529     1.0000000        -0.5838512  -0.4719156\nflipper_length_mm      0.6561813    -0.5838512         1.0000000   0.8712018\nbody_mass_g            0.5951098    -0.4719156         0.8712018   1.0000000\n\n\nNOTE: Since there are missing values in the penguins data we need to indicate in the cor() function how to handle missing values using the use argument. By default, the correlations are returned as NA, which is not what we want. Instead, we can change this to only use observations without NA values for the considered columns (see help(cor) for more options).\nNow, we can create the correlogram using ggcorrplot() using this correlation matrix:\n\nlibrary(ggcorrplot)\nggcorrplot(penguins_cor_matrix)\n\n\n\n\n\n\n\n\nThere are several ways we can improve this correlogram:\n\nwe can avoid redundancy by only using one half of matrix by changing the type input: the default is full, we can make it lower or upper instead:\n\n\nggcorrplot(penguins_cor_matrix, type = \"lower\")\n\n\n\n\n\n\n\n\n\nwe can rearrange the variables using hierarchical clustering so that variables displaying stronger levels of correlation are closer together along the diagonal by setting hc.order = TRUE:\n\n\nggcorrplot(penguins_cor_matrix, type = \"lower\", hc.order = TRUE)\n\n\n\n\n\n\n\n\n\nif we want to add the correlation values directly to the plot, we can include those labels setting lab = TRUE - but we should round the correlation values first using the round() function:\n\n\nggcorrplot(round(penguins_cor_matrix, digits = 4), \n           type = \"lower\", hc.order = TRUE, lab = TRUE)\n\n\n\n\n\n\n\n\n\nif we want to place more stress on the correlation magnitude, we can change the method input to circle so that the size of the displayed circles is mapped to the absolute value of the correlation value:\n\n\nggcorrplot(penguins_cor_matrix, type = \"lower\", hc.order = TRUE,\n           method = \"circle\")\n\n\n\n\n\n\n\n\nYou can ignore the Warning message that is displayed - just from the differences in ggplot implementation."
  },
  {
    "objectID": "demos/05-heatmap-highdim.html#parallel-coordinates-plot-with-ggally",
    "href": "demos/05-heatmap-highdim.html#parallel-coordinates-plot-with-ggally",
    "title": "Demo 05: Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Parallel coordinates plot with GGally",
    "text": "Parallel coordinates plot with GGally\nIn a parallel coordinates plot, we create an axis for each varaible and align these axes side-by-side, drawing lines between observations from one axis to the next. This can be useful for visualizing structure among both the variables and observations in our dataset. These are useful when working with a moderate number of observations and variables - but can be overwhelming with too many.\nWe use the ggparcoord() function from the GGally package to make parallel coordinates plots:\n\nlibrary(GGally)\n\nWarning: package 'GGally' was built under R version 4.2.3\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\npenguins |&gt;\n  ggparcoord(columns = 3:6)\n\n\n\n\n\n\n\n\nThere are several ways we can modify this parallel coordinates plot:\n\nwe should always adjust the transparency of the lines using the alphaLines input to help handle overlap:\n\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2)\n\n\n\n\n\n\n\n\n\nwe can color each observation’s lines by a categorical variable, which can be useful for revealing group structure:\n\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2, groupColumn = \"species\")\n\n\n\n\n\n\n\n\n\nwe can change how the y-axis is constructed by modifying the scale input, which by default is std that is simply subtracting the mean and dividing by the standard deviation. We could instead use uniminmax so that minimum of the variable is zero and the maximum is one:\n\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2, groupColumn = \"species\",\n             scale = \"uniminmax\")\n\n\n\n\n\n\n\n\n\nwe can also reorder the variables a number of different ways with the order input (see help(ggparcoord) for details). There appears to be some weird errors however with the different options, but you can still manually provide the order of indices as follows:\n\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2, groupColumn = \"species\",\n             order = c(6, 5, 3, 4))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Graphics and Visualization 36-315",
    "section": "",
    "text": "This is the companion website for Statistical Graphics and Visualization 36-315. While all of the assignments will be posted on Canvas, this website provides an alternative way to access lecture materials and additional demo files (see the see side-bar).\nLectures are on Mondays and Wednesdays from 12 - 12:50 PM, located in DH A302.\nOffice hours schedule:\n\n\n\n\n\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\nProf Yurko\nWednesdays and Thursdays @ 2 PM\nBaker Hall 132D\n\n\nAnna Rosengart\nMondays @ 2 PM\nZoom (see Canvas)\n\n\nPerry Lin\nWednesdays @ 11 AM\nZoom (see Canvas)\n\n\n\nThere are no required textbooks for this course, but the following are useful free resources that I will sometimes refer to as recommended reading:\n\nR for Data Science\nData Visualization: A Practical Introduction\nFundamentals of Data Visualization\nggplot2: Elegant Graphics for Data Analysis\n\nAnd the following are interesting data visualization websites:\n\nFlowingData\nHistory of Data Visualization\nFriends Don’t Let Friends Make Bad Graphs",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "demos/04-nonlinear-pairs.html",
    "href": "demos/04-nonlinear-pairs.html",
    "title": "Demo 04: Nonlinear Regression and Pairs Plots",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs. Throughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")"
  },
  {
    "objectID": "demos/04-nonlinear-pairs.html#adjusting-the-span",
    "href": "demos/04-nonlinear-pairs.html#adjusting-the-span",
    "title": "Demo 04: Nonlinear Regression and Pairs Plots",
    "section": "Adjusting the span",
    "text": "Adjusting the span\nWhen using method = \"loess\", we can control the proportion of observations that are used when estimating the local regression (i.e., the size of the neighborhood around the observation of interest) with the span argument. For span &lt; 1, then the “neighborhood” includes proportion span of all possible points. By default, method = \"loess\" using the tri-cubic weighting, such that the weight is proportional to (1 - (dist / maxdist)^3)^3 (where maxdist refers to the maximum distance from the observations in the considered neighborhood). The default setting is span = 0.75, meaning that 75% of the dataset’s observations are used when fitting the local linear regression with weights. We can change span directly in geom_smooth():\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(span = .2)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nUpdate to use all observations instead:\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(span = 1)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/01-into-tidyverse.html",
    "href": "demos/01-into-tidyverse.html",
    "title": "Demo 01: Into the tidyverse",
    "section": "",
    "text": "(Broadly speaking) EDA = questions about data + wrangling + visualization\nR for Data Science: “EDA is a state of mind”, an iterative cycle:\n\ngenerate questions\nanswer via transformations and visualizations\n\nExample of questions?\n\nWhat type of variation do the variables display?\nWhat type of relationships exist between variables?\n\nGoal: develop understanding and become familiar with your data\n\nEDA is NOT a replacement for statistical inference and learning\nEDA is an important and necessary step to build intuition\n\nWe tackle the challenges of EDA with a data science workflow. An example of this according to Hadley Wickham in R for Data Science:\n\n\n\n\n\nAspects of data wrangling:\n\nimport: reading in data (e.g., read_csv())\ntidy: rows = observations, columns = variables (i.e. tabular data)\ntransform: filter observations, create new variables, summarize, etc."
  },
  {
    "objectID": "demos/01-into-tidyverse.html#what-is-exploratory-data-analysis-eda",
    "href": "demos/01-into-tidyverse.html#what-is-exploratory-data-analysis-eda",
    "title": "Demo 01: Into the tidyverse",
    "section": "",
    "text": "(Broadly speaking) EDA = questions about data + wrangling + visualization\nR for Data Science: “EDA is a state of mind”, an iterative cycle:\n\ngenerate questions\nanswer via transformations and visualizations\n\nExample of questions?\n\nWhat type of variation do the variables display?\nWhat type of relationships exist between variables?\n\nGoal: develop understanding and become familiar with your data\n\nEDA is NOT a replacement for statistical inference and learning\nEDA is an important and necessary step to build intuition\n\nWe tackle the challenges of EDA with a data science workflow. An example of this according to Hadley Wickham in R for Data Science:\n\n\n\n\n\nAspects of data wrangling:\n\nimport: reading in data (e.g., read_csv())\ntidy: rows = observations, columns = variables (i.e. tabular data)\ntransform: filter observations, create new variables, summarize, etc."
  },
  {
    "objectID": "demos/01-into-tidyverse.html#working-with-penguins",
    "href": "demos/01-into-tidyverse.html#working-with-penguins",
    "title": "Demo 01: Into the tidyverse",
    "section": "Working with penguins",
    "text": "Working with penguins\nIn R, there are many libraries or packages/groups of programs that are not permanently stored in R, so we have to load them when we want to use them. You can load an R package by typing library(package_name). (Sometimes we need to download/install the package first, as described in HW0.)\nThroughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")\n\nImport the penguins dataset by loading the palmerpenguins package using the library function and then access the data with the data() function:\n\nlibrary(palmerpenguins) \ndata(penguins)\n\nView some basic info about the penguins dataset:\n\n# displays same info as c(nrow(penguins), ncol(penguins))\ndim(penguins) \n\n[1] 344   8\n\nclass(penguins)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\ntbl (pronounced tibble) is the tidyverse way of storing tabular data, like a spreadsheet or data.frame\nI assure you that you’ll run into errors as you code in R; in fact, my attitude as a coder is that something is wrong if I never get any errors while working on a project. When you run into an error, your first reaction may be to panic and post a question to Piazza. However, checking help documentation in R can be a great way to figure out what’s going wrong. (For good or bad, I end up having to read help documentation almost every day of my life - because, well, I regularly make mistakes in R.)\nLook at the help documentation for penguins by typing help(penguins) in the Console. What are the names of the variables in this dataset? How many observations are in this dataset?\n\nhelp(penguins)\n\nYou should always look at your data before doing anything: view the first 6 (by default) rows with head()\n\nhead(penguins) # Try just typing penguins into your console, what happens?\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nIs our penguins dataset tidy?\n\nEach row = a single penguin\nEach column = different measurement about the penguins (can print out column names directly with colnames(penguins) or names(penguins))\n\nWe’ll now explore differences among the penguins using the tidyverse."
  },
  {
    "objectID": "demos/01-into-tidyverse.html#let-the-data-wrangling-begin",
    "href": "demos/01-into-tidyverse.html#let-the-data-wrangling-begin",
    "title": "Demo 01: Into the tidyverse",
    "section": "Let the data wrangling begin…",
    "text": "Let the data wrangling begin…\nFirst, load the tidyverse for exploring the data - and do NOT worry about the warning messages that will pop-up! Warning messages will tell you when other packages that are loaded may have functions replaced with the most recent package you’ve loaded. In general though, you should just be concerned when an error message pops up (errors are different than warnings!).\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWe’ll start by summarizing continuous (e.g., bill_length_mm, flipper_length_mm) and categorical (e.g., species, island) variables in different ways.\nWe can compute summary statistics for continuous variables with the summary() function:\n\nsummary(penguins$bill_length_mm)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  32.10   39.23   44.45   43.92   48.50   59.60       2 \n\n\nCompute counts of categorical variables with table() function:\n\ntable(\"island\" = penguins$island) # be careful it ignores NA values!\n\nisland\n   Biscoe     Dream Torgersen \n      168       124        52 \n\n\nHow do we remove the penguins with missing bill_length_mm values? Within the tidyverse, dplyr is a package with functions for data wrangling (because it’s within the tidyverse that means you do NOT have to load it separately with library(dplyr) after using library(tidyverse)!). It’s considered a “grammar of data manipulation”: dplyr functions are verbs, datasets are nouns.\nWe can filter() our dataset to choose observations meeting conditions:\n\nclean_penguins &lt;- filter(penguins, !is.na(bill_length_mm))\n# Use help(is.na) to see what it returns. And then observe \n# that the ! operator means to negate what comes after it.\n# This means !TRUE == FALSE (i.e., opposite of TRUE is equal to FALSE).\nnrow(penguins) - nrow(clean_penguins) # Difference in rows\n\n[1] 2\n\n\nIf we want to only consider a subset of columns in our data, we can select() variables of interest:\n\nsel_penguins &lt;- select(clean_penguins, species, island, bill_length_mm, flipper_length_mm)\nhead(sel_penguins, n = 3)\n\n# A tibble: 3 × 4\n  species island    bill_length_mm flipper_length_mm\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;             &lt;int&gt;\n1 Adelie  Torgersen           39.1               181\n2 Adelie  Torgersen           39.5               186\n3 Adelie  Torgersen           40.3               195\n\n\nWe can arrange() our dataset to sort observations by variables:\n\nbill_penguins &lt;- arrange(sel_penguins, desc(bill_length_mm)) # use desc() for descending order\nhead(bill_penguins, n = 3)\n\n# A tibble: 3 × 4\n  species   island bill_length_mm flipper_length_mm\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;             &lt;int&gt;\n1 Gentoo    Biscoe           59.6               230\n2 Chinstrap Dream            58                 181\n3 Gentoo    Biscoe           55.9               228\n\n\nWe can summarize() our dataset to one row based on functions of variables:\n\nsummarize(bill_penguins, max(bill_length_mm), median(flipper_length_mm))\n\n# A tibble: 1 × 2\n  `max(bill_length_mm)` `median(flipper_length_mm)`\n                  &lt;dbl&gt;                       &lt;dbl&gt;\n1                  59.6                         197\n\n\nWe can mutate() our dataset to create new variables:\n\nnew_penguins &lt;- mutate(bill_penguins, \n                       bill_flipper_ratio = bill_length_mm / flipper_length_mm,\n                       flipper_bill_ratio = flipper_length_mm / bill_length_mm)\nhead(new_penguins, n = 1)\n\n# A tibble: 1 × 6\n  species island bill_length_mm flipper_length_mm bill_flipper_ratio\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;             &lt;int&gt;              &lt;dbl&gt;\n1 Gentoo  Biscoe           59.6               230              0.259\n# ℹ 1 more variable: flipper_bill_ratio &lt;dbl&gt;\n\n\nHow do we perform several of these actions?\n\nhead(arrange(select(mutate(filter(penguins, !is.na(flipper_length_mm)), bill_flipper_ratio = bill_length_mm / flipper_length_mm), species, island, bill_flipper_ratio), desc(bill_flipper_ratio)), n = 1)\n\n# A tibble: 1 × 3\n  species   island bill_flipper_ratio\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Chinstrap Dream               0.320\n\n\nThat’s awfully annoying to do, and also difficult to read…"
  },
  {
    "objectID": "demos/01-into-tidyverse.html#enter-the-pipeline",
    "href": "demos/01-into-tidyverse.html#enter-the-pipeline",
    "title": "Demo 01: Into the tidyverse",
    "section": "Enter the pipeline",
    "text": "Enter the pipeline\nThe |&gt; (pipe) operator is used in the to chain commands together. Note: you can also use the tidyverse pipe %&gt;% (from magrittr), but |&gt; is the built-in pipe that is native to new versions of R without loading the tidyverse.\n|&gt; directs the data analyis pipeline: output of one function pipes into input of the next function\n\npenguins |&gt;\n  filter(!is.na(flipper_length_mm)) |&gt;\n  mutate(bill_flipper_ratio = bill_length_mm / flipper_length_mm) |&gt;\n  select(species, island, bill_flipper_ratio) |&gt;\n  arrange(desc(bill_flipper_ratio)) |&gt;\n  head(n = 5)\n\n# A tibble: 5 × 3\n  species   island bill_flipper_ratio\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Chinstrap Dream               0.320\n2 Chinstrap Dream               0.275\n3 Chinstrap Dream               0.270\n4 Chinstrap Dream               0.270\n5 Chinstrap Dream               0.268"
  },
  {
    "objectID": "demos/01-into-tidyverse.html#more-pipeline-actions",
    "href": "demos/01-into-tidyverse.html#more-pipeline-actions",
    "title": "Demo 01: Into the tidyverse",
    "section": "More pipeline actions!",
    "text": "More pipeline actions!\nInstead of head(), we can slice() our dataset to choose the observations based on the position\n\npenguins |&gt;\n  filter(!is.na(flipper_length_mm)) |&gt;\n  mutate(bill_flipper_ratio = bill_length_mm / flipper_length_mm) |&gt;\n  select(species, island, bill_flipper_ratio) |&gt;\n  arrange(desc(bill_flipper_ratio)) |&gt;\n  slice(c(1, 2, 10, 100))\n\n# A tibble: 4 × 3\n  species   island bill_flipper_ratio\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Chinstrap Dream               0.320\n2 Chinstrap Dream               0.275\n3 Chinstrap Dream               0.264\n4 Gentoo    Biscoe              0.227"
  },
  {
    "objectID": "demos/01-into-tidyverse.html#grouped-operations",
    "href": "demos/01-into-tidyverse.html#grouped-operations",
    "title": "Demo 01: Into the tidyverse",
    "section": "Grouped operations",
    "text": "Grouped operations\nWe group_by() to split our dataset into groups based on a variable’s values\n\npenguins |&gt;\n  filter(!is.na(flipper_length_mm)) |&gt;\n  group_by(island) |&gt;\n  summarize(n_penguins = n(), #counts number of rows in group\n            ave_flipper_length = mean(flipper_length_mm), \n            sum_bill_depth = sum(bill_depth_mm),\n            .groups = \"drop\") |&gt; # all levels of grouping dropping\n  arrange(desc(n_penguins)) |&gt;\n  slice(1:5)\n\n# A tibble: 3 × 4\n  island    n_penguins ave_flipper_length sum_bill_depth\n  &lt;fct&gt;          &lt;int&gt;              &lt;dbl&gt;          &lt;dbl&gt;\n1 Biscoe           167               210.          2651.\n2 Dream            124               193.          2275.\n3 Torgersen         51               191.           940.\n\n\n\ngroup_by() is only useful in a pipeline (e.g. with summarize()), and pay attention to its behavior\nspecify the .groups field to decide if observations remain grouped or not after summarizing (you can also use ungroup() for this as well)"
  },
  {
    "objectID": "demos/01-into-tidyverse.html#putting-it-all-together",
    "href": "demos/01-into-tidyverse.html#putting-it-all-together",
    "title": "Demo 01: Into the tidyverse",
    "section": "Putting it all together…",
    "text": "Putting it all together…\nAs your own exercise, create a tidy dataset where each row == an island with the following variables:\n\nnumber of penguins,\nnumber of unique species on the island (see help(unique)),\naverage body_mass_g,\nvariance (see help(var)) of bill_depth_mm\n\nPrior to making those variables, make sure to filter missings and also only consider female penguins. Then arrange the islands in order of the average body_mass_g:\n\n# INSERT YOUR CODE HERE"
  },
  {
    "objectID": "demos/03-regression.html",
    "href": "demos/03-regression.html",
    "title": "Demo 03: More Regression with Penguins",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs. Throughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")\n\nWe load the penguins data in the same way as before:\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "demos/03-regression.html#regression-with-penguins",
    "href": "demos/03-regression.html#regression-with-penguins",
    "title": "Demo 03: More Regression with Penguins",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs. Throughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")\n\nWe load the penguins data in the same way as before:\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "demos/03-regression.html#simple-linear-regression-based-only-on-bill-length",
    "href": "demos/03-regression.html#simple-linear-regression-based-only-on-bill-length",
    "title": "Demo 03: More Regression with Penguins",
    "section": "Simple Linear Regression (based only on bill length)",
    "text": "Simple Linear Regression (based only on bill length)\nFirst, we can run a simple linear regression (the first model) based only on bill length. We can display this line via geom_smooth():\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE)\n\n\n\n\n\n\n\n\nAnd display the regression model output using summary():\n\nsummary(lm(bill_depth_mm ~ bill_length_mm, data = penguins))\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1381 -1.4263  0.0164  1.3841  4.5255 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    20.88547    0.84388  24.749  &lt; 2e-16 ***\nbill_length_mm -0.08502    0.01907  -4.459 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.922 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.05525,   Adjusted R-squared:  0.05247 \nF-statistic: 19.88 on 1 and 340 DF,  p-value: 1.12e-05\n\n\nWe can write this regression model as:\n\\[\\text{depth} \\sim N(\\beta_0 + \\beta_L \\cdot \\text{length}, \\sigma^2)\\]\nNote that \\(\\beta_0\\) is the intercept and \\(\\beta_L\\) is the slope.\nThus, our estimates are:\n\n\\(\\hat{\\beta}_0 = 20.88547\\)\n\\(\\hat{\\beta}_L = 12.43\\)\n\\(\\hat{\\sigma}^2 = 1.922^2\\)"
  },
  {
    "objectID": "demos/03-regression.html#multiple-linear-regression-additive",
    "href": "demos/03-regression.html#multiple-linear-regression-additive",
    "title": "Demo 03: More Regression with Penguins",
    "section": "Multiple Linear Regression (Additive)",
    "text": "Multiple Linear Regression (Additive)\nWe can also run the second model, which is based on length and species, but with only additive effects. First, we’ll check the counts of the species variable to ensure that the species with the highest number of observations if the reference level (i.e., the first level for a factor variable):\n\ntable(penguins$species)\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n\nLooks like we’re lucky and that the Adelie species is the most popular and is already first due to alphabetical order. What function would we need to do to re-order the variable?\nNext, we’ll fit the regression that accounts for species without an interaction - so it’s just an additive effect:\n\ndepth_lm_species_add &lt;- lm(bill_depth_mm ~ bill_length_mm + species,\n                           data = penguins)\nsummary(depth_lm_species_add)\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm + species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4529 -0.6864 -0.0508  0.5519  3.5915 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      10.59218    0.68302  15.508  &lt; 2e-16 ***\nbill_length_mm    0.19989    0.01749  11.427  &lt; 2e-16 ***\nspeciesChinstrap -1.93319    0.22416  -8.624 2.55e-16 ***\nspeciesGentoo    -5.10602    0.19142 -26.674  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9533 on 338 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.769, Adjusted R-squared:  0.7669 \nF-statistic: 375.1 on 3 and 338 DF,  p-value: &lt; 2.2e-16\n\n\nThis tells us that Chinstrap is different from Adelie and Gentoo is different from Adelie, but it does NOT tell us Chinstrap is different from Gentoo. That would require another model with a reordered species variable. Exercise: Reorder species so that Gentoo is the reference level and compare to the results above.\nWe can manually extract intercepts and coefficients to use for plotting (read the code comments!):\n\n# Calculate species-specific intercepts in order:\nintercepts &lt;- # First for `Adelie` it's just the initial intercept\n  c(coef(depth_lm_species_add)[\"(Intercept)\"],\n    # Next for `Chinstrap` it's the intercept plus the `Chinstrap` term:\n    coef(depth_lm_species_add)[\"(Intercept)\"] + \n      coef(depth_lm_species_add)[\"speciesChinstrap\"],\n    # And finally for `Gentoo` it's again the intercept plus the `Gentoo` term\n    coef(depth_lm_species_add)[\"(Intercept)\"] + \n      coef(depth_lm_species_add)[\"speciesGentoo\"])\n\n# Create a small table to store the intercept, slopes, and species:\nlines_tbl &lt;- tibble(\"intercepts\" = intercepts,\n                    # Slopes are the same for each, thus use rep()\n                    \"slopes\" = rep(coef(depth_lm_species_add)[\"bill_length_mm\"],\n                                   3),\n                    # And the levels of species:\n                    \"species\" = levels(penguins$species))\n\nWe can now plot this model by specifying the regression lines with geom_abline() using the newly constructed lines_tbl as the data for this layer:\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(data = lines_tbl,\n              aes(intercept = intercepts, slope = slopes,\n                  color = species)) +\n  labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\", \n       title = \"Bill depth versus weight by species\")\n\n\n\n\n\n\n\n\nThis is a great example of Simpson’s Paradox! We originally observed a negative linear relationship between depth and length, but now observe a positive linear relationship within species!"
  },
  {
    "objectID": "demos/03-regression.html#multiple-linear-regression-interactive",
    "href": "demos/03-regression.html#multiple-linear-regression-interactive",
    "title": "Demo 03: More Regression with Penguins",
    "section": "Multiple Linear Regression (Interactive)",
    "text": "Multiple Linear Regression (Interactive)\nNext, we can run the third model, which is based on length and species, including interaction effects. This is the default type of model displayed when we map species to the color aesthetic for the geom_smooth() layer. In the plot below, we display across both layers, geom_point() and geom_smooth() by mapping species to color in the initial ggplot canvas construction:\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\", \n       title = \"Bill depth versus weight by species\")\n\n\n\n\n\n\n\n\nWhat about the summary of this model? Is the inclusion of interaction terms relevant? Note that by default, multiplying two variables in the lm() formula below includes both the additive AND interaction terms.\n\ndepth_lm_species_int &lt;- lm(bill_depth_mm ~ bill_length_mm * species,\n                           data = penguins)\nsummary(depth_lm_species_int)\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm * species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6574 -0.6675 -0.0524  0.5383  3.5032 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     11.40912    1.13812  10.025  &lt; 2e-16 ***\nbill_length_mm                   0.17883    0.02927   6.110 2.76e-09 ***\nspeciesChinstrap                -3.83998    2.05398  -1.870 0.062419 .  \nspeciesGentoo                   -6.15812    1.75451  -3.510 0.000509 ***\nbill_length_mm:speciesChinstrap  0.04338    0.04558   0.952 0.341895    \nbill_length_mm:speciesGentoo     0.02601    0.04054   0.642 0.521590    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9548 on 336 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7662 \nF-statistic: 224.5 on 5 and 336 DF,  p-value: &lt; 2.2e-16\n\n\nThe interaction terms do NOT appear to be necessary to include. This is justified by both the lack of significance and the slight drop in adjusted R-squared."
  },
  {
    "objectID": "demos/03-regression.html#what-about-the-intercept",
    "href": "demos/03-regression.html#what-about-the-intercept",
    "title": "Demo 03: More Regression with Penguins",
    "section": "What about the intercept?",
    "text": "What about the intercept?\nRemember the meaning of the intercept term… that is not reasonable in this setting because penguins will never have bills with length of 0mm! We should update the additive model (since we found the interaction terms to not be meaningful) to remove the intercept. This can be done by adding a 0 term to the lm() formula:\n\ndepth_lm_remove_b0 &lt;- lm(bill_depth_mm ~ 0 + bill_length_mm + species,\n                         data = penguins)\nsummary(depth_lm_remove_b0)\n\n\nCall:\nlm(formula = bill_depth_mm ~ 0 + bill_length_mm + species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4529 -0.6864 -0.0508  0.5519  3.5915 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \nbill_length_mm    0.19989    0.01749  11.427  &lt; 2e-16 ***\nspeciesAdelie    10.59218    0.68302  15.508  &lt; 2e-16 ***\nspeciesChinstrap  8.65899    0.86207  10.044  &lt; 2e-16 ***\nspeciesGentoo     5.48616    0.83547   6.567 1.94e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9533 on 338 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.997, Adjusted R-squared:  0.997 \nF-statistic: 2.795e+04 on 4 and 338 DF,  p-value: &lt; 2.2e-16\n\n\nWhat changed in the summary output? Why did that occur?\nWe can copy-and-paste our code from above to add these appropriate regression lines:\n\n# Calculate species-specific intercepts in order:\nnew_intercepts &lt;- # First for `Adelie` \n  c(coef(depth_lm_remove_b0)[\"speciesAdelie\"],\n    # Next for `Chinstrap` \n    coef(depth_lm_remove_b0)[\"speciesChinstrap\"],\n    # And finally for `Gentoo` \n    coef(depth_lm_remove_b0)[\"speciesGentoo\"])\n\n# Create a small table to store the intercept, slopes, and species:\nnew_lines_tbl &lt;- \n  tibble(\"intercepts\" = new_intercepts,\n         # Slopes are the same for each, thus use rep()\n         \"slopes\" = rep(coef(depth_lm_remove_b0)[\"bill_length_mm\"],\n                        3),\n         # And the levels of species:\n         \"species\" = levels(penguins$species))\n\nAgain, create the display:\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(data = new_lines_tbl,\n              aes(intercept = intercepts, slope = slopes,\n                  color = species)) +\n  labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\", \n       title = \"Bill depth versus weight by species\")\n\n\n\n\n\n\n\n\nWhy is this the same display as before? Here’s a great description of why we observe a higher R-squared with the intercept-term excluded from the model."
  },
  {
    "objectID": "demos.html",
    "href": "demos.html",
    "title": "Demos",
    "section": "",
    "text": "Demo\nDate\nTitle\nDemo file\n\n\n\n\n1\nJan 13\nInto the tidyverse\nHTML\n\n\n2\nFeb 17\nScatterplots and Linear Regression\nHTML\n\n\n3\nFeb 19\nMore Regression with Penguins\nHTML\n\n\n4\nMar 10\nNonlinear Regression and Pairs Plots\nHTML\n\n\n5\nMar 12\nContour Plots, Heat Maps, and Into High-Dimensional Data\nHTML\n\n\n6\nMar 17\nVisualizing Distances for High-Dimensional Data\nHTML",
    "crumbs": [
      "Demos"
    ]
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#announcements-previously-and-today",
    "href": "lectures/09-compare-distr-power.html#announcements-previously-and-today",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW3 is due TONIGHT by 11:59 PM and you have Lab 5 again on Friday!\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Tuesdays @ 11 AM\n\n\n\n\nFinished discussed density based visualizations\nIntroduced KS test for testing if distribution follows a particular distribution\nGraphics are extremely useful because human eyes can quickly compare and contrast distributions…\n\n\n\n\nTODAY:\n\nUnderstanding the statistical power of tests and graphics"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#flipper-length-example",
    "href": "lectures/09-compare-distr-power.html#flipper-length-example",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Flipper length example",
    "text": "Flipper length example"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#statistical-tests-for-comparing-distributions",
    "href": "lectures/09-compare-distr-power.html#statistical-tests-for-comparing-distributions",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Statistical Tests for Comparing Distributions",
    "text": "Statistical Tests for Comparing Distributions\n\n\nWe’ve focused on assessing if a single quantitative variable follows a particular distribution\n\nLogic of one-sample KS test: Compare empirical distribution to theoretical distribution\n\n\n\n\n\nHow do we compare multiple empirical distributions?\nVery common scenario: Determine if a quantitative variable depends on a categorical variable, examples:\n\nClinical trials with multiple treatments\nAssessing differences across race, gender, socioeconomic status\nIndustrial experiments, A/B testing\nComparing song duration across different genres?\n\nCan use overlayed densities, side-by-side violin plots, facetted histograms\nRemember: plotting conditional distributions… but when are differences in a graphic statistically significant?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre",
    "href": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre",
    "title": "Comparing Distributions and Statistical Power",
    "section": "TidyTuesday Spotify Songs - Duration by Genre",
    "text": "TidyTuesday Spotify Songs - Duration by Genre"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre-1",
    "href": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "TidyTuesday Spotify Songs - Duration by Genre",
    "text": "TidyTuesday Spotify Songs - Duration by Genre"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#kolmogorov-smirnov-test-again",
    "href": "lectures/09-compare-distr-power.html#kolmogorov-smirnov-test-again",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Kolmogorov-Smirnov Test… Again",
    "text": "Kolmogorov-Smirnov Test… Again\nKS test can also be used to compare two empirical distributions \\(\\hat{F}_A(x)\\) and \\(\\hat{F}_B\\), via test statistic for two samples:\n\\[\n\\text{max}_x |\\hat{F}_A(x) - \\hat{F}_B(x)|\n\\]\n\nspotify_songs &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')\nrap_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"rap\") |&gt; pull(duration_ms)\nrock_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"rock\") |&gt; pull(duration_ms)\n\nks.test(rap_duration, y = rock_duration)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  rap_duration and rock_duration\nD = 0.22386, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#kolmogorov-smirnov-test-again-1",
    "href": "lectures/09-compare-distr-power.html#kolmogorov-smirnov-test-again-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Kolmogorov-Smirnov Test… Again",
    "text": "Kolmogorov-Smirnov Test… Again\nKS test can also be used to compare two empirical distributions \\(\\hat{F}_A(x)\\) and \\(\\hat{F}_B\\), via test statistic for two samples:\n\\[\n\\text{max}_x |\\hat{F}_A(x) - \\hat{F}_B(x)|\n\\]"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#statistical-tests-for-comparing-distributions-1",
    "href": "lectures/09-compare-distr-power.html#statistical-tests-for-comparing-distributions-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Statistical Tests for Comparing Distributions",
    "text": "Statistical Tests for Comparing Distributions\n\nInfinite number of ways that you can compare multiple quantitative distributions, 3 common ways:\n\nAny difference at all? \nDifference in means?\n\n\nNull hypothesis: \\(H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_K\\) (use t.test or oneway.test() functions)\nCan assume the variances are all the same or differ\nIf reject, can only conclude not all means are equal\n\n\nDifference in variances?\n\n\nNull hypothesis: \\(H_0: \\sigma^2_1 = \\sigma^2_2 = \\cdots = \\sigma^2_K\\) (use bartlett.test() function)\nIf reject, can only conclude not all variances are equal\n\nUnlike the KS test, difference in means and variances are sensitive to non-Normality\n\nDifferent distributions can yield insignificant results"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock-1",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock-2",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock-2",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-pop-and-rap",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-pop-and-rap",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between pop and rap?",
    "text": "Test difference between pop and rap?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#recap-one-sample-ks-test",
    "href": "lectures/09-compare-distr-power.html#recap-one-sample-ks-test",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Recap: One-Sample KS Test",
    "text": "Recap: One-Sample KS Test\n\n\nHave a single sample \\(\\mathbf{X} = (X_1,\\dots,X_n)\\)\nWant to test: Does \\(\\mathbf{X}\\) follow a particular distribution?\nCompares the empirical CDF of \\(\\mathbf{X}\\) to the theoretical CDF of a particular distribution:\n\n\\[\\underbrace{F(x) = P(X \\leq x)}_{\\text{theoretical CDF}}, \\hspace{0.2in} \\underbrace{\\hat{F}(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(X_i \\leq x)}_{\\text{empirical CDF}}\\]\n\nNull hypothesis: \\(\\mathbf{X}\\) follows a distribution with CDF \\(F(x)\\)\nAlternative hypothesis: \\(\\mathbf{X}\\) does not follow this distribution\nTest statistic: \\(\\max_x |\\hat{F}(x) - F(x)|\\)\nIf \\(\\hat{F}(x)\\) is far away from \\(F(x)\\) \\(\\rightarrow\\) reject null"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#recap-two-sample-ks-test",
    "href": "lectures/09-compare-distr-power.html#recap-two-sample-ks-test",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Recap: Two-Sample KS Test",
    "text": "Recap: Two-Sample KS Test\n\n\nHave two samples \\(\\mathbf{X} = (X_1,\\dots,X_m)\\), \\(\\mathbf{Y} = (Y_1,\\dots,Y_n)\\)\nWant to test: Do \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) follow the same distribution?\nCompares the empirical CDFs of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\):\n\n\\[\\underbrace{\\hat{F}_X(z) = \\frac{1}{m} \\sum_{i=1}^m \\mathbb{I}(X_i \\leq z)}_{\\text{empirical CDF of } \\mathbf{X}} \\hspace{0.2in} \\underbrace{\\hat{F}_Y(z) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(Y_i \\leq z)}_{\\text{empirical CDF of } \\mathbf{Y}}\\]\n\nNull hypothesis: \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) follow the same distribution.\nAlternative hypothesis: \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) do not follow the same distribution\nTest statistic: \\(\\max_z |\\hat{F}_X(z) - \\hat{F}_Y(z)|\\)\nIf \\(\\hat{F}_X\\) and \\(\\hat{F}_Y\\) are far away from each other \\(\\rightarrow\\) reject null"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre-2",
    "href": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre-2",
    "title": "Comparing Distributions and Statistical Power",
    "section": "TidyTuesday Spotify Songs - Duration by Genre",
    "text": "TidyTuesday Spotify Songs - Duration by Genre"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#what-about-the-difference-between-pop-and-rap",
    "href": "lectures/09-compare-distr-power.html#what-about-the-difference-between-pop-and-rap",
    "title": "Comparing Distributions and Statistical Power",
    "section": "What about the difference between pop and rap?",
    "text": "What about the difference between pop and rap?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#what-about-the-difference-between-pop-and-rap-1",
    "href": "lectures/09-compare-distr-power.html#what-about-the-difference-between-pop-and-rap-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "What about the difference between pop and rap?",
    "text": "What about the difference between pop and rap?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#significant-difference-with-large-sample-size",
    "href": "lectures/09-compare-distr-power.html#significant-difference-with-large-sample-size",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Significant difference with large sample size",
    "text": "Significant difference with large sample size\n\nspotify_songs &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')\ntable(spotify_songs$playlist_genre)\n\n\n  edm latin   pop   r&b   rap  rock \n 6043  5155  5507  5431  5746  4951 \n\nrap_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"rap\") |&gt; pull(duration_ms)\npop_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"pop\") |&gt; pull(duration_ms)\n\nks.test(rap_duration, y = pop_duration)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  rap_duration and pop_duration\nD = 0.14569, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#what-happens-if-we-had-a-smaller-sample",
    "href": "lectures/09-compare-distr-power.html#what-happens-if-we-had-a-smaller-sample",
    "title": "Comparing Distributions and Statistical Power",
    "section": "What happens if we had a smaller sample?",
    "text": "What happens if we had a smaller sample?\n\nset.seed(2017)\nsample_songs &lt;- spotify_songs |&gt;\n  group_by(playlist_genre) |&gt; \n  slice_sample(n = 100)\n\ntable(sample_songs$playlist_genre)\n\n\n  edm latin   pop   r&b   rap  rock \n  100   100   100   100   100   100 \n\nsample_rap_duration &lt;- sample_songs |&gt; filter(playlist_genre == \"rap\") |&gt; pull(duration_ms)\nsample_pop_duration &lt;- sample_songs |&gt; filter(playlist_genre == \"pop\") |&gt; pull(duration_ms)\n\nks.test(sample_rap_duration, y = sample_pop_duration)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  sample_rap_duration and sample_pop_duration\nD = 0.16, p-value = 0.1545\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#but-it-still-looks-different",
    "href": "lectures/09-compare-distr-power.html#but-it-still-looks-different",
    "title": "Comparing Distributions and Statistical Power",
    "section": "But it still looks different???",
    "text": "But it still looks different???"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-means-and-variances",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-means-and-variances",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between means and variances?",
    "text": "Test difference between means and variances?\nCan test difference in means using t.test():\n\nt.test(sample_rap_duration, sample_pop_duration)\n\n\n    Welch Two Sample t-test\n\ndata:  sample_rap_duration and sample_pop_duration\nt = 0.83091, df = 172.78, p-value = 0.4072\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -8766.158 21512.638\nsample estimates:\nmean of x mean of y \n 221645.7  215272.5"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-means-and-variances-1",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-means-and-variances-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between means and variances?",
    "text": "Test difference between means and variances?\nCan test difference in variances using bartlett.test():\n\nbartlett.test(list(sample_rap_duration, sample_pop_duration))\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  list(sample_rap_duration, sample_pop_duration)\nBartlett's K-squared = 15.54, df = 1, p-value = 8.08e-05\n\n\nRejects at \\(\\alpha = 0.05\\) even with this smaller sample size!\n\n\nWhy did the KS test say they weren’t different when the graph were clearly different? Two possible reasons:\n\nThe sample size might be too small to detect a difference\nThe KS test is known to have low power"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#statistical-power",
    "href": "lectures/09-compare-distr-power.html#statistical-power",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Statistical power",
    "text": "Statistical power\nStatistical power is key to really understanding graphics - you need to know when you’re looking at real effects versus noise\nHere are two definitions of power (one in English, one in math):\n\nEnglish: The probability that we reject the null hypothesis when the null hypothesis is false.\nMath: \\(P(\\text{p-value} \\leq \\alpha | H_0\\) is false)"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#toy-example-for-understanding-statistical-power",
    "href": "lectures/09-compare-distr-power.html#toy-example-for-understanding-statistical-power",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Toy example for understanding statistical power",
    "text": "Toy example for understanding statistical power\n\nConsider two samples:\n\\[(X_1,\\dots,X_n) \\sim N(0, 1)\\] \\[(Y_1,\\dots,Y_n) \\sim N(\\delta, 1)\\]\nLet’s say we use t.test(x, y)\nWe’ll simulate \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) 1000 times for some \\(n\\) and \\(\\delta &gt; 0\\)\nWe’ll count the number of times we reject\n\\[\\text{Power} = P(\\text{p-value} \\leq \\alpha | H_0 \\text{ false}) \\\\\n            = P(\\text{p-value} \\leq \\alpha | \\delta &gt; 0) \\\\\n            \\approx \\frac{\\text{# times reject}}{1000}\\]\nWe’ll consider \\(n = 10, 20, \\dots, 1000\\) and \\(\\delta = 0.1\\) or \\(\\delta = 0.25\\)"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#toy-example-power-of-t-test",
    "href": "lectures/09-compare-distr-power.html#toy-example-power-of-t-test",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Toy example: power of \\(t\\)-test",
    "text": "Toy example: power of \\(t\\)-test"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#another-toy-example",
    "href": "lectures/09-compare-distr-power.html#another-toy-example",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Another toy example",
    "text": "Another toy example\nConsider two samples:\n\\[(X_1,\\dots,X_n) \\sim N(0, 1)\\] \\[(Y_1,\\dots,Y_n) \\sim N(0, 1.5)\\]\nLet’s consider three ways to test differences:\n\nt.test(x, y)\nbartlett.test(list(x, y))\nks.test(x,y)\n\nWe’ll simulate \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) 1000 times for samples sizes \\(n = 10, 20, \\dots, 1000\\)\nWhat do you think the power curves will look like for these methods?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#comparison-of-power-for-the-different-tests",
    "href": "lectures/09-compare-distr-power.html#comparison-of-power-for-the-different-tests",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Comparison of power for the different tests",
    "text": "Comparison of power for the different tests"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#recap-and-next-steps",
    "href": "lectures/09-compare-distr-power.html#recap-and-next-steps",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nGraphics should be paired with statistical analyses to determine if what you see is a true effect versus noise\nEven if there is a true effect, you may have limited power to detect it (some effects are easier to detect than others)\nRemember: Power is the probability you reject when the null is false. Things that increase statistical power:\n\nIncrease sample size\nReduce variance/error\nIncrease differences / effects\nChoose appropriate tests!\n\n\n\n\n\nHW3 is due TONIGHT and you have Lab 5 on Friday\nNext time: 2D Quantitative Data - Scatterplots and Linear Regression"
  },
  {
    "objectID": "lectures/07-density-estimation.html#announcements-previously-and-today",
    "href": "lectures/07-density-estimation.html#announcements-previously-and-today",
    "title": "Density Estimation",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW2 is due TONIGHT by 11:59 PM\nYou have Lab 4 again on Friday!\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Wednesdays @ 11 AM\n\n\n\n\n\nVisualize 1D quantitative data to inspect center, spread, and shape\nBoxplots are only a display of summary statistics (i.e., they suck)\nHistograms display shape of the distribution, but comes with tradeoffs\nDensity curves provide an easy way to visualize conditional distributions\n\n\n\n\n\nTODAY:\n\nDisplaying smooth densities\nHow does kernel density estimation work?"
  },
  {
    "objectID": "lectures/07-density-estimation.html#continuous-densities",
    "href": "lectures/07-density-estimation.html#continuous-densities",
    "title": "Density Estimation",
    "section": "Continuous Densities",
    "text": "Continuous Densities\nDistribution of any continuous random variable \\(X\\) is defined by a probability density function (PDF), typically denoted by \\(f(x)\\)\n\nProbability continuous variable \\(X\\) takes a particular value is 0, why?\n\nUse PDF to provide a relative likelihood,\n\ne.g., Normal distribution: \\(f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp(- \\frac{(x - \\mu)^2}{2\\sigma^2})\\)\n\n\n\nProperties of densities\n\nHow do we estimate densities?"
  },
  {
    "objectID": "lectures/07-density-estimation.html#normal-distribution",
    "href": "lectures/07-density-estimation.html#normal-distribution",
    "title": "Density Estimation",
    "section": "Normal distribution",
    "text": "Normal distribution"
  },
  {
    "objectID": "lectures/07-density-estimation.html#uniform-distribution",
    "href": "lectures/07-density-estimation.html#uniform-distribution",
    "title": "Density Estimation",
    "section": "Uniform distribution",
    "text": "Uniform distribution"
  },
  {
    "objectID": "lectures/07-density-estimation.html#gamma-also-exponential-and-chi-squared-distribution",
    "href": "lectures/07-density-estimation.html#gamma-also-exponential-and-chi-squared-distribution",
    "title": "Density Estimation",
    "section": "Gamma (also Exponential and Chi-squared) distribution",
    "text": "Gamma (also Exponential and Chi-squared) distribution"
  },
  {
    "objectID": "lectures/07-density-estimation.html#beta-distribution",
    "href": "lectures/07-density-estimation.html#beta-distribution",
    "title": "Density Estimation",
    "section": "Beta distribution",
    "text": "Beta distribution"
  },
  {
    "objectID": "lectures/07-density-estimation.html#normalize-histogram-frequencies-with-density",
    "href": "lectures/07-density-estimation.html#normalize-histogram-frequencies-with-density",
    "title": "Density Estimation",
    "section": "Normalize histogram frequencies with density",
    "text": "Normalize histogram frequencies with density\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_histogram(aes(y = after_stat(density), fill = species),\n                 position = \"identity\", alpha = 0.3)"
  },
  {
    "objectID": "lectures/07-density-estimation.html#can-use-density-curves-instead",
    "href": "lectures/07-density-estimation.html#can-use-density-curves-instead",
    "title": "Density Estimation",
    "section": "Can use density curves instead",
    "text": "Can use density curves instead\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(aes(color = species))"
  },
  {
    "objectID": "lectures/07-density-estimation.html#kernel-density-estimation",
    "href": "lectures/07-density-estimation.html#kernel-density-estimation",
    "title": "Density Estimation",
    "section": "Kernel density estimation",
    "text": "Kernel density estimation\nGoal: estimate PDF \\(f(x)\\) for all possible values (assuming it is continuous & smooth)\n\n\\[\n\\text{Kernel density estimate: } \\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\n\\]\n\n\n\n\n\\(n =\\) sample size, \\(x =\\) new point to estimate \\(f(x)\\) (does NOT have to be in dataset!)\n\n\n\n\n\n\n\\(h =\\) bandwidth, analogous to histogram bin width, ensures \\(\\hat{f}(x)\\) integrates to 1\n\\(x_i =\\) \\(i\\)th observation in dataset\n\n\n\n\n\n\n\\(K_h(x - x_i)\\) is the Kernel function, creates weight given distance of \\(i\\)th observation from new point\n\nas \\(|x - x_i| \\rightarrow \\infty\\) then \\(K_h(x - x_i) \\rightarrow 0\\), i.e. further apart \\(i\\)th row is from \\(x\\), smaller the weight\nas bandwidth \\(h \\uparrow\\) weights are more evenly spread out (as \\(h \\downarrow\\) more concentrated around \\(x\\))\ntypically use Gaussian / Normal kernel: \\(\\propto e^{-(x - x_i)^2 / 2h^2}\\)\n\\(K_h(x - x_i)\\) is large when \\(x_i\\) is close to \\(x\\)"
  },
  {
    "objectID": "lectures/07-density-estimation.html#wikipedia-example",
    "href": "lectures/07-density-estimation.html#wikipedia-example",
    "title": "Density Estimation",
    "section": "Wikipedia example",
    "text": "Wikipedia example"
  },
  {
    "objectID": "lectures/07-density-estimation.html#we-display-kernel-density-estimates-with-geom_density",
    "href": "lectures/07-density-estimation.html#we-display-kernel-density-estimates-with-geom_density",
    "title": "Density Estimation",
    "section": "We display kernel density estimates with geom_density()",
    "text": "We display kernel density estimates with geom_density()\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#choice-of-kernel",
    "href": "lectures/07-density-estimation.html#choice-of-kernel",
    "title": "Density Estimation",
    "section": "Choice of kernel?",
    "text": "Choice of kernel?"
  },
  {
    "objectID": "lectures/07-density-estimation.html#what-about-the-bandwidth",
    "href": "lectures/07-density-estimation.html#what-about-the-bandwidth",
    "title": "Density Estimation",
    "section": "What about the bandwidth?",
    "text": "What about the bandwidth?\nUse Gaussian reference rule (rule-of-thumb) \\(\\approx 1.06 \\cdot \\sigma \\cdot n^{-1/5}\\), where \\(\\sigma\\) is the observed standard deviation\nModify the bandwidth using the adjust argument - value to multiply default bandwidth by\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(adjust = 0.5) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#what-about-the-bandwidth-1",
    "href": "lectures/07-density-estimation.html#what-about-the-bandwidth-1",
    "title": "Density Estimation",
    "section": "What about the bandwidth?",
    "text": "What about the bandwidth?\nUse Gaussian reference rule (rule-of-thumb) \\(\\approx 1.06 \\cdot \\sigma \\cdot n^{-1/5}\\), where \\(\\sigma\\) is the observed standard deviation\nModify the bandwidth using the adjust argument - value to multiply default bandwidth by\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(adjust = 2) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#caution-dealing-with-bounded-data",
    "href": "lectures/07-density-estimation.html#caution-dealing-with-bounded-data",
    "title": "Density Estimation",
    "section": "CAUTION: dealing with bounded data…",
    "text": "CAUTION: dealing with bounded data…\n\nset.seed(101)\nbound_data &lt;- tibble(fake_x = runif(100))\n\nbound_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_density() +\n  geom_rug(alpha = 0.5) + #&lt;&lt;\n  stat_function(data = \n                  tibble(fake_x = c(0, 1)),\n                fun = dunif, color = \"red\") +\n  scale_x_continuous(limits = c(-.5, 1.5))"
  },
  {
    "objectID": "lectures/07-density-estimation.html#visualizing-conditional-distributions-violin-plots",
    "href": "lectures/07-density-estimation.html#visualizing-conditional-distributions-violin-plots",
    "title": "Density Estimation",
    "section": "Visualizing conditional distributions: violin plots",
    "text": "Visualizing conditional distributions: violin plots\n\npenguins |&gt;\n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_violin() +\n  coord_flip()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#visualizing-conditional-distributions-violin-plots-1",
    "href": "lectures/07-density-estimation.html#visualizing-conditional-distributions-violin-plots-1",
    "title": "Density Estimation",
    "section": "Visualizing conditional distributions: violin plots",
    "text": "Visualizing conditional distributions: violin plots\n\npenguins |&gt;\n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_violin() + \n  geom_boxplot(width = .2) +\n  coord_flip()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#visualizing-conditional-distributions-ggridges",
    "href": "lectures/07-density-estimation.html#visualizing-conditional-distributions-ggridges",
    "title": "Density Estimation",
    "section": "Visualizing conditional distributions: ggridges",
    "text": "Visualizing conditional distributions: ggridges\n\nlibrary(ggridges)\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = species)) +\n  geom_density_ridges(rel_min_height = 0.01)"
  },
  {
    "objectID": "lectures/07-density-estimation.html#visualizing-conditional-distributions-ggbeeswarm",
    "href": "lectures/07-density-estimation.html#visualizing-conditional-distributions-ggbeeswarm",
    "title": "Density Estimation",
    "section": "Visualizing conditional distributions: ggbeeswarm",
    "text": "Visualizing conditional distributions: ggbeeswarm\n\nlibrary(ggbeeswarm)\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = species)) +\n  geom_beeswarm(cex = 1.5) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#recap-and-next-steps",
    "href": "lectures/07-density-estimation.html#recap-and-next-steps",
    "title": "Density Estimation",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nSmoothed densities are a flexible tool for visualizing 1D distribution\nThere are two choices we need to make for kernel density estimation:\n\nBandwidth: Determines smoothness of distribution, usually data-driven choice\nKernel: Determines how much influence each observation should have on each other during estimation, usually context driven\n\nSeveral other types of density-based displays: violins, ridges, beeswarm plots\n\n\n\n\n\nHW2 is due TONIGHT and you have Lab 4 on Friday\nNext time: Graphical inference for 1D quantitative data\nRecommended reading: CW Chapter 7 Visualizing distributions: Histograms and density plots"
  },
  {
    "objectID": "lectures/05-2dcat.html#announcements-previously-and-today",
    "href": "lectures/05-2dcat.html#announcements-previously-and-today",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW1 is due TONIGHT by 11:59 PM\nYou have Lab 3 again on Friday\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Wednesdays @ 11 AM\n\n\n\n\n\nDiscussed how similar looking graphics can have very different statistical results (thinking about power)\nDiscussed the challenges of multiple testing\n\n\n\n\n\nTODAY:\n\nVisuals for 2D categorical data\nHow do we visualize inference for 2D categorical data?"
  },
  {
    "objectID": "lectures/05-2dcat.html#d-categorical-basics",
    "href": "lectures/05-2dcat.html#d-categorical-basics",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "2D categorical basics",
    "text": "2D categorical basics"
  },
  {
    "objectID": "lectures/05-2dcat.html#d-categorical-basics-1",
    "href": "lectures/05-2dcat.html#d-categorical-basics-1",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "2D categorical basics",
    "text": "2D categorical basics\n\naddmargins(table(\"Species\" = penguins$species, \"Island\" = penguins$island))\n\n           Island\nSpecies     Biscoe Dream Torgersen Sum\n  Adelie        44    56        52 152\n  Chinstrap      0    68         0  68\n  Gentoo       124     0         0 124\n  Sum          168   124        52 344\n\n\n\nColumn and row sums: marginal distributions\nValues within rows: conditional distribution for Island given Species\nValues within columns: conditional distribution for Species given Island\nBottom right: total number of observations"
  },
  {
    "objectID": "lectures/05-2dcat.html#connecting-distributions-to-visualizations",
    "href": "lectures/05-2dcat.html#connecting-distributions-to-visualizations",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Connecting distributions to visualizations",
    "text": "Connecting distributions to visualizations\nFive distributions for two categorical variables \\(A\\) and \\(B\\):\n\nMarginals: \\(P(A)\\) and \\(P(B)\\)\nConditionals: \\(P(A | B)\\) and \\(P(B|A)\\)\nJoint: \\(P(A, B)\\)\n\nWe use bar charts to visualize marginal distributions for categorical variables…\n\nAnd we’ll use more bar charts to visualize conditional and joint distributions!"
  },
  {
    "objectID": "lectures/05-2dcat.html#stacked-bar-charts---a-bar-chart-of-spine-charts",
    "href": "lectures/05-2dcat.html#stacked-bar-charts---a-bar-chart-of-spine-charts",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Stacked bar charts - a bar chart of spine charts",
    "text": "Stacked bar charts - a bar chart of spine charts\n\npenguins |&gt;\n  ggplot(aes(x = species, fill = island)) +\n  geom_bar() + \n  theme_bw()\n\n\n\n\nEasy to see marginal of species, i.e., \\(P(\\) x \\()\\)\nCan see conditional of island | species, i.e., \\(P(\\) fill | x \\()\\)\nHarder to see conditional of species | island, i.e., \\(P(\\) x | fill \\()\\)"
  },
  {
    "objectID": "lectures/05-2dcat.html#side-by-side-bar-charts",
    "href": "lectures/05-2dcat.html#side-by-side-bar-charts",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Side-by-side bar charts",
    "text": "Side-by-side bar charts\n\npenguins |&gt;\n  ggplot(aes(x = species, fill = island)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw()\n\n\n\n\nEasy to see conditional of island | species, i.e., \\(P(\\) fill | x \\()\\)\nCan see conditional of species | island, i.e., \\(P(\\) x | fill \\()\\)"
  },
  {
    "objectID": "lectures/05-2dcat.html#side-by-side-bar-charts-1",
    "href": "lectures/05-2dcat.html#side-by-side-bar-charts-1",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Side-by-side bar charts",
    "text": "Side-by-side bar charts\n\npenguins |&gt;\n  ggplot(aes(x = species, fill = island)) + \n  geom_bar(position = position_dodge(preserve = \"single\")) +\n  theme_bw()\n\n\n\n\nEasy to see conditional of island | species, i.e., \\(P(\\) fill | x \\()\\)\nCan see conditional of species | island, i.e., \\(P(\\) x | fill \\()\\)"
  },
  {
    "objectID": "lectures/05-2dcat.html#complete-missing-values-to-preserve-location",
    "href": "lectures/05-2dcat.html#complete-missing-values-to-preserve-location",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Complete missing values to preserve location",
    "text": "Complete missing values to preserve location\n\npenguins |&gt;\n  count(species, island) |&gt;\n  complete(species = unique(species), island = unique(island), \n           fill = list(n = 0)) |&gt;\n  ggplot(aes(x = species, y = n, fill = island)) + \n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/05-2dcat.html#what-do-you-prefer",
    "href": "lectures/05-2dcat.html#what-do-you-prefer",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "What do you prefer?",
    "text": "What do you prefer?"
  },
  {
    "objectID": "lectures/05-2dcat.html#chi-squared-test-for-1d-categorical-data",
    "href": "lectures/05-2dcat.html#chi-squared-test-for-1d-categorical-data",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Chi-squared test for 1D categorical data:",
    "text": "Chi-squared test for 1D categorical data:\n\n\nNull hypothesis \\(H_0\\): \\(p_1 = p_2 = \\dots = p_K\\), compute the test statistic:\n\n\\[\n\\chi^2 = \\sum_{j=1}^K \\frac{(O_j - E_j)^2}{E_j}\n\\]\n\n\\(O_j\\): observed counts in category \\(j\\)\n\\(E_j\\): expected counts under \\(H_0\\), i.e., each category is equally to occur \\(n / K = p_1 = p_2 = \\dots = p_K\\)\n\n\n\n\nchisq.test(table(penguins$species))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(penguins$species)\nX-squared = 31.907, df = 2, p-value = 1.179e-07"
  },
  {
    "objectID": "lectures/05-2dcat.html#inference-for-2d-categorical-data",
    "href": "lectures/05-2dcat.html#inference-for-2d-categorical-data",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Inference for 2D categorical data",
    "text": "Inference for 2D categorical data\n\nAgain we use the chi-squared test:\n\nNull hypothesis \\(H_0\\): variables \\(A\\) and \\(B\\) are independent, compute the test statistic:\n\n\\[\\chi^2 = \\sum_{i}^{K_A} \\sum_{j}^{K_B} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\n\n\\(O_{ij}\\): observed counts in contingency table\n\\(E_{ij}\\): expected counts under \\(H_0\\)\n\n\\[\n\\begin{aligned}\nE_{ij} &= n \\cdot P(A = a_i, B = b_j) \\\\\n&= n \\cdot P(A = a_i) P(B = b_j) \\\\\n&= n \\cdot \\left( \\frac{n_{i \\cdot}}{n} \\right) \\left( \\frac{ n_{\\cdot j}}{n} \\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/05-2dcat.html#inference-for-2d-categorical-data-1",
    "href": "lectures/05-2dcat.html#inference-for-2d-categorical-data-1",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Inference for 2D categorical data",
    "text": "Inference for 2D categorical data\n\nAgain we use the chi-squared test:\n\nNull hypothesis \\(H_0\\): variables \\(A\\) and \\(B\\) are independent, compute the test statistic:\n\n\\[\\chi^2 = \\sum_{i}^{K_A} \\sum_{j}^{K_B} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\n\n\\(O_{ij}\\): observed counts in contingency table\n\\(E_{ij}\\): expected counts under \\(H_0\\)\n\n\n\nchisq.test(table(penguins$species, penguins$island))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(penguins$species, penguins$island)\nX-squared = 299.55, df = 4, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/05-2dcat.html#visualize-independence-test-with-mosaic-plots",
    "href": "lectures/05-2dcat.html#visualize-independence-test-with-mosaic-plots",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Visualize independence test with mosaic plots",
    "text": "Visualize independence test with mosaic plots\n\nTwo variables are independent if knowing the level of one tells us nothing about the other\n\ni.e. \\(P(A | B) = P(A)\\), and that \\(P(A, B) = P(A) \\times P(B)\\)\n\nCreate a mosaic plot using base R\n\n\nmosaicplot(table(penguins$species, penguins$island))"
  },
  {
    "objectID": "lectures/05-2dcat.html#shade-by-pearson-residuals",
    "href": "lectures/05-2dcat.html#shade-by-pearson-residuals",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Shade by Pearson residuals",
    "text": "Shade by Pearson residuals\n\n\nThe test statistic is:\n\n\\[\\chi^2 = \\sum_{i}^{K_A} \\sum_{j}^{K_B} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\n\nDefine the Pearson residuals as:\n\n\\[r_{ij} = \\frac{O_{ij} - E_{ij}}{\\sqrt{E_{ij}}}\\]\n\nSide-note: In general, Pearson residuals are \\(\\frac{\\text{residuals}}{\\sqrt{\\text{variance}}}\\)\n\n\n\n\n\n\\(r_{ij} \\approx 0 \\rightarrow\\) observed counts are close to expected counts\n\\(|r_{ij}| &gt; 2 \\rightarrow\\) “significant” at level \\(\\alpha = 0.05\\).\nVery positive \\(r_{ij} \\rightarrow\\) more than expected, while very negative \\(r_{ij} \\rightarrow\\) fewer than expected\nColor by Pearson residuals to tell us which combos are much bigger/smaller than expected."
  },
  {
    "objectID": "lectures/05-2dcat.html#titanic-dataset-example",
    "href": "lectures/05-2dcat.html#titanic-dataset-example",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Titanic Dataset Example",
    "text": "Titanic Dataset Example\n\ntitanic &lt;- read_csv(\"https://raw.githubusercontent.com/ryurko/DataViz-Class-Data/main/titanic.csv\")\n\nQuestion: Does survival (yes/no) depend on cabin (1st/2nd/3rd)?\n\ntable(\"Survived?\" = titanic$Survived, \"Class\" = titanic$Pclass)\n\n         Class\nSurvived?   1   2   3\n        0  64  90 270\n        1 120  83  85\n\n\n\n\nchisq.test(table(\"Survived?\" = titanic$Survived, \"Class\" = titanic$Pclass))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(`Survived?` = titanic$Survived, Class = titanic$Pclass)\nX-squared = 91.081, df = 2, p-value &lt; 2.2e-16\n\n\nConclusion: Class and survival are dependent - but how?"
  },
  {
    "objectID": "lectures/05-2dcat.html#guardian-1000-songs-to-hear-before-you-die",
    "href": "lectures/05-2dcat.html#guardian-1000-songs-to-hear-before-you-die",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Guardian: 1000 songs to hear before you die",
    "text": "Guardian: 1000 songs to hear before you die"
  },
  {
    "objectID": "lectures/05-2dcat.html#recap-and-next-steps",
    "href": "lectures/05-2dcat.html#recap-and-next-steps",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nFor 2D categorical data we create visualizations for marginal, conditional, and joint distributions\nCan create stacked and side-by-side bar charts to visualize 2D categorical data\nPerform 2D Chi-squared test to test if two categorical variables are associated with each other\nCreate mosaic plots to visualize 2D categorical data\nShade mosaic plot tiles by Pearson residuals to see what drives association between two categorical variables (if any)\n\n\n\n\n\nHW1 is due TONIGHT and you have Lab 3 on Friday!\nNext time: Visualizing 1D quantitative data\nRecommended reading: CW Chapter 11 Visualizing nested proportions"
  },
  {
    "objectID": "lectures/02-1dcat.html#announcements-previously-and-today",
    "href": "lectures/02-1dcat.html#announcements-previously-and-today",
    "title": "1D Categorical Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nComplete HW0 by tonight! Confirms you have everything installed and can render .qmd files to PDF via tinytex\nOffice hours will be announced soon… (I’ll be in my office BH 132D today at 2:30 PM)\n\n\n\nDiscussed the importance of data visualization in your role as a statistician / data scientist\nIntroduced the Grammar of Graphics as a framework for building visualizations\nDiscussed historical examples and principles of visualization to keep in mind\n\n\n\nTODAY: 1D Categorical Data\n\nBriefly talk about variable types\nWalk through different graphs for visualizing 1D categorical data"
  },
  {
    "objectID": "lectures/02-1dcat.html#reminder-tidy-data-structure",
    "href": "lectures/02-1dcat.html#reminder-tidy-data-structure",
    "title": "1D Categorical Data",
    "section": "Reminder: tidy data structure",
    "text": "Reminder: tidy data structure\nData are often stored in tabular (or matrix) form:\n\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nEach row == unit of observation, e.g., penguins\nEach column == variable/measurement about each observation, e.g., flipper_length_mm\nKnown as a data.frame in base R and tibble in the tidyverse\nTwo main variable types: quantitative and categorical"
  },
  {
    "objectID": "lectures/02-1dcat.html#variable-types",
    "href": "lectures/02-1dcat.html#variable-types",
    "title": "1D Categorical Data",
    "section": "Variable Types",
    "text": "Variable Types\n\nMost visualizations are about understanding the distribution of different variables (which are stored in columns of tabular/matrix data)\nThe variable type often dictates the type of graphs you should make\nThere are two main types of variables:"
  },
  {
    "objectID": "lectures/02-1dcat.html#d-categorical-data",
    "href": "lectures/02-1dcat.html#d-categorical-data",
    "title": "1D Categorical Data",
    "section": "1D Categorical Data",
    "text": "1D Categorical Data\nTwo different versions of categorical:\n\nNominal: coded with arbitrary numbers, i.e., no real order\n\n\nExamples: race, gender, species, text\n\n\n\nOrdinal: levels with a meaningful order\n\n\nExamples: education level, grades, ranks\n\n\n\nNOTE: R and ggplot considers a categorical variable to be factor\n\nR will always treat categorical variables as ordinal! Defaults to alphabetical…\nWe will need to manually define the factor levels"
  },
  {
    "objectID": "lectures/02-1dcat.html#d-categorical-data-structure",
    "href": "lectures/02-1dcat.html#d-categorical-data-structure",
    "title": "1D Categorical Data",
    "section": "1D categorical data structure",
    "text": "1D categorical data structure\n\nObservations are collected into a vector \\((x_1, \\dots, x_n)\\), where \\(n\\) is number of observations\nEach observed value \\(x_i\\) can only belong to one category level \\(\\{ C_1, C_2, \\dots \\}\\)\n\n\nLook at penguins data from the palmerpenguins package, focusing on species:\n\nlibrary(palmerpenguins)\nhead(penguins$species)\n\n[1] Adelie Adelie Adelie Adelie Adelie Adelie\nLevels: Adelie Chinstrap Gentoo\n\n\nHow could we summarize these data? What information would you report?\n\n\n\ntable(penguins$species)\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124"
  },
  {
    "objectID": "lectures/02-1dcat.html#area-plots",
    "href": "lectures/02-1dcat.html#area-plots",
    "title": "1D Categorical Data",
    "section": "Area plots",
    "text": "Area plots\n\n\nEach area corresponds to one categorical level\nArea is proportional to counts/frequencies/percentages\nDifferences between areas correspond to differences between counts/frequencies/percentages"
  },
  {
    "objectID": "lectures/02-1dcat.html#bar-charts",
    "href": "lectures/02-1dcat.html#bar-charts",
    "title": "1D Categorical Data",
    "section": "Bar charts",
    "text": "Bar charts\n\nlibrary(tidyverse)\npenguins |&gt;\n  ggplot(aes(x = species)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/02-1dcat.html#behind-the-scenes-statistical-summaries",
    "href": "lectures/02-1dcat.html#behind-the-scenes-statistical-summaries",
    "title": "1D Categorical Data",
    "section": "Behind the scenes: statistical summaries",
    "text": "Behind the scenes: statistical summaries\n\nFrom Chapter 3 of R for Data Science"
  },
  {
    "objectID": "lectures/02-1dcat.html#spine-charts---height-version",
    "href": "lectures/02-1dcat.html#spine-charts---height-version",
    "title": "1D Categorical Data",
    "section": "Spine charts - height version",
    "text": "Spine charts - height version\n\npenguins |&gt;\n  ggplot(aes(fill = species, x = \"\")) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/02-1dcat.html#spine-charts---width-version",
    "href": "lectures/02-1dcat.html#spine-charts---width-version",
    "title": "1D Categorical Data",
    "section": "Spine charts - width version",
    "text": "Spine charts - width version\n\npenguins |&gt;\n  ggplot(aes(fill = species, x = \"\")) +\n  geom_bar() +\n  coord_flip()"
  },
  {
    "objectID": "lectures/02-1dcat.html#so-you-want-to-make-pie-charts",
    "href": "lectures/02-1dcat.html#so-you-want-to-make-pie-charts",
    "title": "1D Categorical Data",
    "section": "So you want to make pie charts…",
    "text": "So you want to make pie charts…\n\npenguins |&gt; \n  ggplot(aes(fill = species, x = \"\")) + \n  geom_bar(aes(y = after_stat(count))) +\n  coord_polar(theta = \"y\") +\n  theme_void()"
  },
  {
    "objectID": "lectures/02-1dcat.html#friends-dont-let-friends-make-pie-charts",
    "href": "lectures/02-1dcat.html#friends-dont-let-friends-make-pie-charts",
    "title": "1D Categorical Data",
    "section": "Friends Don’t Let Friends Make Pie Charts",
    "text": "Friends Don’t Let Friends Make Pie Charts"
  },
  {
    "objectID": "lectures/02-1dcat.html#waffle-charts-are-cooler-anyway",
    "href": "lectures/02-1dcat.html#waffle-charts-are-cooler-anyway",
    "title": "1D Categorical Data",
    "section": "Waffle charts are cooler anyway…",
    "text": "Waffle charts are cooler anyway…\n\nlibrary(waffle)\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  ggplot(aes(fill = species, values = count)) +\n  geom_waffle(n_rows = 20, color = \"white\", flip = TRUE) +\n  coord_equal() +\n  theme_void()"
  },
  {
    "objectID": "lectures/02-1dcat.html#florence-nightingales-rose-diagram",
    "href": "lectures/02-1dcat.html#florence-nightingales-rose-diagram",
    "title": "1D Categorical Data",
    "section": "Florence Nightingale’s Rose Diagram",
    "text": "Florence Nightingale’s Rose Diagram"
  },
  {
    "objectID": "lectures/02-1dcat.html#rose-diagrams",
    "href": "lectures/02-1dcat.html#rose-diagrams",
    "title": "1D Categorical Data",
    "section": "Rose diagrams",
    "text": "Rose diagrams\n\npenguins |&gt; \n  ggplot(aes(x = species)) + \n  geom_bar(fill = \"darkblue\") +\n  coord_polar() +\n  scale_y_sqrt()"
  },
  {
    "objectID": "lectures/02-1dcat.html#recap-and-next-steps",
    "href": "lectures/02-1dcat.html#recap-and-next-steps",
    "title": "1D Categorical Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\n1D Categorical Data: look at counts, frequencies, percentages\nArea plots, where area \\(\\propto\\) counts/frequencies/percentages:\n\nBar charts (you should pretty much always just make a bar chart)\nSpine charts (will be more useful with more variables)\nPie charts (DON’T DO IT)\nRose diagrams (temporal or directional context can justify usage)\n\n\n\n\n\n\nComplete HW0 by TONIGHT! Confirms you have everything installed and can render .qmd files to PDF via tinytex\nHW1 is due in two weeks, no class on Monday\n\n\n\n\n\n\nNext time: quantify and display uncertainty for 1D categorical data\nRecommended reading:\n\nCW Chapter 10 Visualizing proportions, CW Chapter 16.2 Visualizing the uncertainty of point estimates, CW Chapter 11 Visualizing nested proportions"
  },
  {
    "objectID": "lectures/06-1dquant.html#announcements-previously-and-today",
    "href": "lectures/06-1dquant.html#announcements-previously-and-today",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW2 is due Wednesday by 11:59 PM\nYou have Lab 4 again on Friday\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Wednesdays @ 11 AM\n\n\n\n\n\nCan create stacked and side-by-side bar charts to visualize 2D categorical data\nPerform 2D Chi-squared test to test if two categorical variables are associated with each other\nCreate mosaic plots to visualize 2D categorical data, shade by Pearson residuals\n\n\n\n\n\nTODAY:\n\nHow do we visualize 1D quantitative data?\nFor this week, we’ll focus on visualization issues and move to inference next week"
  },
  {
    "objectID": "lectures/06-1dquant.html#d-quantitative-data",
    "href": "lectures/06-1dquant.html#d-quantitative-data",
    "title": "Visualizing 1D Quantitative Data",
    "section": "1D Quantitative Data",
    "text": "1D Quantitative Data\nObservations are collected into a vector \\((x_1, \\dots, x_n)\\), \\(x_i \\in \\mathbb{R}\\) (or \\(\\mathbb{R}^+\\), \\(\\mathbb{Z}\\))\nCommon summary statistics for 1D quantitative data:\n\n\nCenter: Mean, median, weighted mean, mode\n\nRelated to the first moment, i.e., \\(\\mathbb{E}[X]\\)\n\n\n\n\n\nSpread: Variance, range, min/max, quantiles, IQR\n\nRelated to the second moment, i.e., \\(\\mathbb{E}[X^2]\\)\n\n\n\n\n\nShape: symmetry, skew, kurtosis (“peakedness”)\n\nRelated to higher order moments, i.e., skewness is \\(\\mathbb{E}[X^3]\\), kurtosis is \\(\\mathbb{E}[X^4]\\)\n\n\nCompute various statistics with summary(), mean(), median(), quantile(), range(), sd(), var(), etc."
  },
  {
    "objectID": "lectures/06-1dquant.html#box-plots-visualize-summary-statistics",
    "href": "lectures/06-1dquant.html#box-plots-visualize-summary-statistics",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Box plots visualize summary statistics",
    "text": "Box plots visualize summary statistics\n\npenguins |&gt;\n  ggplot(aes(y = flipper_length_mm)) +\n  geom_boxplot(aes(x = \"\")) +\n  coord_flip()"
  },
  {
    "objectID": "lectures/06-1dquant.html#histograms-display-1d-continuous-distributions",
    "href": "lectures/06-1dquant.html#histograms-display-1d-continuous-distributions",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Histograms display 1D continuous distributions",
    "text": "Histograms display 1D continuous distributions\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) +\n  geom_histogram()"
  },
  {
    "objectID": "lectures/06-1dquant.html#do-not-rely-on-box-plots",
    "href": "lectures/06-1dquant.html#do-not-rely-on-box-plots",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Do NOT rely on box plots…",
    "text": "Do NOT rely on box plots…"
  },
  {
    "objectID": "lectures/06-1dquant.html#simulate-from-mixture-of-normal-distributions",
    "href": "lectures/06-1dquant.html#simulate-from-mixture-of-normal-distributions",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Simulate from mixture of Normal distributions",
    "text": "Simulate from mixture of Normal distributions\nWill sample 100 draws from \\(N(-1.5, 1)\\) and 100 draws from \\(N(1.5, 1)\\)"
  },
  {
    "objectID": "lectures/06-1dquant.html#can-we-trust-the-default",
    "href": "lectures/06-1dquant.html#can-we-trust-the-default",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Can we trust the default?",
    "text": "Can we trust the default?\n\nset.seed(2025)\nfake_data &lt;- \n  tibble(fake_x = c(rnorm(100, -1.5), rnorm(100, 1.5))) |&gt;\n  mutate(component = c(rep(\"left\", 100), rep(\"right\", 100)))\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram() +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins",
    "href": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What happens as we change the number of bins?",
    "text": "What happens as we change the number of bins?\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(bins = 15) +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-1",
    "href": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-1",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What happens as we change the number of bins?",
    "text": "What happens as we change the number of bins?\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(bins = 60) +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-2",
    "href": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-2",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What happens as we change the number of bins?",
    "text": "What happens as we change the number of bins?\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(bins = 5) +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-3",
    "href": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-3",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What happens as we change the number of bins?",
    "text": "What happens as we change the number of bins?\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(bins = 100) +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#variability-of-graphs---30-bins",
    "href": "lectures/06-1dquant.html#variability-of-graphs---30-bins",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Variability of graphs - 30 bins",
    "text": "Variability of graphs - 30 bins\n\nset.seed(2025)\nfake_data &lt;- \n  tibble(fake_x = c(rnorm(100, -1.5), rnorm(100, 1.5))) |&gt;\n  mutate(component = c(rep(\"left\", 100), rep(\"right\", 100)))\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram() +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-happens-with-a-different-sample",
    "href": "lectures/06-1dquant.html#what-happens-with-a-different-sample",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What happens with a different sample?",
    "text": "What happens with a different sample?\n\nset.seed(1985)\nfake_data2 &lt;- \n  tibble(fake_x = c(rnorm(100, -1.5), rnorm(100, 1.5))) |&gt;\n  mutate(component = c(rep(\"left\", 100), rep(\"right\", 100)))\n\nfake_data2 |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram() +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#variability-of-graphs---15-bins",
    "href": "lectures/06-1dquant.html#variability-of-graphs---15-bins",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Variability of graphs - 15 bins",
    "text": "Variability of graphs - 15 bins"
  },
  {
    "objectID": "lectures/06-1dquant.html#variability-of-graphs---a-few-bins",
    "href": "lectures/06-1dquant.html#variability-of-graphs---a-few-bins",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Variability of graphs - a few bins",
    "text": "Variability of graphs - a few bins"
  },
  {
    "objectID": "lectures/06-1dquant.html#variability-of-graphs---too-many-bins",
    "href": "lectures/06-1dquant.html#variability-of-graphs---too-many-bins",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Variability of graphs - too many bins",
    "text": "Variability of graphs - too many bins"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-about-displaying-conditional-distributions",
    "href": "lectures/06-1dquant.html#what-about-displaying-conditional-distributions",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What about displaying conditional distributions?",
    "text": "What about displaying conditional distributions?\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_histogram(aes(fill = species))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-about-displaying-conditional-distributions-1",
    "href": "lectures/06-1dquant.html#what-about-displaying-conditional-distributions-1",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What about displaying conditional distributions?",
    "text": "What about displaying conditional distributions?\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_histogram(aes(fill = species),\n                 position = \"identity\", alpha = 0.3)"
  },
  {
    "objectID": "lectures/06-1dquant.html#normalize-histogram-frequencies-with-density",
    "href": "lectures/06-1dquant.html#normalize-histogram-frequencies-with-density",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Normalize histogram frequencies with density",
    "text": "Normalize histogram frequencies with density\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_histogram(aes(y = after_stat(density), fill = species),\n                 position = \"identity\", alpha = 0.3)"
  },
  {
    "objectID": "lectures/06-1dquant.html#can-use-density-curves-instead",
    "href": "lectures/06-1dquant.html#can-use-density-curves-instead",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Can use density curves instead",
    "text": "Can use density curves instead\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(aes(color = species))"
  },
  {
    "objectID": "lectures/06-1dquant.html#we-should-not-fill-the-density-curves",
    "href": "lectures/06-1dquant.html#we-should-not-fill-the-density-curves",
    "title": "Visualizing 1D Quantitative Data",
    "section": "We should NOT fill the density curves",
    "text": "We should NOT fill the density curves\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(aes(fill = species), alpha = .3)"
  },
  {
    "objectID": "lectures/06-1dquant.html#recap-and-next-steps",
    "href": "lectures/06-1dquant.html#recap-and-next-steps",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nVisualize 1D quantitative data to inspect center, spread, and shape\nBoxplots are only a display of summary statistics (i.e., they suck)\nHistograms display shape of the distribution, but comes with tradeoffs\nDensity curves provide an easy way to visualize conditional distributions\n\n\n\n\n\nHW2 is due Wednesday and you have Lab 4 on Friday\nNext time: Density estimation\nRecommended reading: CW Chapter 7 Visualizing distributions: Histograms and density plots"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#announcements-previously-and-today",
    "href": "lectures/08-1dquant-inference.html#announcements-previously-and-today",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW3 is due Wednesday by 11:59 PM and you have Lab 5 again on Friday!\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Tuesdays @ 11 AM\n\n\n\n\n\nSmoothed densities are a flexible tool for visualizing 1D distribution\nThere are two choices we need to make for kernel density estimation:\n\nBandwidth: Determines smoothness of distribution, usually data-driven choice\nKernel: Determines how much influence each observation should have on each other during estimation, usually context driven\n\nSeveral other types of density-based displays: violins, ridges, beeswarm plots\n\n\n\n\n\nTODAY:\n\nGraphical inference for 1D quantitative data\nParametric density estimates\nECDFs and Kolmogorov-Smirnov (KS) test"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-violin-plots",
    "href": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-violin-plots",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Visualizing conditional distributions: violin plots",
    "text": "Visualizing conditional distributions: violin plots\n\npenguins |&gt;\n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_violin() +\n  coord_flip()"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-violin-plots-1",
    "href": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-violin-plots-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Visualizing conditional distributions: violin plots",
    "text": "Visualizing conditional distributions: violin plots\n\npenguins |&gt;\n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_violin() + \n  geom_boxplot(width = .2) +\n  coord_flip()"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-ggridges",
    "href": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-ggridges",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Visualizing conditional distributions: ggridges",
    "text": "Visualizing conditional distributions: ggridges\n\nlibrary(ggridges)\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = species)) +\n  geom_density_ridges(rel_min_height = 0.01)"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#gallery-of-ggridges-examples",
    "href": "lectures/08-1dquant-inference.html#gallery-of-ggridges-examples",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Gallery of ggridges examples",
    "text": "Gallery of ggridges examples"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-ggbeeswarm",
    "href": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-ggbeeswarm",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Visualizing conditional distributions: ggbeeswarm",
    "text": "Visualizing conditional distributions: ggbeeswarm\n\nlibrary(ggbeeswarm)\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = species)) +\n  geom_beeswarm(cex = 1.5) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#kernel-density-estimation",
    "href": "lectures/08-1dquant-inference.html#kernel-density-estimation",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Kernel density estimation",
    "text": "Kernel density estimation\nGoal: estimate PDF \\(f(x)\\) for all possible values (assuming it is continuous & smooth)\n\\[\n\\text{Kernel density estimate: } \\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\n\\]\n\n\n\\(n =\\) sample size, \\(x =\\) new point to estimate \\(f(x)\\) (does NOT have to be in dataset!)\n\\(h =\\) bandwidth, analogous to histogram bin width, ensures \\(\\hat{f}(x)\\) integrates to 1\n\\(x_i =\\) \\(i\\)th observation in dataset\n\\(K_h(x - x_i)\\) is the Kernel function, creates weight given distance of \\(i\\)th observation from new point\n\nas \\(|x - x_i| \\rightarrow \\infty\\) then \\(K_h(x - x_i) \\rightarrow 0\\), i.e. further apart \\(i\\)th row is from \\(x\\), smaller the weight\nas bandwidth \\(h \\uparrow\\) weights are more evenly spread out (as \\(h \\downarrow\\) more concentrated around \\(x\\))\ntypically use Gaussian / Normal kernel: \\(\\propto e^{-(x - x_i)^2 / 2h^2}\\)\n\\(K_h(x - x_i)\\) is large when \\(x_i\\) is close to \\(x\\)"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#from-the-pdf-to-the-cdf",
    "href": "lectures/08-1dquant-inference.html#from-the-pdf-to-the-cdf",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "From the PDF to the CDF",
    "text": "From the PDF to the CDF\n\nProbability that continuous variable \\(X\\) takes a particular value is 0\ne.g., \\(P\\) (flipper_length_mm \\(= 200\\)) \\(= 0\\)\nInstead we use the probability density function (PDF) to provide a relative likelihood\n\n\n\nFor continuous variables we can use the cumulative distribution function (CDF),\n\\[\nF(x) = P(X \\leq x)\n\\]\n\n\n\n\nFor \\(n\\) observations we can easily compute the Empirical CDF (ECDF):\n\\[\\hat{F}_n(x)  = \\frac{\\text{# obs. with variable} \\leq x}{n} = \\frac{1}{n} \\sum_{i=1}^{n}1(x_i \\leq x)\\]\n\nwhere \\(1()\\) is the indicator function, i.e. ifelse(x_i &lt;= x, 1, 0)"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#display-full-distribution-with-ecdf-plot",
    "href": "lectures/08-1dquant-inference.html#display-full-distribution-with-ecdf-plot",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Display full distribution with ECDF plot",
    "text": "Display full distribution with ECDF plot\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  stat_ecdf() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#whats-the-relationship-between-these-two",
    "href": "lectures/08-1dquant-inference.html#whats-the-relationship-between-these-two",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "What’s the relationship between these two?",
    "text": "What’s the relationship between these two?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#comparing-to-theoretical-distributions",
    "href": "lectures/08-1dquant-inference.html#comparing-to-theoretical-distributions",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Comparing to theoretical distributions",
    "text": "Comparing to theoretical distributions"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#one-sample-kolmogorov-smirnov-test",
    "href": "lectures/08-1dquant-inference.html#one-sample-kolmogorov-smirnov-test",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "One-Sample Kolmogorov-Smirnov Test",
    "text": "One-Sample Kolmogorov-Smirnov Test\n\nWe compare the ECDF \\(\\hat{F}(x)\\) to a theoretical distribution’s CDF \\(F(x)\\)\nThe one sample KS test statistic is: \\(\\text{max}_x |\\hat{F}(x) - F(x)|\\)"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#parametric-density-estimation",
    "href": "lectures/08-1dquant-inference.html#parametric-density-estimation",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Parametric Density Estimation",
    "text": "Parametric Density Estimation\n\nInstead of trying to estimate the whole \\(f(x)\\) non-parametrically, we can assume a particular \\(f(x)\\) and estimate its parameters\nFor example, assume \\(X_i \\sim N(\\mu, \\sigma^2)\\). Then estimate the parameters:\n\n\\[\n\\hat{\\mu} = \\bar{x}, \\hspace{0.1in} \\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}\n\\]\n\nThen our density estimate is:\n\n\\[\n\\hat{f}(x) = \\frac{1}{\\sqrt{2\\pi} \\hat{\\sigma}} \\exp \\left( - \\frac{(x - \\hat{\\mu})^2}{2\\hat{\\sigma}^2} \\right)\n\\]"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#flipper-length-example",
    "href": "lectures/08-1dquant-inference.html#flipper-length-example",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Flipper length example",
    "text": "Flipper length example\nWhat if we assume flipper_length_mm follows Normal distribution?\n\ni.e., flipper_length_mm \\(\\sim N(\\mu, \\sigma^2)\\)\n\nNeed estimates for mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n\nflipper_length_mean &lt;- mean(penguins$flipper_length_mm, na.rm = TRUE)\nflipper_length_sd &lt;- sd(penguins$flipper_length_mm, na.rm = TRUE)\n\n\nPerform one-sample KS test using ks.test():\n\nks.test(x = penguins$flipper_length_mm, y = \"pnorm\",\n        mean = flipper_length_mean, sd = flipper_length_sd)\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  penguins$flipper_length_mm\nD = 0.12428, p-value = 5.163e-05\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#flipper-length-example-1",
    "href": "lectures/08-1dquant-inference.html#flipper-length-example-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Flipper length example",
    "text": "Flipper length example"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#statistical-tests-for-comparing-distributions",
    "href": "lectures/08-1dquant-inference.html#statistical-tests-for-comparing-distributions",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Statistical Tests for Comparing Distributions",
    "text": "Statistical Tests for Comparing Distributions\n\n\nWe’ve focused on assessing if a single quantitative variable follows a particular distribution\n\nLogic of one-sample KS test: Compare empirical distribution to theoretical distribution\n\n\n\n\n\nHow do we compare multiple empirical distributions?\nVery common scenario: Determine if a quantitative variable depends on a categorical variable, examples:\n\nClinical trials with multiple treatments\nAssessing differences across race, gender, socioeconomic status\nIndustrial experiments, A/B testing\nComparing song duration across different genres?\n\nCan use overlayed densities, side-by-side violin plots, facetted histograms\nRemember: plotting conditional distributions… but when are differences in a graphic statistically significant?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#tidytuesday-spotify-songs---duration-by-genre",
    "href": "lectures/08-1dquant-inference.html#tidytuesday-spotify-songs---duration-by-genre",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "TidyTuesday Spotify Songs - Duration by Genre",
    "text": "TidyTuesday Spotify Songs - Duration by Genre"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#tidytuesday-spotify-songs---duration-by-genre-1",
    "href": "lectures/08-1dquant-inference.html#tidytuesday-spotify-songs---duration-by-genre-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "TidyTuesday Spotify Songs - Duration by Genre",
    "text": "TidyTuesday Spotify Songs - Duration by Genre"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock",
    "href": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#kolmogorov-smirnov-test-again",
    "href": "lectures/08-1dquant-inference.html#kolmogorov-smirnov-test-again",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Kolmogorov-Smirnov Test… Again",
    "text": "Kolmogorov-Smirnov Test… Again\nKS test can also be used to compare two empirical distributions \\(\\hat{F}_A(x)\\) and \\(\\hat{F}_B\\), via test statistic for two samples:\n\\[\n\\text{max}_x |\\hat{F}_A(x) - \\hat{F}_B(x)|\n\\]\n\nspotify_songs &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')\nrap_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"rap\") |&gt; pull(duration_ms)\nrock_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"rock\") |&gt; pull(duration_ms)\n\nks.test(rap_duration, y = rock_duration)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  rap_duration and rock_duration\nD = 0.22386, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#kolmogorov-smirnov-test-again-1",
    "href": "lectures/08-1dquant-inference.html#kolmogorov-smirnov-test-again-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Kolmogorov-Smirnov Test… Again",
    "text": "Kolmogorov-Smirnov Test… Again\nKS test can also be used to compare two empirical distributions \\(\\hat{F}_A(x)\\) and \\(\\hat{F}_B\\), via test statistic for two samples:\n\\[\n\\text{max}_x |\\hat{F}_A(x) - \\hat{F}_B(x)|\n\\]"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#statistical-tests-for-comparing-distributions-1",
    "href": "lectures/08-1dquant-inference.html#statistical-tests-for-comparing-distributions-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Statistical Tests for Comparing Distributions",
    "text": "Statistical Tests for Comparing Distributions\n\nInfinite number of ways that you can compare multiple quantitative distributions, 3 common ways:\n\nAny difference at all? \nDifference in means?\n\n\nNull hypothesis: \\(H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_K\\) (use t.test or oneway.test() functions)\nCan assume the variances are all the same or differ\nIf reject, can only conclude not all means are equal\n\n\nDifference in variances?\n\n\nNull hypothesis: \\(H_0: \\sigma^2_1 = \\sigma^2_2 = \\cdots = \\sigma^2_K\\) (use bartlett.test() function)\nIf reject, can only conclude not all variances are equal\n\nUnlike the KS test, difference in means and variances are sensitive to non-Normality\n\nDifferent distributions can yield insignificant results"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock-1",
    "href": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock-2",
    "href": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock-2",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#test-difference-between-pop-and-rap",
    "href": "lectures/08-1dquant-inference.html#test-difference-between-pop-and-rap",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Test difference between pop and rap?",
    "text": "Test difference between pop and rap?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#recap-and-next-steps",
    "href": "lectures/08-1dquant-inference.html#recap-and-next-steps",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nIntroduced KS tests for testing differences in distributions\nBut when are the differences we’re seeing statistically significant?\n\nAny distributional difference? \\(\\rightarrow\\) KS test\nJust care about mean differences? \\(\\rightarrow\\) t-test\nJust care about variance differences? \\(\\rightarrow\\) Bartlett’s test\n\n\n\n\n\n\nHW3 is due Wednesday and you have Lab 5 on Friday\nNext time: Comparing Distributions and Statistical Power\nRecommended reading: CW Chapter 8 Visualizing distributions: Empirical cumulative distribution functions and q-q plots"
  },
  {
    "objectID": "lectures/15-distance-mds.html#announcements-previously-and-today",
    "href": "lectures/15-distance-mds.html#announcements-previously-and-today",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW5 is due Wednesday March 19th by 11:59 PM ET\nAs part of Homework 5, you’ll form groups for final projects\n\nTeams should be 3-4 students, you can pick your teammates or be randomized to a team\nGoal of the project: create and interpet hiqh-quality visualizations for a dataset of your choice\nProject requirements and rubrics are available on Canvas\nEDA reports are graded as as group, while presentations are graded individually\nHW5 is short so you have time to form teams and explore datasets\n\nYou have Lab 7 this Friday\n\n\n\nLast time: Contour plots, heat maps, and diving into high-dimensional data\nTODAY: How do we visualize structure of high-dimensional data?\n\nExample: What if I give you a dataset with 50 variables, and ask you to make one visualization that best represents the data? What do you do?\nDo NOT panic and make \\(\\binom{50}{2} = 1225\\) pairs of plots!\nIntuition: Take high-dimensional data and represent it in 2-3 dimensions, then visualize those dimensions"
  },
  {
    "objectID": "lectures/15-distance-mds.html#what-about-high-dimensional-data",
    "href": "lectures/15-distance-mds.html#what-about-high-dimensional-data",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "What about high-dimensional data?",
    "text": "What about high-dimensional data?\nConsider this dataset about Starbucks drinks:\n\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))\nstarbucks |&gt; slice(1)\n\n# A tibble: 1 × 15\n  product_name              size   milk  whip serv_size_m_l calories total_fat_g\n  &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 brewed coffee - dark roa… short     0     0           236        3         0.1\n# ℹ 8 more variables: saturated_fat_g &lt;dbl&gt;, trans_fat_g &lt;dbl&gt;,\n#   cholesterol_mg &lt;dbl&gt;, sodium_mg &lt;dbl&gt;, total_carbs_g &lt;dbl&gt;, fiber_g &lt;dbl&gt;,\n#   sugar_g &lt;dbl&gt;, caffeine_mg &lt;dbl&gt;\n\n\nToday: Visualize structure among observations using distances matrices"
  },
  {
    "objectID": "lectures/15-distance-mds.html#thinking-about-distance",
    "href": "lectures/15-distance-mds.html#thinking-about-distance",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Thinking about distance…",
    "text": "Thinking about distance…\n\nWhen describing visuals, we’ve implicitly “clustered” observations together\n\ne.g., where are the mode(s) in the data?\n\nThese types of task require characterizing the distance between observations\n\nClusters: groups of observations that are “close” together\n\n\n\n\nThis is easy to do for 2 quantitative variables: just make a scatterplot (possibly with contours or heatmap)\nBut how do we define “distance” for high-dimensional data?\nLet \\(\\boldsymbol{x}_i = (x_{i1}, \\dots, x_{ip})\\) be a vector of \\(p\\) features for observation \\(i\\)\nQuestion of interest: How “far away” is \\(\\boldsymbol{x}_i\\) from \\(\\boldsymbol{x}_j\\)?\n\n\n\n\nWhen looking at a scatterplot, you’re using Euclidean distance (length of the line in \\(p\\)-dimensional space):\n\\[d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\sqrt{(x_{i1} - x_{j1})^2 + \\dots + (x_{ip} - x_{jp})^2}\\]"
  },
  {
    "objectID": "lectures/15-distance-mds.html#distances-in-general",
    "href": "lectures/15-distance-mds.html#distances-in-general",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Distances in general",
    "text": "Distances in general\nThere’s a variety of different types of distance metrics: Manhattan, Mahalanobis, Cosine, Kullback-Leiber Divergence, Wasserstein, but we’re just going to focus on Euclidean distance\n\\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\) measures pairwise distance between two observations \\(i,j\\) and has the following properties:\n\nIdentity: \\(\\boldsymbol{x}_i = \\boldsymbol{x}_j \\iff d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = 0\\)\nNon-Negativity: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\geq 0\\)\nSymmetry: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = d(\\boldsymbol{x}_j, \\boldsymbol{x}_i)\\)\nTriangle Inequality: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\leq d(\\boldsymbol{x}_i, \\boldsymbol{x}_k) + d(\\boldsymbol{x}_k, \\boldsymbol{x}_j)\\)\n\n\nDistance Matrix: matrix \\(D\\) of all pairwise distances\n\n\\(D_{ij} = d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\)\nwhere \\(D_{ii} = 0\\) and \\(D_{ij} = D_{ji}\\)"
  },
  {
    "objectID": "lectures/15-distance-mds.html#what-could-go-wrong-with-euclidean-distance",
    "href": "lectures/15-distance-mds.html#what-could-go-wrong-with-euclidean-distance",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "What could go wrong with Euclidean distance?",
    "text": "What could go wrong with Euclidean distance?"
  },
  {
    "objectID": "lectures/15-distance-mds.html#multi-dimensional-scaling-mds",
    "href": "lectures/15-distance-mds.html#multi-dimensional-scaling-mds",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Multi-dimensional scaling (MDS)",
    "text": "Multi-dimensional scaling (MDS)\n\nGeneral approach for visualizing distance matrices\nPuts \\(n\\) observations in a \\(k\\)-dimensional space such that the distances are preserved as much as possible\n\nwhere \\(k &lt;&lt; p\\) typically choose \\(k = 2\\)\n\n\n\n\nMDS attempts to create new point \\(\\boldsymbol{y}_i = (y_{i1}, y_{i2})\\) for each unit such that:\n\\[\\sqrt{(y_{i1} - y_{j1})^2 + (y_{i2} - y_{j2})^2} \\approx D_{ij}\\]\n\ni.e., distance in 2D MDS world is approximately equal to the actual distance\n\n\n\n\n\nThen plot the new \\(\\boldsymbol{y}\\)s on a scatterplot\n\nUse the scale() function to ensure variables are comparable\nMake a distance matrix for this dataset\nVisualize it with MDS"
  },
  {
    "objectID": "lectures/15-distance-mds.html#mds-workflow-example-with-starbucks-drinks",
    "href": "lectures/15-distance-mds.html#mds-workflow-example-with-starbucks-drinks",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "MDS workflow example with Starbucks drinks",
    "text": "MDS workflow example with Starbucks drinks\n\nstarbucks_quant_data &lt;- starbucks |&gt; \n  dplyr::select(serv_size_m_l:caffeine_mg)\n\nstarbucks_scaled_quant_data &lt;- \n  scale(starbucks_quant_data, center = FALSE, \n        scale = apply(starbucks_quant_data, 2, sd, na.rm = TRUE))\n\ndist_euc &lt;- dist(starbucks_scaled_quant_data)\nstarbucks_mds &lt;- cmdscale(d = dist_euc, k = 2)\n\nstarbucks &lt;- starbucks |&gt; \n  mutate(mds1 = starbucks_mds[,1], mds2 = starbucks_mds[,2])\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2)) + \n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/15-distance-mds.html#mds-workflow-example-with-starbucks-drinks-output",
    "href": "lectures/15-distance-mds.html#mds-workflow-example-with-starbucks-drinks-output",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "MDS workflow example with Starbucks drinks",
    "text": "MDS workflow example with Starbucks drinks"
  },
  {
    "objectID": "lectures/15-distance-mds.html#what-does-dist-return",
    "href": "lectures/15-distance-mds.html#what-does-dist-return",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "What does dist() return?",
    "text": "What does dist() return?\n\ndist(starbucks_scaled_quant_data[1:8,])\n\n          1         2         3         4         5         6         7\n2 1.0597905                                                            \n3 2.1605009 1.1015883                                                  \n4 3.3885615 2.3316426 1.2323454                                        \n5 1.4722992 2.3802999 3.4258192 4.6439965                              \n6 1.5671249 2.2148502 3.1494033 4.3218904 0.6904406                    \n7 1.9247890 2.2591636 3.0086115 4.0906365 1.3835219 0.6941308          \n8 2.4275777 2.4999068 3.0232984 3.9688066 2.0714599 1.3824217 0.6883099\n\n\nDefault distance calculation is Euclidean"
  },
  {
    "objectID": "lectures/15-distance-mds.html#what-does-cmdscale-do",
    "href": "lectures/15-distance-mds.html#what-does-cmdscale-do",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "What does cmdscale do?",
    "text": "What does cmdscale do?\n\nstarbucks_mds &lt;- cmdscale(d = dist_euc, k = 2)\n\ncmdscale() is the function we use to run MDS and it has two inputs:\n\nd: distance matrix, e.g., dist_euc\nk: number of dimensions we want, e.g., usually 2 for visualization purposes\n\nInput is \\(N \\times N\\) matrix, and the output is \\(N \\times 2\\)\nTo grab the output, we just grab the two columns of starbucks_mds and then can make a scatterplot of these two new dimensions\n\nstarbucks &lt;- starbucks |&gt; \n  mutate(mds1 = starbucks_mds[,1],  mds2 = starbucks_mds[,2])"
  },
  {
    "objectID": "lectures/15-distance-mds.html#interpreting-the-2d-projection",
    "href": "lectures/15-distance-mds.html#interpreting-the-2d-projection",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Interpreting the 2D projection",
    "text": "Interpreting the 2D projection\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/15-distance-mds.html#view-structure-with-additional-variables---size",
    "href": "lectures/15-distance-mds.html#view-structure-with-additional-variables---size",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "View structure with additional variables - size",
    "text": "View structure with additional variables - size\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2, color = size)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/15-distance-mds.html#view-structure-with-additional-variables---sugar_g",
    "href": "lectures/15-distance-mds.html#view-structure-with-additional-variables---sugar_g",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "View structure with additional variables - sugar_g",
    "text": "View structure with additional variables - sugar_g\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2, color = sugar_g)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\") +\n  scale_color_gradient(low = \"darkblue\", high = \"darkorange\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/15-distance-mds.html#recap-and-next-steps",
    "href": "lectures/15-distance-mds.html#recap-and-next-steps",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nWhen data is high dimensional, it’s impossible to visualize every dimension in the data, so instead:\nWe reduce the data to a small number of dimensions, and then plot those dimensions\n\nCompute a distance matrix: reduces the data to a single distance between points\nRun Multi-Dimensional Scaling (MDS): summarizes the distance matrix in 2 or 3 dimensions\nPlot the dimensions provided by MDS\n\nAdding other dimensions (e.g., via color) when plotting MDS can be a great way to see structure (such as clusters) in the data\n\n\n\n\nHW5 is due Wednesday March 19th and you have lab this Friday!\nNext time: Dendrograms to visualize distances and clusters\nReview more code in lecture demos!"
  }
]
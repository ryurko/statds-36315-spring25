[
  {
    "objectID": "lectures/12-midsemester-review.html#announcements-previously-and-today",
    "href": "lectures/12-midsemester-review.html#announcements-previously-and-today",
    "title": "Midsemester Review",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nTake-home exam is Wednesday Feb 26th!\nHere’s how the exam will work:\n\nI’ll post the exam tonight, and it’s due Wednesday by 11:59 PM EDT (Feb 26th)\nExam will cover material from HW 1-4 and Labs 1-6\nQuestions will be similar to homeworks but more open-ended, e.g, instead of “make a side-by-side violin plot…” I’ll ask “Make a plot that compares the conditional distributions…”\nWe do NOT have class on Wednesday Feb 26th\n\n\n\n\nUse graphs to assess linear regression assumptions, i.e., residual diagnostic plots\nDiscussed interpretation of coefficients, the role of categorical variables, and interactions\n\n\n\n\nTODAY: Wrapping up regression and midsemester review!"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#understanding-interactions-categorical-example",
    "href": "lectures/12-midsemester-review.html#understanding-interactions-categorical-example",
    "title": "Midsemester Review",
    "section": "Understanding Interactions (Categorical Example)",
    "text": "Understanding Interactions (Categorical Example)\n\nSay we also have a quantitative variable \\(X\\) (bill length). Consider two statistical models:\n\n\\(Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_X X + \\beta_C I_C + \\beta_G I_G, \\sigma^2)\\)\n\\(Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_X X + \\beta_C I_C + \\beta_G I_G + \\beta_{CX} I_C X + \\beta_{GX} I_G X, \\sigma^2)\\)\n\n\n\n\nFor Model 1…\n\nThe intercept for Adelie is \\(\\beta_0\\); for Chinstrap it is \\(\\beta_0 + \\beta_C\\); for Gentoo it is \\(\\beta_0 + \\beta_G\\)\nThe slope for all species is \\(\\beta_X\\).\n\n\n\n\n\nFor Model 2…\n\nThe intercept for Adelie is \\(\\beta_0\\); for Chinstrap it is \\(\\beta_0 + \\beta_C\\); for Gentoo it is \\(\\beta_0 + \\beta_G\\).\nThe slope for Adelie is \\(\\beta_X\\); for Chinstrap it is \\(\\beta_X + \\beta_{CX}\\); for Gentoo it is \\(\\beta_X + \\beta_{GX}\\)\n\n\n\n\n\nSignificant coefficient for categorical variables by themselves? Significantly different intercepts\nSignificant coefficient for interactions with categorical variables? Significantly different slopes"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#model-2-bill_depth_mm-bill_length_mm-species",
    "href": "lectures/12-midsemester-review.html#model-2-bill_depth_mm-bill_length_mm-species",
    "title": "Midsemester Review",
    "section": "Model 2: bill_depth_mm ~ bill_length_mm + species",
    "text": "Model 2: bill_depth_mm ~ bill_length_mm + species"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#model-3-bill_depth_mm-bill_length_mm-species-bill_length_mm-times-species",
    "href": "lectures/12-midsemester-review.html#model-3-bill_depth_mm-bill_length_mm-species-bill_length_mm-times-species",
    "title": "Midsemester Review",
    "section": "Model 3: bill_depth_mm ~ bill_length_mm + species + bill_length_mm \\(\\times\\) species",
    "text": "Model 3: bill_depth_mm ~ bill_length_mm + species + bill_length_mm \\(\\times\\) species"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#a-few-linear-regression-warnings",
    "href": "lectures/12-midsemester-review.html#a-few-linear-regression-warnings",
    "title": "Midsemester Review",
    "section": "A Few Linear Regression Warnings",
    "text": "A Few Linear Regression Warnings\n\nSimpson’s Paradox\n\nThere is a negative linear relationship between two variables but a positive linear relationship within every subpopulation\nIn these cases, subgroup analysis is especially important\n\n\n\n\nIs the intercept meaningful?\n\nThink about whether \\(X = 0\\) makes scientific sense for a particular variable before you interpret the intercept\n\n\n\n\n\nInterpolation versus Extrapolation\n\nInterpolation is defined as prediction within the range of a variable\nExtrapolation is defined as prediction outside the range of a variable\nGenerally speaking, interpolation is more reliable than extrapolation (less sensitive to model misspecification)"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#extrapolation-example",
    "href": "lectures/12-midsemester-review.html#extrapolation-example",
    "title": "Midsemester Review",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#extrapolation-example-1",
    "href": "lectures/12-midsemester-review.html#extrapolation-example-1",
    "title": "Midsemester Review",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#extrapolation-example-2",
    "href": "lectures/12-midsemester-review.html#extrapolation-example-2",
    "title": "Midsemester Review",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#take-home-exam-logistics",
    "href": "lectures/12-midsemester-review.html#take-home-exam-logistics",
    "title": "Midsemester Review",
    "section": "Take-home exam logistics",
    "text": "Take-home exam logistics\nI will post it today, due Wednesday Feb 26th by 11:59 PM ET on Gradescope\nWhile the exam is in progress…\n\nYou can NOT talk to anyone else about 36-315\nYou can NOT post on Piazza\nYou can use any materials that are available to you from class (lectures, labs, homeworks, R demos)\n\nBest way to prepare:\n\nLook over lecture notes, R demos, homework/lab solutions"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#main-skill-ive-wanted-you-to-learn",
    "href": "lectures/12-midsemester-review.html#main-skill-ive-wanted-you-to-learn",
    "title": "Midsemester Review",
    "section": "Main skill I’ve wanted you to learn…",
    "text": "Main skill I’ve wanted you to learn…\n\nPick graph types that are most appropriate for a particular dataset\n\nRequires a working knowledge of different graph types and need to appropriately distinguish categorical vs quantitative variables\nFor any graph, need to know what information is visible vs hidden\n\n\n\n\nCharacterizing distributions (visually and quantitatively)\n\nNeed a “distributional vocabulary” (center/mode, spread, skewness) and need to choose graphs that showcase distributional quantities\nNeed to choose graph specifications that showcase distribution quantities (e.g., binwidth/bandwidth)\n\n\n\n\n\nConduct statistical inference to complement graphs\n\nFor most differences you spot in a graph, should be able to follow-up with an analysis to determine if that difference is significant\nRequires a working knowledge of different statistical tests\nNeed to know how to interpret the output from statistical tests (knowing the null/alternative hypotheses is key!)"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#variable-types",
    "href": "lectures/12-midsemester-review.html#variable-types",
    "title": "Midsemester Review",
    "section": "Variable Types",
    "text": "Variable Types\n\nFirst thing to do when looking at a dataset is determine what the variable types are.\nCategorical: May have order (ordinal) or no order (nominal).\n\nOften represented as a factor in R\nMay be coded with numbers!\nIf only 3-5 values, probably appropriate to treat as categorical.\n\nQuantitative: Represented numerically. Always has order.\n\nRepresented as numeric or integer in R.\n\nHow to determine if a variable is quantitative or categorical?\n\nOften obvious, but not always.\nSubtraction test: Does \\(X_1 - X_2\\) lead to a sensible value? If so, it’s quantitative.\nIf a variable is used in scatterplots/regression, it shouldn’t have a super strict range. 1-to-5 Likert scale variables fail this."
  },
  {
    "objectID": "lectures/12-midsemester-review.html#variable-type-situations",
    "href": "lectures/12-midsemester-review.html#variable-type-situations",
    "title": "Midsemester Review",
    "section": "Variable Type Situations",
    "text": "Variable Type Situations"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#variable-type-situations-1",
    "href": "lectures/12-midsemester-review.html#variable-type-situations-1",
    "title": "Midsemester Review",
    "section": "Variable Type Situations",
    "text": "Variable Type Situations"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#statistical-testsanalyses",
    "href": "lectures/12-midsemester-review.html#statistical-testsanalyses",
    "title": "Midsemester Review",
    "section": "Statistical Tests/Analyses",
    "text": "Statistical Tests/Analyses\n\nChi-square test for equal proportions: \\(H_0: p_1 = \\cdots = p_K\\).\nChi-square test for independence: \\(H_0:\\) Variables are independent.\n\nDependence: \\(P(A | B) \\neq P(A)\\)\n\n\n\n\nOne-sample KS test: \\(H_0\\): Variable follows a distribution.\nt-test/ANOVA: \\(H_0\\): Group means equal.\nBartlett’s test: \\(H_0\\): Group variances equal.\nTwo-Sample KS Test: \\(H_0\\): Variables follow the same distribution.\n\n\n\n\nLinear Regression: \\(H_0: \\beta = 0\\)\n\nNeed to distinguish between intercepts and slopes!\n\nRemember: Different tests have different power (chance of rejecting \\(H_0\\) when you should)"
  },
  {
    "objectID": "lectures/12-midsemester-review.html#distribution-terminology",
    "href": "lectures/12-midsemester-review.html#distribution-terminology",
    "title": "Midsemester Review",
    "section": "Distribution Terminology",
    "text": "Distribution Terminology\n\nMarginal Distributions: \\(P(A)\\) - plot a graph of a single variable \\(A\\).\n\nPerhaps compare confidence intervals for different categories of \\(A\\).\n\n\n\n\nConditional Distributions: \\(P(A | B)\\) - in English: Distribution of \\(A\\) given a particular value of \\(B\\).\n\nGoal: Compare \\(P(A | B = b)\\) for different \\(b\\) when \\(A\\) is quantitative and \\(B\\) categorical\nA univariate graph (histograms, densities, violins) for each category.\nWhen \\(A\\) and \\(B\\) are categorical, can visualize with stacked bar plots or mosaic plots.\nNote: Linear regression estimates \\(\\mathbb{E}[Y | X]\\)\n\n\n\n\n\nJoint Distribution: \\(P(A, B)\\)\n\nUse mosaic plots when \\(A\\) and \\(B\\) are categorical.\n\\(P(A | B) P(B) = P(A, B)\\)\nScatterplots display joint distribution for continuous."
  },
  {
    "objectID": "lectures/12-midsemester-review.html#good-luck",
    "href": "lectures/12-midsemester-review.html#good-luck",
    "title": "Midsemester Review",
    "section": "Good luck!",
    "text": "Good luck!"
  },
  {
    "objectID": "lectures/15-distance-mds.html#announcements-previously-and-today",
    "href": "lectures/15-distance-mds.html#announcements-previously-and-today",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW5 is due Wednesday March 19th by 11:59 PM ET\nAs part of Homework 5, you’ll form groups for final projects\n\nTeams should be 3-4 students, you can pick your teammates or be randomized to a team\nGoal of the project: create and interpet hiqh-quality visualizations for a dataset of your choice\nProject requirements and rubrics are available on Canvas\nEDA reports are graded as as group, while presentations are graded individually\nHW5 is short so you have time to form teams and explore datasets\n\nYou have Lab 7 this Friday\n\n\n\nLast time: Contour plots, heat maps, and diving into high-dimensional data\nTODAY: How do we visualize structure of high-dimensional data?\n\nExample: What if I give you a dataset with 50 variables, and ask you to make one visualization that best represents the data? What do you do?\nDo NOT panic and make \\(\\binom{50}{2} = 1225\\) pairs of plots!\nIntuition: Take high-dimensional data and represent it in 2-3 dimensions, then visualize those dimensions"
  },
  {
    "objectID": "lectures/15-distance-mds.html#what-about-high-dimensional-data",
    "href": "lectures/15-distance-mds.html#what-about-high-dimensional-data",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "What about high-dimensional data?",
    "text": "What about high-dimensional data?\nConsider this dataset about Starbucks drinks:\n\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))\nstarbucks |&gt; slice(1)\n\n# A tibble: 1 × 15\n  product_name              size   milk  whip serv_size_m_l calories total_fat_g\n  &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 brewed coffee - dark roa… short     0     0           236        3         0.1\n# ℹ 8 more variables: saturated_fat_g &lt;dbl&gt;, trans_fat_g &lt;dbl&gt;,\n#   cholesterol_mg &lt;dbl&gt;, sodium_mg &lt;dbl&gt;, total_carbs_g &lt;dbl&gt;, fiber_g &lt;dbl&gt;,\n#   sugar_g &lt;dbl&gt;, caffeine_mg &lt;dbl&gt;\n\n\nToday: Visualize structure among observations using distances matrices"
  },
  {
    "objectID": "lectures/15-distance-mds.html#thinking-about-distance",
    "href": "lectures/15-distance-mds.html#thinking-about-distance",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Thinking about distance…",
    "text": "Thinking about distance…\n\nWhen describing visuals, we’ve implicitly “clustered” observations together\n\ne.g., where are the mode(s) in the data?\n\nThese types of task require characterizing the distance between observations\n\nClusters: groups of observations that are “close” together\n\n\n\n\nThis is easy to do for 2 quantitative variables: just make a scatterplot (possibly with contours or heatmap)\nBut how do we define “distance” for high-dimensional data?\nLet \\(\\boldsymbol{x}_i = (x_{i1}, \\dots, x_{ip})\\) be a vector of \\(p\\) features for observation \\(i\\)\nQuestion of interest: How “far away” is \\(\\boldsymbol{x}_i\\) from \\(\\boldsymbol{x}_j\\)?\n\n\n\n\nWhen looking at a scatterplot, you’re using Euclidean distance (length of the line in \\(p\\)-dimensional space):\n\\[d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\sqrt{(x_{i1} - x_{j1})^2 + \\dots + (x_{ip} - x_{jp})^2}\\]"
  },
  {
    "objectID": "lectures/15-distance-mds.html#distances-in-general",
    "href": "lectures/15-distance-mds.html#distances-in-general",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Distances in general",
    "text": "Distances in general\nThere’s a variety of different types of distance metrics: Manhattan, Mahalanobis, Cosine, Kullback-Leiber Divergence, Wasserstein, but we’re just going to focus on Euclidean distance\n\\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\) measures pairwise distance between two observations \\(i,j\\) and has the following properties:\n\nIdentity: \\(\\boldsymbol{x}_i = \\boldsymbol{x}_j \\iff d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = 0\\)\nNon-Negativity: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\geq 0\\)\nSymmetry: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = d(\\boldsymbol{x}_j, \\boldsymbol{x}_i)\\)\nTriangle Inequality: \\(d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\leq d(\\boldsymbol{x}_i, \\boldsymbol{x}_k) + d(\\boldsymbol{x}_k, \\boldsymbol{x}_j)\\)\n\n\nDistance Matrix: matrix \\(D\\) of all pairwise distances\n\n\\(D_{ij} = d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\)\nwhere \\(D_{ii} = 0\\) and \\(D_{ij} = D_{ji}\\)"
  },
  {
    "objectID": "lectures/15-distance-mds.html#what-could-go-wrong-with-euclidean-distance",
    "href": "lectures/15-distance-mds.html#what-could-go-wrong-with-euclidean-distance",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "What could go wrong with Euclidean distance?",
    "text": "What could go wrong with Euclidean distance?"
  },
  {
    "objectID": "lectures/15-distance-mds.html#multi-dimensional-scaling-mds",
    "href": "lectures/15-distance-mds.html#multi-dimensional-scaling-mds",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Multi-dimensional scaling (MDS)",
    "text": "Multi-dimensional scaling (MDS)\n\nGeneral approach for visualizing distance matrices\nPuts \\(n\\) observations in a \\(k\\)-dimensional space such that the distances are preserved as much as possible\n\nwhere \\(k &lt;&lt; p\\) typically choose \\(k = 2\\)\n\n\n\n\nMDS attempts to create new point \\(\\boldsymbol{y}_i = (y_{i1}, y_{i2})\\) for each unit such that:\n\\[\\sqrt{(y_{i1} - y_{j1})^2 + (y_{i2} - y_{j2})^2} \\approx D_{ij}\\]\n\ni.e., distance in 2D MDS world is approximately equal to the actual distance\n\n\n\n\n\nThen plot the new \\(\\boldsymbol{y}\\)s on a scatterplot\n\nUse the scale() function to ensure variables are comparable\nMake a distance matrix for this dataset\nVisualize it with MDS"
  },
  {
    "objectID": "lectures/15-distance-mds.html#mds-workflow-example-with-starbucks-drinks",
    "href": "lectures/15-distance-mds.html#mds-workflow-example-with-starbucks-drinks",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "MDS workflow example with Starbucks drinks",
    "text": "MDS workflow example with Starbucks drinks\n\nstarbucks_quant_data &lt;- starbucks |&gt; \n  dplyr::select(serv_size_m_l:caffeine_mg)\n\nstarbucks_scaled_quant_data &lt;- \n  scale(starbucks_quant_data, center = FALSE, \n        scale = apply(starbucks_quant_data, 2, sd, na.rm = TRUE))\n\ndist_euc &lt;- dist(starbucks_scaled_quant_data)\nstarbucks_mds &lt;- cmdscale(d = dist_euc, k = 2)\n\nstarbucks &lt;- starbucks |&gt; \n  mutate(mds1 = starbucks_mds[,1], mds2 = starbucks_mds[,2])\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2)) + \n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/15-distance-mds.html#mds-workflow-example-with-starbucks-drinks-output",
    "href": "lectures/15-distance-mds.html#mds-workflow-example-with-starbucks-drinks-output",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "MDS workflow example with Starbucks drinks",
    "text": "MDS workflow example with Starbucks drinks"
  },
  {
    "objectID": "lectures/15-distance-mds.html#what-does-dist-return",
    "href": "lectures/15-distance-mds.html#what-does-dist-return",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "What does dist() return?",
    "text": "What does dist() return?\n\ndist(starbucks_scaled_quant_data[1:8,])\n\n          1         2         3         4         5         6         7\n2 1.0597905                                                            \n3 2.1605009 1.1015883                                                  \n4 3.3885615 2.3316426 1.2323454                                        \n5 1.4722992 2.3802999 3.4258192 4.6439965                              \n6 1.5671249 2.2148502 3.1494033 4.3218904 0.6904406                    \n7 1.9247890 2.2591636 3.0086115 4.0906365 1.3835219 0.6941308          \n8 2.4275777 2.4999068 3.0232984 3.9688066 2.0714599 1.3824217 0.6883099\n\n\nDefault distance calculation is Euclidean"
  },
  {
    "objectID": "lectures/15-distance-mds.html#what-does-cmdscale-do",
    "href": "lectures/15-distance-mds.html#what-does-cmdscale-do",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "What does cmdscale do?",
    "text": "What does cmdscale do?\n\nstarbucks_mds &lt;- cmdscale(d = dist_euc, k = 2)\n\ncmdscale() is the function we use to run MDS and it has two inputs:\n\nd: distance matrix, e.g., dist_euc\nk: number of dimensions we want, e.g., usually 2 for visualization purposes\n\nInput is \\(N \\times N\\) matrix, and the output is \\(N \\times 2\\)\nTo grab the output, we just grab the two columns of starbucks_mds and then can make a scatterplot of these two new dimensions\n\nstarbucks &lt;- starbucks |&gt; \n  mutate(mds1 = starbucks_mds[,1],  mds2 = starbucks_mds[,2])"
  },
  {
    "objectID": "lectures/15-distance-mds.html#interpreting-the-2d-projection",
    "href": "lectures/15-distance-mds.html#interpreting-the-2d-projection",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Interpreting the 2D projection",
    "text": "Interpreting the 2D projection\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/15-distance-mds.html#view-structure-with-additional-variables---size",
    "href": "lectures/15-distance-mds.html#view-structure-with-additional-variables---size",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "View structure with additional variables - size",
    "text": "View structure with additional variables - size\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2, color = size)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/15-distance-mds.html#view-structure-with-additional-variables---sugar_g",
    "href": "lectures/15-distance-mds.html#view-structure-with-additional-variables---sugar_g",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "View structure with additional variables - sugar_g",
    "text": "View structure with additional variables - sugar_g\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2, color = sugar_g)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\") +\n  scale_color_gradient(low = \"darkblue\", high = \"darkorange\") +\n  coord_fixed()"
  },
  {
    "objectID": "lectures/15-distance-mds.html#recap-and-next-steps",
    "href": "lectures/15-distance-mds.html#recap-and-next-steps",
    "title": "Visualizing Distances for High-Dimensional Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nWhen data is high dimensional, it’s impossible to visualize every dimension in the data, so instead:\nWe reduce the data to a small number of dimensions, and then plot those dimensions\n\nCompute a distance matrix: reduces the data to a single distance between points\nRun Multi-Dimensional Scaling (MDS): summarizes the distance matrix in 2 or 3 dimensions\nPlot the dimensions provided by MDS\n\nAdding other dimensions (e.g., via color) when plotting MDS can be a great way to see structure (such as clusters) in the data\n\n\n\n\nHW5 is due Wednesday March 19th and you have lab this Friday!\nNext time: Dendrograms to visualize distances and clusters\nReview more code in lecture demos!"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#announcements-previously-and-today",
    "href": "lectures/08-1dquant-inference.html#announcements-previously-and-today",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW3 is due Wednesday by 11:59 PM and you have Lab 5 again on Friday!\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Tuesdays @ 11 AM\n\n\n\n\n\nSmoothed densities are a flexible tool for visualizing 1D distribution\nThere are two choices we need to make for kernel density estimation:\n\nBandwidth: Determines smoothness of distribution, usually data-driven choice\nKernel: Determines how much influence each observation should have on each other during estimation, usually context driven\n\nSeveral other types of density-based displays: violins, ridges, beeswarm plots\n\n\n\n\n\nTODAY:\n\nGraphical inference for 1D quantitative data\nParametric density estimates\nECDFs and Kolmogorov-Smirnov (KS) test"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-violin-plots",
    "href": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-violin-plots",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Visualizing conditional distributions: violin plots",
    "text": "Visualizing conditional distributions: violin plots\n\npenguins |&gt;\n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_violin() +\n  coord_flip()"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-violin-plots-1",
    "href": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-violin-plots-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Visualizing conditional distributions: violin plots",
    "text": "Visualizing conditional distributions: violin plots\n\npenguins |&gt;\n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_violin() + \n  geom_boxplot(width = .2) +\n  coord_flip()"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-ggridges",
    "href": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-ggridges",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Visualizing conditional distributions: ggridges",
    "text": "Visualizing conditional distributions: ggridges\n\nlibrary(ggridges)\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = species)) +\n  geom_density_ridges(rel_min_height = 0.01)"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#gallery-of-ggridges-examples",
    "href": "lectures/08-1dquant-inference.html#gallery-of-ggridges-examples",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Gallery of ggridges examples",
    "text": "Gallery of ggridges examples"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-ggbeeswarm",
    "href": "lectures/08-1dquant-inference.html#visualizing-conditional-distributions-ggbeeswarm",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Visualizing conditional distributions: ggbeeswarm",
    "text": "Visualizing conditional distributions: ggbeeswarm\n\nlibrary(ggbeeswarm)\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = species)) +\n  geom_beeswarm(cex = 1.5) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#kernel-density-estimation",
    "href": "lectures/08-1dquant-inference.html#kernel-density-estimation",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Kernel density estimation",
    "text": "Kernel density estimation\nGoal: estimate PDF \\(f(x)\\) for all possible values (assuming it is continuous & smooth)\n\\[\n\\text{Kernel density estimate: } \\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\n\\]\n\n\n\\(n =\\) sample size, \\(x =\\) new point to estimate \\(f(x)\\) (does NOT have to be in dataset!)\n\\(h =\\) bandwidth, analogous to histogram bin width, ensures \\(\\hat{f}(x)\\) integrates to 1\n\\(x_i =\\) \\(i\\)th observation in dataset\n\\(K_h(x - x_i)\\) is the Kernel function, creates weight given distance of \\(i\\)th observation from new point\n\nas \\(|x - x_i| \\rightarrow \\infty\\) then \\(K_h(x - x_i) \\rightarrow 0\\), i.e. further apart \\(i\\)th row is from \\(x\\), smaller the weight\nas bandwidth \\(h \\uparrow\\) weights are more evenly spread out (as \\(h \\downarrow\\) more concentrated around \\(x\\))\ntypically use Gaussian / Normal kernel: \\(\\propto e^{-(x - x_i)^2 / 2h^2}\\)\n\\(K_h(x - x_i)\\) is large when \\(x_i\\) is close to \\(x\\)"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#from-the-pdf-to-the-cdf",
    "href": "lectures/08-1dquant-inference.html#from-the-pdf-to-the-cdf",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "From the PDF to the CDF",
    "text": "From the PDF to the CDF\n\nProbability that continuous variable \\(X\\) takes a particular value is 0\ne.g., \\(P\\) (flipper_length_mm \\(= 200\\)) \\(= 0\\)\nInstead we use the probability density function (PDF) to provide a relative likelihood\n\n\n\nFor continuous variables we can use the cumulative distribution function (CDF),\n\\[\nF(x) = P(X \\leq x)\n\\]\n\n\n\n\nFor \\(n\\) observations we can easily compute the Empirical CDF (ECDF):\n\\[\\hat{F}_n(x)  = \\frac{\\text{# obs. with variable} \\leq x}{n} = \\frac{1}{n} \\sum_{i=1}^{n}1(x_i \\leq x)\\]\n\nwhere \\(1()\\) is the indicator function, i.e. ifelse(x_i &lt;= x, 1, 0)"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#display-full-distribution-with-ecdf-plot",
    "href": "lectures/08-1dquant-inference.html#display-full-distribution-with-ecdf-plot",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Display full distribution with ECDF plot",
    "text": "Display full distribution with ECDF plot\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  stat_ecdf() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#whats-the-relationship-between-these-two",
    "href": "lectures/08-1dquant-inference.html#whats-the-relationship-between-these-two",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "What’s the relationship between these two?",
    "text": "What’s the relationship between these two?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#comparing-to-theoretical-distributions",
    "href": "lectures/08-1dquant-inference.html#comparing-to-theoretical-distributions",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Comparing to theoretical distributions",
    "text": "Comparing to theoretical distributions"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#one-sample-kolmogorov-smirnov-test",
    "href": "lectures/08-1dquant-inference.html#one-sample-kolmogorov-smirnov-test",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "One-Sample Kolmogorov-Smirnov Test",
    "text": "One-Sample Kolmogorov-Smirnov Test\n\nWe compare the ECDF \\(\\hat{F}(x)\\) to a theoretical distribution’s CDF \\(F(x)\\)\nThe one sample KS test statistic is: \\(\\text{max}_x |\\hat{F}(x) - F(x)|\\)"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#parametric-density-estimation",
    "href": "lectures/08-1dquant-inference.html#parametric-density-estimation",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Parametric Density Estimation",
    "text": "Parametric Density Estimation\n\nInstead of trying to estimate the whole \\(f(x)\\) non-parametrically, we can assume a particular \\(f(x)\\) and estimate its parameters\nFor example, assume \\(X_i \\sim N(\\mu, \\sigma^2)\\). Then estimate the parameters:\n\n\\[\n\\hat{\\mu} = \\bar{x}, \\hspace{0.1in} \\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}\n\\]\n\nThen our density estimate is:\n\n\\[\n\\hat{f}(x) = \\frac{1}{\\sqrt{2\\pi} \\hat{\\sigma}} \\exp \\left( - \\frac{(x - \\hat{\\mu})^2}{2\\hat{\\sigma}^2} \\right)\n\\]"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#flipper-length-example",
    "href": "lectures/08-1dquant-inference.html#flipper-length-example",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Flipper length example",
    "text": "Flipper length example\nWhat if we assume flipper_length_mm follows Normal distribution?\n\ni.e., flipper_length_mm \\(\\sim N(\\mu, \\sigma^2)\\)\n\nNeed estimates for mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n\nflipper_length_mean &lt;- mean(penguins$flipper_length_mm, na.rm = TRUE)\nflipper_length_sd &lt;- sd(penguins$flipper_length_mm, na.rm = TRUE)\n\n\nPerform one-sample KS test using ks.test():\n\nks.test(x = penguins$flipper_length_mm, y = \"pnorm\",\n        mean = flipper_length_mean, sd = flipper_length_sd)\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  penguins$flipper_length_mm\nD = 0.12428, p-value = 5.163e-05\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#flipper-length-example-1",
    "href": "lectures/08-1dquant-inference.html#flipper-length-example-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Flipper length example",
    "text": "Flipper length example"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#statistical-tests-for-comparing-distributions",
    "href": "lectures/08-1dquant-inference.html#statistical-tests-for-comparing-distributions",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Statistical Tests for Comparing Distributions",
    "text": "Statistical Tests for Comparing Distributions\n\n\nWe’ve focused on assessing if a single quantitative variable follows a particular distribution\n\nLogic of one-sample KS test: Compare empirical distribution to theoretical distribution\n\n\n\n\n\nHow do we compare multiple empirical distributions?\nVery common scenario: Determine if a quantitative variable depends on a categorical variable, examples:\n\nClinical trials with multiple treatments\nAssessing differences across race, gender, socioeconomic status\nIndustrial experiments, A/B testing\nComparing song duration across different genres?\n\nCan use overlayed densities, side-by-side violin plots, facetted histograms\nRemember: plotting conditional distributions… but when are differences in a graphic statistically significant?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#tidytuesday-spotify-songs---duration-by-genre",
    "href": "lectures/08-1dquant-inference.html#tidytuesday-spotify-songs---duration-by-genre",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "TidyTuesday Spotify Songs - Duration by Genre",
    "text": "TidyTuesday Spotify Songs - Duration by Genre"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#tidytuesday-spotify-songs---duration-by-genre-1",
    "href": "lectures/08-1dquant-inference.html#tidytuesday-spotify-songs---duration-by-genre-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "TidyTuesday Spotify Songs - Duration by Genre",
    "text": "TidyTuesday Spotify Songs - Duration by Genre"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock",
    "href": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#kolmogorov-smirnov-test-again",
    "href": "lectures/08-1dquant-inference.html#kolmogorov-smirnov-test-again",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Kolmogorov-Smirnov Test… Again",
    "text": "Kolmogorov-Smirnov Test… Again\nKS test can also be used to compare two empirical distributions \\(\\hat{F}_A(x)\\) and \\(\\hat{F}_B\\), via test statistic for two samples:\n\\[\n\\text{max}_x |\\hat{F}_A(x) - \\hat{F}_B(x)|\n\\]\n\nspotify_songs &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')\nrap_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"rap\") |&gt; pull(duration_ms)\nrock_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"rock\") |&gt; pull(duration_ms)\n\nks.test(rap_duration, y = rock_duration)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  rap_duration and rock_duration\nD = 0.22386, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#kolmogorov-smirnov-test-again-1",
    "href": "lectures/08-1dquant-inference.html#kolmogorov-smirnov-test-again-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Kolmogorov-Smirnov Test… Again",
    "text": "Kolmogorov-Smirnov Test… Again\nKS test can also be used to compare two empirical distributions \\(\\hat{F}_A(x)\\) and \\(\\hat{F}_B\\), via test statistic for two samples:\n\\[\n\\text{max}_x |\\hat{F}_A(x) - \\hat{F}_B(x)|\n\\]"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#statistical-tests-for-comparing-distributions-1",
    "href": "lectures/08-1dquant-inference.html#statistical-tests-for-comparing-distributions-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Statistical Tests for Comparing Distributions",
    "text": "Statistical Tests for Comparing Distributions\n\nInfinite number of ways that you can compare multiple quantitative distributions, 3 common ways:\n\nAny difference at all? \nDifference in means?\n\n\nNull hypothesis: \\(H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_K\\) (use t.test or oneway.test() functions)\nCan assume the variances are all the same or differ\nIf reject, can only conclude not all means are equal\n\n\nDifference in variances?\n\n\nNull hypothesis: \\(H_0: \\sigma^2_1 = \\sigma^2_2 = \\cdots = \\sigma^2_K\\) (use bartlett.test() function)\nIf reject, can only conclude not all variances are equal\n\nUnlike the KS test, difference in means and variances are sensitive to non-Normality\n\nDifferent distributions can yield insignificant results"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock-1",
    "href": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock-1",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock-2",
    "href": "lectures/08-1dquant-inference.html#test-difference-between-rap-and-rock-2",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#test-difference-between-pop-and-rap",
    "href": "lectures/08-1dquant-inference.html#test-difference-between-pop-and-rap",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Test difference between pop and rap?",
    "text": "Test difference between pop and rap?"
  },
  {
    "objectID": "lectures/08-1dquant-inference.html#recap-and-next-steps",
    "href": "lectures/08-1dquant-inference.html#recap-and-next-steps",
    "title": "Graphical Inference for 1D Quantitative Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nIntroduced KS tests for testing differences in distributions\nBut when are the differences we’re seeing statistically significant?\n\nAny distributional difference? \\(\\rightarrow\\) KS test\nJust care about mean differences? \\(\\rightarrow\\) t-test\nJust care about variance differences? \\(\\rightarrow\\) Bartlett’s test\n\n\n\n\n\n\nHW3 is due Wednesday and you have Lab 5 on Friday\nNext time: Comparing Distributions and Statistical Power\nRecommended reading: CW Chapter 8 Visualizing distributions: Empirical cumulative distribution functions and q-q plots"
  },
  {
    "objectID": "lectures/19-time-series.html#announcements-previously-and-today",
    "href": "lectures/19-time-series.html#announcements-previously-and-today",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW7 is due Wednesday March April 2nd by 11:59 PM ET\nGraphics Critique 2 is due TONIGHT!\nYou do NOT have lab this week - CARNIVAL!\n\n\n\nLast time:\n\nDiscussed various aspects of visualizing trends\nWhen visualizing many lines, often useful to consider highlighting a small subset\n\nTODAY: Time series, autocorrelation, and seasonal decomposition"
  },
  {
    "objectID": "lectures/19-time-series.html#things-of-interest-for-time-series-data",
    "href": "lectures/19-time-series.html#things-of-interest-for-time-series-data",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Things of interest for time series data",
    "text": "Things of interest for time series data\nTime series can be characterized by three features:\n\nTrends: Does the variable increase or decrease over time, on average?\nSeasonality: Are there changes in the variable that regularly happen (e.g., every winter, every hour, etc.)? Sometimes called periodicity.\nNoise: Variation in the variable beyond average trends and seasonality.\n\nMoving averages are a starting point for visualizing how a trend changes over time"
  },
  {
    "objectID": "lectures/19-time-series.html#be-responsible-with-your-axes",
    "href": "lectures/19-time-series.html#be-responsible-with-your-axes",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Be responsible with your axes!",
    "text": "Be responsible with your axes!"
  },
  {
    "objectID": "lectures/19-time-series.html#be-responsible-with-your-axes-1",
    "href": "lectures/19-time-series.html#be-responsible-with-your-axes-1",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Be responsible with your axes!",
    "text": "Be responsible with your axes!"
  },
  {
    "objectID": "lectures/19-time-series.html#moving-average-plots",
    "href": "lectures/19-time-series.html#moving-average-plots",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Moving Average Plots",
    "text": "Moving Average Plots\nThe Financial Times COVID-19 plots displayed a moving average (sometimes called a rolling average)\nIntuition\n\nDivide your data into small subsets (“windows”)\nCompute the average within each window\nConnect the averages together to make a trend line\n\n\nSometimes called a simple moving average\nThis is exactly what we did with LOESS… we called this a sliding window, but it’s the same thing"
  },
  {
    "objectID": "lectures/19-time-series.html#how-are-moving-averages-computed",
    "href": "lectures/19-time-series.html#how-are-moving-averages-computed",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "How are moving averages computed?",
    "text": "How are moving averages computed?\nIntuition\n\nDivide your data into small subsets (windows)\nCompute the average within each window\nConnect the averages together to make a trend line\n\n\nMathematically, a moving average can be written as the following:\n\\[\\mu_k = \\frac{\\sum_{t=k - h + 1}^k X_t}{h}\\]\n\nLarge \\(h\\): Smooth line; captures global trends\nSmall \\(h\\): Jagged/volatile line; captures local trends"
  },
  {
    "objectID": "lectures/19-time-series.html#working-with-time-series",
    "href": "lectures/19-time-series.html#working-with-time-series",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Working with Time Series",
    "text": "Working with Time Series\nco2: Mauna Loa Atmospheric CO2 Concentration dataset (monthly \\(\\text{CO}^2\\) concentration 1959 to 1997)\n\nco2_tbl |&gt;\n  ggplot(aes(x = obs_i, y = co2_val)) + \n  geom_line() + \n  labs(x = \"Time index\", y = \"CO2 (ppm)\")"
  },
  {
    "objectID": "lectures/19-time-series.html#formatting-dates",
    "href": "lectures/19-time-series.html#formatting-dates",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Formatting Dates",
    "text": "Formatting Dates\nCan use as.Date() to create time indexes.\n\nDefault format is Year/Month/Day. For something else, need to specify format in as.Date() (e.g., format = \"%m/%d/%Y\")"
  },
  {
    "objectID": "lectures/19-time-series.html#use-scale_x_date-to-create-interpretable-axis-labels",
    "href": "lectures/19-time-series.html#use-scale_x_date-to-create-interpretable-axis-labels",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Use scale_x_date() to create interpretable axis labels",
    "text": "Use scale_x_date() to create interpretable axis labels"
  },
  {
    "objectID": "lectures/19-time-series.html#other-moving-averages",
    "href": "lectures/19-time-series.html#other-moving-averages",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Other Moving Averages",
    "text": "Other Moving Averages\nTwo other common averages: Cumulative moving averages and weighted moving averages.\n\nCumulative moving average: The average at time \\(k\\) is the average of all points at and before \\(k\\). Mathematically:\n\n\\[\\mu_k^{(CMA)} = \\frac{\\sum_{t=1}^k X_t}{k}\\]\n\n\nWeighted moving average: Same as simple moving average, but different measurements get different weights for the average.\n\n\\[\\mu_k^{(WMA)} = \\frac{\\sum_{t=k - h + 1}^k X_t \\cdot w_t}{ \\sum_{t=k - h + 1}^k w_t}\\]"
  },
  {
    "objectID": "lectures/19-time-series.html#working-with-lags",
    "href": "lectures/19-time-series.html#working-with-lags",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Working with lags",
    "text": "Working with lags\nTime series data is fundamentally different from other data problems we’ve worked with because measurements are not independent\nObvious example: The temperature today is correlated with temperature yesterday. (Maybe not in Pittsburgh?)\n\nImportant term: lags. Used to determine if one time point influences future time points.\nLag 1: Comparing time series at time \\(t\\) with time series at time \\(t - 1\\).\nLag 2: Comparing time series at time \\(t\\) with time series at time \\(t - 2\\).\nAnd so on…\n\n\nLet’s say we have time measurements \\((X_1, X_2, X_3, X_4, X_5)\\).\nThe \\(\\ell = 1\\) lag is \\((X_2, X_3, X_4, X_5)\\) vs \\((X_1, X_2, X_3, X_4)\\).\n\n\nThe \\(\\ell = 2\\) lag is \\((X_3, X_4, X_5)\\) vs \\((X_1, X_2, X_3)\\).\nConsider: Are previous outcomes (lags) predictive of future outcomes?"
  },
  {
    "objectID": "lectures/19-time-series.html#autocorrelation",
    "href": "lectures/19-time-series.html#autocorrelation",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nAutocorrelation: Correlation between a time series and a lagged version of itself.\nDefine \\(r_{\\ell}\\) as the correlation between a time series and Lag \\(\\ell\\) of that time series.\n\nLag 1: \\(r_1\\) is correlation between \\((X_2, X_3, X_4, X_5)\\) and \\((X_1,X_2,X_3,X_4)\\)\nLag 2: \\(r_2\\) is correlation between \\((X_3, X_4, X_5)\\) and \\((X_1,X_2,X_3)\\)\nAnd so on…\n\n\nCommon diagnostic: Plot \\(\\ell\\) on x-axis, \\(r_{\\ell}\\) on y-axis.\nTells us if correlations are “significantly large” or “significantly small” for certain lags\nTo make an autocorrelation plot, we use the acf() function; the ggplot version uses autoplot()"
  },
  {
    "objectID": "lectures/19-time-series.html#autocorrelation-plots",
    "href": "lectures/19-time-series.html#autocorrelation-plots",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Autocorrelation plots",
    "text": "Autocorrelation plots\n\nlibrary(ggfortify)\nauto_corr &lt;- acf(co2_tbl$co2_val, plot = FALSE)\nautoplot(auto_corr)"
  },
  {
    "objectID": "lectures/19-time-series.html#autocorrelation-plots-and-seasonality",
    "href": "lectures/19-time-series.html#autocorrelation-plots-and-seasonality",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Autocorrelation Plots and Seasonality",
    "text": "Autocorrelation Plots and Seasonality\nWith strong global trends, autocorrelations will be very positive.\nHelpful: Visualize autocorrelations after removing the global trend (compute moving average with rollapply())"
  },
  {
    "objectID": "lectures/19-time-series.html#autocorrelation-plots-and-seasonality-1",
    "href": "lectures/19-time-series.html#autocorrelation-plots-and-seasonality-1",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Autocorrelation Plots and Seasonality",
    "text": "Autocorrelation Plots and Seasonality"
  },
  {
    "objectID": "lectures/19-time-series.html#seasonality-decomposition",
    "href": "lectures/19-time-series.html#seasonality-decomposition",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Seasonality Decomposition",
    "text": "Seasonality Decomposition\nRemember that there are three main components to a time series:\n\nAverage trends\nSeasonality\nNoise\n\n\nUse ggsdc() (from ggseas) to decompose a time series into these three components\n\nPlots the observed time series.\nPlots a loess curve as the global trend.\nPlots another loess curve on (observed - trend) as the seasonality.\nPlots the noise (observed - trend - seasonality)."
  },
  {
    "objectID": "lectures/19-time-series.html#seasonality-decomposition-1",
    "href": "lectures/19-time-series.html#seasonality-decomposition-1",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Seasonality Decomposition",
    "text": "Seasonality Decomposition\n\nco2_tbl |&gt;\n  ggsdc(aes(obs_date, co2_val), frequency = 12, method = \"stl\", s.window = 12) +\n  geom_line() + labs(x = \"Year\", y = \"CO2 (ppm)\")"
  },
  {
    "objectID": "lectures/19-time-series.html#recap-and-next-steps",
    "href": "lectures/19-time-series.html#recap-and-next-steps",
    "title": "Time Series, Autocorrelation, and Seasonal Decomposition",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nFundamental characteristic of time series data: measurements are dependent over time\nThe things to look out for in time series data are: (1) Average trends, (2) Seasonality, (3) Noise\nAutocorrelation plots are also useful for assessing average trends and seasonality.\n\n\n\nHW7 is due Wednesday!\nGraphics Critique 2 is due TONIGHT!\n\n\n\n\nNext time: Animations, infographics, and annotations\nRecommended reading: CW CH 13 Visualizing time series and other functions of an independent variable, CW CH 14 Visualizing trends"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#announcements-previously-and-today",
    "href": "lectures/14-contours-heatmaps.html#announcements-previously-and-today",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW5 is due next Wednesday March 19th by 11:59 PM ET\nAs part of Homework 5, you’ll form groups for final projects\n\nTeams should be 3-4 students, you can pick your teammates or be randomized to a team\nGoal of the project: create and interpet hiqh-quality visualizations for a dataset of your choice\nProject requirements and rubrics are available on Canvas\nEDA reports are graded as as group, while presentations are graded individually\nHW5 is short so you have time to form teams and explore datasets\n\nYou do NOT have lab this week\n\n\n\nLast time:\n\nLOESS: bunch of little linear regressions glued together\nPairs plots: convenient wrapper to creating several visualizations at once\n\nTODAY: Contour Plots and Heat Maps"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#d-quantitative-data",
    "href": "lectures/14-contours-heatmaps.html#d-quantitative-data",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "2D quantitative data",
    "text": "2D quantitative data\n\nWe’re working with two variables: \\((X, Y) \\in \\mathbb{R}^2\\), i.e., dataset with \\(n\\) rows and 2 columns\nGoals:\n\ndescribing the relationships between two variables\ndescribing the conditional distribution \\(Y | X\\) via regression analysis\nTODAY: describing the joint distribution \\(X,Y\\) via contours, heatmaps, etc.\n\nFew big picture ideas to keep in mind:\n\nscatterplots are by far the most common visual\nregression analysis is by far the most popular analysis (you have a whole class on this…)\nrelationships may vary across other variables, e.g., categorical variables"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#what-about-focusing-on-the-joint-distribution",
    "href": "lectures/14-contours-heatmaps.html#what-about-focusing-on-the-joint-distribution",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "What about focusing on the joint distribution?",
    "text": "What about focusing on the joint distribution?\nExample dataset of pitches thrown by baseball superstar Shohei Ohtani\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  geom_point(alpha = 0.2) +\n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#going-from-1d-to-2d-density-estimation",
    "href": "lectures/14-contours-heatmaps.html#going-from-1d-to-2d-density-estimation",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Going from 1D to 2D density estimation",
    "text": "Going from 1D to 2D density estimation\nIn 1D: estimate density \\(f(x)\\), assuming that \\(f(x)\\) is smooth:\n\\[\n\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\n\\]\n\nIn 2D: estimate joint density \\(f(x_1, x_2)\\)\n\\[\\hat{f}(x_1, x_2) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h_1h_2} K(\\frac{x_1 - x_{i1}}{h_1}) K(\\frac{x_2 - x_{i2}}{h_2})\\]\n\n\nIn 1D there was one bandwidth, now we have two bandwidths\n\n\\(h_1\\): controls smoothness as \\(X_1\\) changes, holding \\(X_2\\) fixed\n\\(h_2\\): controls smoothness as \\(X_2\\) changes, holding \\(X_1\\) fixed\n\nAgain Gaussian kernels are the most popular…"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#so-how-do-we-display-densities-for-2d-data",
    "href": "lectures/14-contours-heatmaps.html#so-how-do-we-display-densities-for-2d-data",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "So how do we display densities for 2D data?",
    "text": "So how do we display densities for 2D data?"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#how-to-read-contour-plots",
    "href": "lectures/14-contours-heatmaps.html#how-to-read-contour-plots",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "How to read contour plots?",
    "text": "How to read contour plots?\nBest known in topology: outlines (contours) denote levels of elevation"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#display-2d-contour-plot",
    "href": "lectures/14-contours-heatmaps.html#display-2d-contour-plot",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Display 2D contour plot",
    "text": "Display 2D contour plot\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  geom_point(alpha = 0.2) +\n  geom_density2d() +\n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-1",
    "href": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-1",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Display 2D contour plot",
    "text": "Display 2D contour plot\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  geom_density2d() +\n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-2",
    "href": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-2",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Display 2D contour plot",
    "text": "Display 2D contour plot\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  stat_density2d(aes(fill = after_stat(level)), geom = \"polygon\") +\n  coord_fixed() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#visualizing-grid-heat-maps",
    "href": "lectures/14-contours-heatmaps.html#visualizing-grid-heat-maps",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Visualizing grid heat maps",
    "text": "Visualizing grid heat maps\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  stat_density2d(aes(fill = after_stat(density)), \n                 geom = \"tile\", contour = FALSE) + \n  coord_fixed() +\n  scale_fill_gradient(low = \"white\", high = \"red\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#alternative-idea-hexagonal-binning",
    "href": "lectures/14-contours-heatmaps.html#alternative-idea-hexagonal-binning",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Alternative idea: hexagonal binning",
    "text": "Alternative idea: hexagonal binning\n\nohtani_pitches |&gt;\n  ggplot(aes(x = plate_x, y = plate_z)) +\n  geom_hex() +\n  coord_fixed() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#lebron-james-shots-from-hoopr",
    "href": "lectures/14-contours-heatmaps.html#lebron-james-shots-from-hoopr",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "LeBron James’ shots from hoopR",
    "text": "LeBron James’ shots from hoopR\n\nlebron_shots &lt;- read_csv(\"https://raw.githubusercontent.com/ryurko/DataViz-Class-Data/main/lebron_shots.csv\")\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = 0.4) +\n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-3",
    "href": "lectures/14-contours-heatmaps.html#display-2d-contour-plot-3",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Display 2D contour plot",
    "text": "Display 2D contour plot\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = 0.4) +\n  geom_density2d(binwidth = 0.0001) + \n  coord_fixed() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#alternative-idea-hexagonal-binning-1",
    "href": "lectures/14-contours-heatmaps.html#alternative-idea-hexagonal-binning-1",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Alternative idea: hexagonal binning",
    "text": "Alternative idea: hexagonal binning\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_hex() +\n  coord_fixed() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#what-about-high-dimensional-data",
    "href": "lectures/14-contours-heatmaps.html#what-about-high-dimensional-data",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "What about high-dimensional data?",
    "text": "What about high-dimensional data?\nConsider this dataset containing nutritional information about Starbucks drinks:\n\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))\nstarbucks |&gt; slice(1)\n\n# A tibble: 1 × 15\n  product_name              size   milk  whip serv_size_m_l calories total_fat_g\n  &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 brewed coffee - dark roa… short     0     0           236        3         0.1\n# ℹ 8 more variables: saturated_fat_g &lt;dbl&gt;, trans_fat_g &lt;dbl&gt;,\n#   cholesterol_mg &lt;dbl&gt;, sodium_mg &lt;dbl&gt;, total_carbs_g &lt;dbl&gt;, fiber_g &lt;dbl&gt;,\n#   sugar_g &lt;dbl&gt;, caffeine_mg &lt;dbl&gt;\n\n\nHow do we visualize this dataset? \n\nTedious task: make a series of pairs plots (one giant pairs plot would overwhelming)"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#what-about-high-dimensional-data-1",
    "href": "lectures/14-contours-heatmaps.html#what-about-high-dimensional-data-1",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "What about high-dimensional data?",
    "text": "What about high-dimensional data?\n\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))\nstarbucks |&gt; slice(1)\n\n# A tibble: 1 × 15\n  product_name              size   milk  whip serv_size_m_l calories total_fat_g\n  &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 brewed coffee - dark roa… short     0     0           236        3         0.1\n# ℹ 8 more variables: saturated_fat_g &lt;dbl&gt;, trans_fat_g &lt;dbl&gt;,\n#   cholesterol_mg &lt;dbl&gt;, sodium_mg &lt;dbl&gt;, total_carbs_g &lt;dbl&gt;, fiber_g &lt;dbl&gt;,\n#   sugar_g &lt;dbl&gt;, caffeine_mg &lt;dbl&gt;\n\n\nGoals to keep in mind with visualizing high-dimensional data:\n\nVisualize structure among observations based on distances and projections (next lecture)\nVisualize structure among variables using correlation as “distance”"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#correlogram-to-visualize-correlation-matrix",
    "href": "lectures/14-contours-heatmaps.html#correlogram-to-visualize-correlation-matrix",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Correlogram to visualize correlation matrix",
    "text": "Correlogram to visualize correlation matrix\nUse the ggcorrplot package:\n\nstarbucks_quant_cor &lt;- cor(dplyr::select(starbucks, serv_size_m_l:caffeine_mg))\n\nlibrary(ggcorrplot)\nggcorrplot(starbucks_quant_cor)"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#options-to-customize-correlogram",
    "href": "lectures/14-contours-heatmaps.html#options-to-customize-correlogram",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Options to customize correlogram",
    "text": "Options to customize correlogram\n\nggcorrplot(starbucks_quant_cor,\n           type = \"lower\", method = \"circle\")"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#reorder-variables-based-on-correlation",
    "href": "lectures/14-contours-heatmaps.html#reorder-variables-based-on-correlation",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Reorder variables based on correlation",
    "text": "Reorder variables based on correlation\n\nggcorrplot(starbucks_quant_cor,\n           type = \"lower\", method = \"circle\",\n           hc.order = TRUE)"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#heatmap-displays-of-observations",
    "href": "lectures/14-contours-heatmaps.html#heatmap-displays-of-observations",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Heatmap displays of observations",
    "text": "Heatmap displays of observations\n\nheatmap(as.matrix(dplyr::select(starbucks, serv_size_m_l:caffeine_mg)),\n        scale = \"column\", \n        labRow = starbucks$product_name,\n        cexRow = .5, cexCol = .75,\n        Rowv = NA, Colv = NA)"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps",
    "href": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Manual version of heatmaps",
    "text": "Manual version of heatmaps\n\nstarbucks |&gt;\n  dplyr::select(product_name, serv_size_m_l:caffeine_mg) |&gt;\n  pivot_longer(serv_size_m_l:caffeine_mg,\n               names_to = \"variable\",\n               values_to = \"raw_value\") |&gt;\n  group_by(variable) |&gt;\n  mutate(std_value = (raw_value - mean(raw_value)) / sd(raw_value)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(y = variable, x = product_name, fill = std_value)) +\n  geom_tile() +\n  theme_light() +\n  theme(axis.text.x = element_text(size = 1, angle = 45),\n        legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-output",
    "href": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-output",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Manual version of heatmaps",
    "text": "Manual version of heatmaps"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-1",
    "href": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-1",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Manual version of heatmaps",
    "text": "Manual version of heatmaps\n\nstarbucks |&gt;\n  dplyr::select(product_name, serv_size_m_l:caffeine_mg) |&gt;\n  mutate(product_name = fct_reorder(product_name, calories)) |&gt;\n  pivot_longer(serv_size_m_l:caffeine_mg,\n               names_to = \"variable\",\n               values_to = \"raw_value\") |&gt;\n  group_by(variable) |&gt;\n  mutate(std_value = (raw_value - mean(raw_value)) / sd(raw_value)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(y = variable, x = product_name, fill = std_value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") +\n  theme_light() +\n  theme(axis.text.x = element_text(size = 1, angle = 45),\n        legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-1-output",
    "href": "lectures/14-contours-heatmaps.html#manual-version-of-heatmaps-1-output",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Manual version of heatmaps",
    "text": "Manual version of heatmaps"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#parallel-coordinates-plot-with-ggparcoord",
    "href": "lectures/14-contours-heatmaps.html#parallel-coordinates-plot-with-ggparcoord",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Parallel coordinates plot with ggparcoord",
    "text": "Parallel coordinates plot with ggparcoord\n\nlibrary(GGally)\nstarbucks |&gt;\n  ggparcoord(columns = 5:15, alphaLines = .1) +\n  theme(axis.text.x = element_text(angle = 90))"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#easier-example-with-penguins",
    "href": "lectures/14-contours-heatmaps.html#easier-example-with-penguins",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Easier example with penguins…",
    "text": "Easier example with penguins…\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2, groupColumn = \"species\",\n             order = c(6, 5, 3, 4))"
  },
  {
    "objectID": "lectures/14-contours-heatmaps.html#recap-and-next-steps",
    "href": "lectures/14-contours-heatmaps.html#recap-and-next-steps",
    "title": "Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nWe can extend kernel density estimation from 1 to \\(p\\)-dimensions (don’t say easily though…)\nContour plots: Common way to visualize two-dimensional densities\nHeat maps: divide the space into a grid, and then color the grid according to high/low densities\nHexagonal bins: creating histograms in 2D\nCorrelograms and Parallel Coordinates Plots are helpful tools for visualizing high-dimensional data\n\n\n\n\nHW5 is due Wednesday March 19th and you do NOT have lab this Friday!\nNext time: Visualizing Distances and MDS"
  },
  {
    "objectID": "lectures/02-1dcat.html#announcements-previously-and-today",
    "href": "lectures/02-1dcat.html#announcements-previously-and-today",
    "title": "1D Categorical Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nComplete HW0 by tonight! Confirms you have everything installed and can render .qmd files to PDF via tinytex\nOffice hours will be announced soon… (I’ll be in my office BH 132D today at 2:30 PM)\n\n\n\nDiscussed the importance of data visualization in your role as a statistician / data scientist\nIntroduced the Grammar of Graphics as a framework for building visualizations\nDiscussed historical examples and principles of visualization to keep in mind\n\n\n\nTODAY: 1D Categorical Data\n\nBriefly talk about variable types\nWalk through different graphs for visualizing 1D categorical data"
  },
  {
    "objectID": "lectures/02-1dcat.html#reminder-tidy-data-structure",
    "href": "lectures/02-1dcat.html#reminder-tidy-data-structure",
    "title": "1D Categorical Data",
    "section": "Reminder: tidy data structure",
    "text": "Reminder: tidy data structure\nData are often stored in tabular (or matrix) form:\n\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nEach row == unit of observation, e.g., penguins\nEach column == variable/measurement about each observation, e.g., flipper_length_mm\nKnown as a data.frame in base R and tibble in the tidyverse\nTwo main variable types: quantitative and categorical"
  },
  {
    "objectID": "lectures/02-1dcat.html#variable-types",
    "href": "lectures/02-1dcat.html#variable-types",
    "title": "1D Categorical Data",
    "section": "Variable Types",
    "text": "Variable Types\n\nMost visualizations are about understanding the distribution of different variables (which are stored in columns of tabular/matrix data)\nThe variable type often dictates the type of graphs you should make\nThere are two main types of variables:"
  },
  {
    "objectID": "lectures/02-1dcat.html#d-categorical-data",
    "href": "lectures/02-1dcat.html#d-categorical-data",
    "title": "1D Categorical Data",
    "section": "1D Categorical Data",
    "text": "1D Categorical Data\nTwo different versions of categorical:\n\nNominal: coded with arbitrary numbers, i.e., no real order\n\n\nExamples: race, gender, species, text\n\n\n\nOrdinal: levels with a meaningful order\n\n\nExamples: education level, grades, ranks\n\n\n\nNOTE: R and ggplot considers a categorical variable to be factor\n\nR will always treat categorical variables as ordinal! Defaults to alphabetical…\nWe will need to manually define the factor levels"
  },
  {
    "objectID": "lectures/02-1dcat.html#d-categorical-data-structure",
    "href": "lectures/02-1dcat.html#d-categorical-data-structure",
    "title": "1D Categorical Data",
    "section": "1D categorical data structure",
    "text": "1D categorical data structure\n\nObservations are collected into a vector \\((x_1, \\dots, x_n)\\), where \\(n\\) is number of observations\nEach observed value \\(x_i\\) can only belong to one category level \\(\\{ C_1, C_2, \\dots \\}\\)\n\n\nLook at penguins data from the palmerpenguins package, focusing on species:\n\nlibrary(palmerpenguins)\nhead(penguins$species)\n\n[1] Adelie Adelie Adelie Adelie Adelie Adelie\nLevels: Adelie Chinstrap Gentoo\n\n\nHow could we summarize these data? What information would you report?\n\n\n\ntable(penguins$species)\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124"
  },
  {
    "objectID": "lectures/02-1dcat.html#area-plots",
    "href": "lectures/02-1dcat.html#area-plots",
    "title": "1D Categorical Data",
    "section": "Area plots",
    "text": "Area plots\n\n\nEach area corresponds to one categorical level\nArea is proportional to counts/frequencies/percentages\nDifferences between areas correspond to differences between counts/frequencies/percentages"
  },
  {
    "objectID": "lectures/02-1dcat.html#bar-charts",
    "href": "lectures/02-1dcat.html#bar-charts",
    "title": "1D Categorical Data",
    "section": "Bar charts",
    "text": "Bar charts\n\nlibrary(tidyverse)\npenguins |&gt;\n  ggplot(aes(x = species)) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/02-1dcat.html#behind-the-scenes-statistical-summaries",
    "href": "lectures/02-1dcat.html#behind-the-scenes-statistical-summaries",
    "title": "1D Categorical Data",
    "section": "Behind the scenes: statistical summaries",
    "text": "Behind the scenes: statistical summaries\n\nFrom Chapter 3 of R for Data Science"
  },
  {
    "objectID": "lectures/02-1dcat.html#spine-charts---height-version",
    "href": "lectures/02-1dcat.html#spine-charts---height-version",
    "title": "1D Categorical Data",
    "section": "Spine charts - height version",
    "text": "Spine charts - height version\n\npenguins |&gt;\n  ggplot(aes(fill = species, x = \"\")) +\n  geom_bar()"
  },
  {
    "objectID": "lectures/02-1dcat.html#spine-charts---width-version",
    "href": "lectures/02-1dcat.html#spine-charts---width-version",
    "title": "1D Categorical Data",
    "section": "Spine charts - width version",
    "text": "Spine charts - width version\n\npenguins |&gt;\n  ggplot(aes(fill = species, x = \"\")) +\n  geom_bar() +\n  coord_flip()"
  },
  {
    "objectID": "lectures/02-1dcat.html#so-you-want-to-make-pie-charts",
    "href": "lectures/02-1dcat.html#so-you-want-to-make-pie-charts",
    "title": "1D Categorical Data",
    "section": "So you want to make pie charts…",
    "text": "So you want to make pie charts…\n\npenguins |&gt; \n  ggplot(aes(fill = species, x = \"\")) + \n  geom_bar(aes(y = after_stat(count))) +\n  coord_polar(theta = \"y\") +\n  theme_void()"
  },
  {
    "objectID": "lectures/02-1dcat.html#friends-dont-let-friends-make-pie-charts",
    "href": "lectures/02-1dcat.html#friends-dont-let-friends-make-pie-charts",
    "title": "1D Categorical Data",
    "section": "Friends Don’t Let Friends Make Pie Charts",
    "text": "Friends Don’t Let Friends Make Pie Charts"
  },
  {
    "objectID": "lectures/02-1dcat.html#waffle-charts-are-cooler-anyway",
    "href": "lectures/02-1dcat.html#waffle-charts-are-cooler-anyway",
    "title": "1D Categorical Data",
    "section": "Waffle charts are cooler anyway…",
    "text": "Waffle charts are cooler anyway…\n\nlibrary(waffle)\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  ggplot(aes(fill = species, values = count)) +\n  geom_waffle(n_rows = 20, color = \"white\", flip = TRUE) +\n  coord_equal() +\n  theme_void()"
  },
  {
    "objectID": "lectures/02-1dcat.html#florence-nightingales-rose-diagram",
    "href": "lectures/02-1dcat.html#florence-nightingales-rose-diagram",
    "title": "1D Categorical Data",
    "section": "Florence Nightingale’s Rose Diagram",
    "text": "Florence Nightingale’s Rose Diagram"
  },
  {
    "objectID": "lectures/02-1dcat.html#rose-diagrams",
    "href": "lectures/02-1dcat.html#rose-diagrams",
    "title": "1D Categorical Data",
    "section": "Rose diagrams",
    "text": "Rose diagrams\n\npenguins |&gt; \n  ggplot(aes(x = species)) + \n  geom_bar(fill = \"darkblue\") +\n  coord_polar() +\n  scale_y_sqrt()"
  },
  {
    "objectID": "lectures/02-1dcat.html#recap-and-next-steps",
    "href": "lectures/02-1dcat.html#recap-and-next-steps",
    "title": "1D Categorical Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\n1D Categorical Data: look at counts, frequencies, percentages\nArea plots, where area \\(\\propto\\) counts/frequencies/percentages:\n\nBar charts (you should pretty much always just make a bar chart)\nSpine charts (will be more useful with more variables)\nPie charts (DON’T DO IT)\nRose diagrams (temporal or directional context can justify usage)\n\n\n\n\n\n\nComplete HW0 by TONIGHT! Confirms you have everything installed and can render .qmd files to PDF via tinytex\nHW1 is due in two weeks, no class on Monday\n\n\n\n\n\n\nNext time: quantify and display uncertainty for 1D categorical data\nRecommended reading:\n\nCW Chapter 10 Visualizing proportions, CW Chapter 16.2 Visualizing the uncertainty of point estimates, CW Chapter 11 Visualizing nested proportions"
  },
  {
    "objectID": "lectures/05-2dcat.html#announcements-previously-and-today",
    "href": "lectures/05-2dcat.html#announcements-previously-and-today",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW1 is due TONIGHT by 11:59 PM\nYou have Lab 3 again on Friday\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Wednesdays @ 11 AM\n\n\n\n\n\nDiscussed how similar looking graphics can have very different statistical results (thinking about power)\nDiscussed the challenges of multiple testing\n\n\n\n\n\nTODAY:\n\nVisuals for 2D categorical data\nHow do we visualize inference for 2D categorical data?"
  },
  {
    "objectID": "lectures/05-2dcat.html#d-categorical-basics",
    "href": "lectures/05-2dcat.html#d-categorical-basics",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "2D categorical basics",
    "text": "2D categorical basics"
  },
  {
    "objectID": "lectures/05-2dcat.html#d-categorical-basics-1",
    "href": "lectures/05-2dcat.html#d-categorical-basics-1",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "2D categorical basics",
    "text": "2D categorical basics\n\naddmargins(table(\"Species\" = penguins$species, \"Island\" = penguins$island))\n\n           Island\nSpecies     Biscoe Dream Torgersen Sum\n  Adelie        44    56        52 152\n  Chinstrap      0    68         0  68\n  Gentoo       124     0         0 124\n  Sum          168   124        52 344\n\n\n\nColumn and row sums: marginal distributions\nValues within rows: conditional distribution for Island given Species\nValues within columns: conditional distribution for Species given Island\nBottom right: total number of observations"
  },
  {
    "objectID": "lectures/05-2dcat.html#connecting-distributions-to-visualizations",
    "href": "lectures/05-2dcat.html#connecting-distributions-to-visualizations",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Connecting distributions to visualizations",
    "text": "Connecting distributions to visualizations\nFive distributions for two categorical variables \\(A\\) and \\(B\\):\n\nMarginals: \\(P(A)\\) and \\(P(B)\\)\nConditionals: \\(P(A | B)\\) and \\(P(B|A)\\)\nJoint: \\(P(A, B)\\)\n\nWe use bar charts to visualize marginal distributions for categorical variables…\n\nAnd we’ll use more bar charts to visualize conditional and joint distributions!"
  },
  {
    "objectID": "lectures/05-2dcat.html#stacked-bar-charts---a-bar-chart-of-spine-charts",
    "href": "lectures/05-2dcat.html#stacked-bar-charts---a-bar-chart-of-spine-charts",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Stacked bar charts - a bar chart of spine charts",
    "text": "Stacked bar charts - a bar chart of spine charts\n\npenguins |&gt;\n  ggplot(aes(x = species, fill = island)) +\n  geom_bar() + \n  theme_bw()\n\n\n\n\nEasy to see marginal of species, i.e., \\(P(\\) x \\()\\)\nCan see conditional of island | species, i.e., \\(P(\\) fill | x \\()\\)\nHarder to see conditional of species | island, i.e., \\(P(\\) x | fill \\()\\)"
  },
  {
    "objectID": "lectures/05-2dcat.html#side-by-side-bar-charts",
    "href": "lectures/05-2dcat.html#side-by-side-bar-charts",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Side-by-side bar charts",
    "text": "Side-by-side bar charts\n\npenguins |&gt;\n  ggplot(aes(x = species, fill = island)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw()\n\n\n\n\nEasy to see conditional of island | species, i.e., \\(P(\\) fill | x \\()\\)\nCan see conditional of species | island, i.e., \\(P(\\) x | fill \\()\\)"
  },
  {
    "objectID": "lectures/05-2dcat.html#side-by-side-bar-charts-1",
    "href": "lectures/05-2dcat.html#side-by-side-bar-charts-1",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Side-by-side bar charts",
    "text": "Side-by-side bar charts\n\npenguins |&gt;\n  ggplot(aes(x = species, fill = island)) + \n  geom_bar(position = position_dodge(preserve = \"single\")) +\n  theme_bw()\n\n\n\n\nEasy to see conditional of island | species, i.e., \\(P(\\) fill | x \\()\\)\nCan see conditional of species | island, i.e., \\(P(\\) x | fill \\()\\)"
  },
  {
    "objectID": "lectures/05-2dcat.html#complete-missing-values-to-preserve-location",
    "href": "lectures/05-2dcat.html#complete-missing-values-to-preserve-location",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Complete missing values to preserve location",
    "text": "Complete missing values to preserve location\n\npenguins |&gt;\n  count(species, island) |&gt;\n  complete(species = unique(species), island = unique(island), \n           fill = list(n = 0)) |&gt;\n  ggplot(aes(x = species, y = n, fill = island)) + \n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/05-2dcat.html#what-do-you-prefer",
    "href": "lectures/05-2dcat.html#what-do-you-prefer",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "What do you prefer?",
    "text": "What do you prefer?"
  },
  {
    "objectID": "lectures/05-2dcat.html#chi-squared-test-for-1d-categorical-data",
    "href": "lectures/05-2dcat.html#chi-squared-test-for-1d-categorical-data",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Chi-squared test for 1D categorical data:",
    "text": "Chi-squared test for 1D categorical data:\n\n\nNull hypothesis \\(H_0\\): \\(p_1 = p_2 = \\dots = p_K\\), compute the test statistic:\n\n\\[\n\\chi^2 = \\sum_{j=1}^K \\frac{(O_j - E_j)^2}{E_j}\n\\]\n\n\\(O_j\\): observed counts in category \\(j\\)\n\\(E_j\\): expected counts under \\(H_0\\), i.e., each category is equally to occur \\(n / K = p_1 = p_2 = \\dots = p_K\\)\n\n\n\n\nchisq.test(table(penguins$species))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(penguins$species)\nX-squared = 31.907, df = 2, p-value = 1.179e-07"
  },
  {
    "objectID": "lectures/05-2dcat.html#inference-for-2d-categorical-data",
    "href": "lectures/05-2dcat.html#inference-for-2d-categorical-data",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Inference for 2D categorical data",
    "text": "Inference for 2D categorical data\n\nAgain we use the chi-squared test:\n\nNull hypothesis \\(H_0\\): variables \\(A\\) and \\(B\\) are independent, compute the test statistic:\n\n\\[\\chi^2 = \\sum_{i}^{K_A} \\sum_{j}^{K_B} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\n\n\\(O_{ij}\\): observed counts in contingency table\n\\(E_{ij}\\): expected counts under \\(H_0\\)\n\n\\[\n\\begin{aligned}\nE_{ij} &= n \\cdot P(A = a_i, B = b_j) \\\\\n&= n \\cdot P(A = a_i) P(B = b_j) \\\\\n&= n \\cdot \\left( \\frac{n_{i \\cdot}}{n} \\right) \\left( \\frac{ n_{\\cdot j}}{n} \\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lectures/05-2dcat.html#inference-for-2d-categorical-data-1",
    "href": "lectures/05-2dcat.html#inference-for-2d-categorical-data-1",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Inference for 2D categorical data",
    "text": "Inference for 2D categorical data\n\nAgain we use the chi-squared test:\n\nNull hypothesis \\(H_0\\): variables \\(A\\) and \\(B\\) are independent, compute the test statistic:\n\n\\[\\chi^2 = \\sum_{i}^{K_A} \\sum_{j}^{K_B} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\n\n\\(O_{ij}\\): observed counts in contingency table\n\\(E_{ij}\\): expected counts under \\(H_0\\)\n\n\n\nchisq.test(table(penguins$species, penguins$island))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(penguins$species, penguins$island)\nX-squared = 299.55, df = 4, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/05-2dcat.html#visualize-independence-test-with-mosaic-plots",
    "href": "lectures/05-2dcat.html#visualize-independence-test-with-mosaic-plots",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Visualize independence test with mosaic plots",
    "text": "Visualize independence test with mosaic plots\n\nTwo variables are independent if knowing the level of one tells us nothing about the other\n\ni.e. \\(P(A | B) = P(A)\\), and that \\(P(A, B) = P(A) \\times P(B)\\)\n\nCreate a mosaic plot using base R\n\n\nmosaicplot(table(penguins$species, penguins$island))"
  },
  {
    "objectID": "lectures/05-2dcat.html#shade-by-pearson-residuals",
    "href": "lectures/05-2dcat.html#shade-by-pearson-residuals",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Shade by Pearson residuals",
    "text": "Shade by Pearson residuals\n\n\nThe test statistic is:\n\n\\[\\chi^2 = \\sum_{i}^{K_A} \\sum_{j}^{K_B} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\n\nDefine the Pearson residuals as:\n\n\\[r_{ij} = \\frac{O_{ij} - E_{ij}}{\\sqrt{E_{ij}}}\\]\n\nSide-note: In general, Pearson residuals are \\(\\frac{\\text{residuals}}{\\sqrt{\\text{variance}}}\\)\n\n\n\n\n\n\\(r_{ij} \\approx 0 \\rightarrow\\) observed counts are close to expected counts\n\\(|r_{ij}| &gt; 2 \\rightarrow\\) “significant” at level \\(\\alpha = 0.05\\).\nVery positive \\(r_{ij} \\rightarrow\\) more than expected, while very negative \\(r_{ij} \\rightarrow\\) fewer than expected\nColor by Pearson residuals to tell us which combos are much bigger/smaller than expected."
  },
  {
    "objectID": "lectures/05-2dcat.html#titanic-dataset-example",
    "href": "lectures/05-2dcat.html#titanic-dataset-example",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Titanic Dataset Example",
    "text": "Titanic Dataset Example\n\ntitanic &lt;- read_csv(\"https://raw.githubusercontent.com/ryurko/DataViz-Class-Data/main/titanic.csv\")\n\nQuestion: Does survival (yes/no) depend on cabin (1st/2nd/3rd)?\n\ntable(\"Survived?\" = titanic$Survived, \"Class\" = titanic$Pclass)\n\n         Class\nSurvived?   1   2   3\n        0  64  90 270\n        1 120  83  85\n\n\n\n\nchisq.test(table(\"Survived?\" = titanic$Survived, \"Class\" = titanic$Pclass))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(`Survived?` = titanic$Survived, Class = titanic$Pclass)\nX-squared = 91.081, df = 2, p-value &lt; 2.2e-16\n\n\nConclusion: Class and survival are dependent - but how?"
  },
  {
    "objectID": "lectures/05-2dcat.html#guardian-1000-songs-to-hear-before-you-die",
    "href": "lectures/05-2dcat.html#guardian-1000-songs-to-hear-before-you-die",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Guardian: 1000 songs to hear before you die",
    "text": "Guardian: 1000 songs to hear before you die"
  },
  {
    "objectID": "lectures/05-2dcat.html#recap-and-next-steps",
    "href": "lectures/05-2dcat.html#recap-and-next-steps",
    "title": "Visualizations and Inference for 2D Categorical Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nFor 2D categorical data we create visualizations for marginal, conditional, and joint distributions\nCan create stacked and side-by-side bar charts to visualize 2D categorical data\nPerform 2D Chi-squared test to test if two categorical variables are associated with each other\nCreate mosaic plots to visualize 2D categorical data\nShade mosaic plot tiles by Pearson residuals to see what drives association between two categorical variables (if any)\n\n\n\n\n\nHW1 is due TONIGHT and you have Lab 3 on Friday!\nNext time: Visualizing 1D quantitative data\nRecommended reading: CW Chapter 11 Visualizing nested proportions"
  },
  {
    "objectID": "lectures/07-density-estimation.html#announcements-previously-and-today",
    "href": "lectures/07-density-estimation.html#announcements-previously-and-today",
    "title": "Density Estimation",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW2 is due TONIGHT by 11:59 PM\nYou have Lab 4 again on Friday!\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Wednesdays @ 11 AM\n\n\n\n\n\nVisualize 1D quantitative data to inspect center, spread, and shape\nBoxplots are only a display of summary statistics (i.e., they suck)\nHistograms display shape of the distribution, but comes with tradeoffs\nDensity curves provide an easy way to visualize conditional distributions\n\n\n\n\n\nTODAY:\n\nDisplaying smooth densities\nHow does kernel density estimation work?"
  },
  {
    "objectID": "lectures/07-density-estimation.html#continuous-densities",
    "href": "lectures/07-density-estimation.html#continuous-densities",
    "title": "Density Estimation",
    "section": "Continuous Densities",
    "text": "Continuous Densities\nDistribution of any continuous random variable \\(X\\) is defined by a probability density function (PDF), typically denoted by \\(f(x)\\)\n\nProbability continuous variable \\(X\\) takes a particular value is 0, why?\n\nUse PDF to provide a relative likelihood,\n\ne.g., Normal distribution: \\(f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp(- \\frac{(x - \\mu)^2}{2\\sigma^2})\\)\n\n\n\nProperties of densities\n\nHow do we estimate densities?"
  },
  {
    "objectID": "lectures/07-density-estimation.html#normal-distribution",
    "href": "lectures/07-density-estimation.html#normal-distribution",
    "title": "Density Estimation",
    "section": "Normal distribution",
    "text": "Normal distribution"
  },
  {
    "objectID": "lectures/07-density-estimation.html#uniform-distribution",
    "href": "lectures/07-density-estimation.html#uniform-distribution",
    "title": "Density Estimation",
    "section": "Uniform distribution",
    "text": "Uniform distribution"
  },
  {
    "objectID": "lectures/07-density-estimation.html#gamma-also-exponential-and-chi-squared-distribution",
    "href": "lectures/07-density-estimation.html#gamma-also-exponential-and-chi-squared-distribution",
    "title": "Density Estimation",
    "section": "Gamma (also Exponential and Chi-squared) distribution",
    "text": "Gamma (also Exponential and Chi-squared) distribution"
  },
  {
    "objectID": "lectures/07-density-estimation.html#beta-distribution",
    "href": "lectures/07-density-estimation.html#beta-distribution",
    "title": "Density Estimation",
    "section": "Beta distribution",
    "text": "Beta distribution"
  },
  {
    "objectID": "lectures/07-density-estimation.html#normalize-histogram-frequencies-with-density",
    "href": "lectures/07-density-estimation.html#normalize-histogram-frequencies-with-density",
    "title": "Density Estimation",
    "section": "Normalize histogram frequencies with density",
    "text": "Normalize histogram frequencies with density\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_histogram(aes(y = after_stat(density), fill = species),\n                 position = \"identity\", alpha = 0.3)"
  },
  {
    "objectID": "lectures/07-density-estimation.html#can-use-density-curves-instead",
    "href": "lectures/07-density-estimation.html#can-use-density-curves-instead",
    "title": "Density Estimation",
    "section": "Can use density curves instead",
    "text": "Can use density curves instead\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(aes(color = species))"
  },
  {
    "objectID": "lectures/07-density-estimation.html#kernel-density-estimation",
    "href": "lectures/07-density-estimation.html#kernel-density-estimation",
    "title": "Density Estimation",
    "section": "Kernel density estimation",
    "text": "Kernel density estimation\nGoal: estimate PDF \\(f(x)\\) for all possible values (assuming it is continuous & smooth)\n\n\\[\n\\text{Kernel density estimate: } \\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)\n\\]\n\n\n\n\n\\(n =\\) sample size, \\(x =\\) new point to estimate \\(f(x)\\) (does NOT have to be in dataset!)\n\n\n\n\n\n\n\\(h =\\) bandwidth, analogous to histogram bin width, ensures \\(\\hat{f}(x)\\) integrates to 1\n\\(x_i =\\) \\(i\\)th observation in dataset\n\n\n\n\n\n\n\\(K_h(x - x_i)\\) is the Kernel function, creates weight given distance of \\(i\\)th observation from new point\n\nas \\(|x - x_i| \\rightarrow \\infty\\) then \\(K_h(x - x_i) \\rightarrow 0\\), i.e. further apart \\(i\\)th row is from \\(x\\), smaller the weight\nas bandwidth \\(h \\uparrow\\) weights are more evenly spread out (as \\(h \\downarrow\\) more concentrated around \\(x\\))\ntypically use Gaussian / Normal kernel: \\(\\propto e^{-(x - x_i)^2 / 2h^2}\\)\n\\(K_h(x - x_i)\\) is large when \\(x_i\\) is close to \\(x\\)"
  },
  {
    "objectID": "lectures/07-density-estimation.html#wikipedia-example",
    "href": "lectures/07-density-estimation.html#wikipedia-example",
    "title": "Density Estimation",
    "section": "Wikipedia example",
    "text": "Wikipedia example"
  },
  {
    "objectID": "lectures/07-density-estimation.html#we-display-kernel-density-estimates-with-geom_density",
    "href": "lectures/07-density-estimation.html#we-display-kernel-density-estimates-with-geom_density",
    "title": "Density Estimation",
    "section": "We display kernel density estimates with geom_density()",
    "text": "We display kernel density estimates with geom_density()\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density() +\n  theme_bw()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#choice-of-kernel",
    "href": "lectures/07-density-estimation.html#choice-of-kernel",
    "title": "Density Estimation",
    "section": "Choice of kernel?",
    "text": "Choice of kernel?"
  },
  {
    "objectID": "lectures/07-density-estimation.html#what-about-the-bandwidth",
    "href": "lectures/07-density-estimation.html#what-about-the-bandwidth",
    "title": "Density Estimation",
    "section": "What about the bandwidth?",
    "text": "What about the bandwidth?\nUse Gaussian reference rule (rule-of-thumb) \\(\\approx 1.06 \\cdot \\sigma \\cdot n^{-1/5}\\), where \\(\\sigma\\) is the observed standard deviation\nModify the bandwidth using the adjust argument - value to multiply default bandwidth by\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(adjust = 0.5) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#what-about-the-bandwidth-1",
    "href": "lectures/07-density-estimation.html#what-about-the-bandwidth-1",
    "title": "Density Estimation",
    "section": "What about the bandwidth?",
    "text": "What about the bandwidth?\nUse Gaussian reference rule (rule-of-thumb) \\(\\approx 1.06 \\cdot \\sigma \\cdot n^{-1/5}\\), where \\(\\sigma\\) is the observed standard deviation\nModify the bandwidth using the adjust argument - value to multiply default bandwidth by\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(adjust = 2) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#caution-dealing-with-bounded-data",
    "href": "lectures/07-density-estimation.html#caution-dealing-with-bounded-data",
    "title": "Density Estimation",
    "section": "CAUTION: dealing with bounded data…",
    "text": "CAUTION: dealing with bounded data…\n\nset.seed(101)\nbound_data &lt;- tibble(fake_x = runif(100))\n\nbound_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_density() +\n  geom_rug(alpha = 0.5) + #&lt;&lt;\n  stat_function(data = \n                  tibble(fake_x = c(0, 1)),\n                fun = dunif, color = \"red\") +\n  scale_x_continuous(limits = c(-.5, 1.5))"
  },
  {
    "objectID": "lectures/07-density-estimation.html#visualizing-conditional-distributions-violin-plots",
    "href": "lectures/07-density-estimation.html#visualizing-conditional-distributions-violin-plots",
    "title": "Density Estimation",
    "section": "Visualizing conditional distributions: violin plots",
    "text": "Visualizing conditional distributions: violin plots\n\npenguins |&gt;\n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_violin() +\n  coord_flip()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#visualizing-conditional-distributions-violin-plots-1",
    "href": "lectures/07-density-estimation.html#visualizing-conditional-distributions-violin-plots-1",
    "title": "Density Estimation",
    "section": "Visualizing conditional distributions: violin plots",
    "text": "Visualizing conditional distributions: violin plots\n\npenguins |&gt;\n  ggplot(aes(x = species, y = flipper_length_mm)) +\n  geom_violin() + \n  geom_boxplot(width = .2) +\n  coord_flip()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#visualizing-conditional-distributions-ggridges",
    "href": "lectures/07-density-estimation.html#visualizing-conditional-distributions-ggridges",
    "title": "Density Estimation",
    "section": "Visualizing conditional distributions: ggridges",
    "text": "Visualizing conditional distributions: ggridges\n\nlibrary(ggridges)\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = species)) +\n  geom_density_ridges(rel_min_height = 0.01)"
  },
  {
    "objectID": "lectures/07-density-estimation.html#visualizing-conditional-distributions-ggbeeswarm",
    "href": "lectures/07-density-estimation.html#visualizing-conditional-distributions-ggbeeswarm",
    "title": "Density Estimation",
    "section": "Visualizing conditional distributions: ggbeeswarm",
    "text": "Visualizing conditional distributions: ggbeeswarm\n\nlibrary(ggbeeswarm)\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = species)) +\n  geom_beeswarm(cex = 1.5) +\n  theme_bw()"
  },
  {
    "objectID": "lectures/07-density-estimation.html#recap-and-next-steps",
    "href": "lectures/07-density-estimation.html#recap-and-next-steps",
    "title": "Density Estimation",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nSmoothed densities are a flexible tool for visualizing 1D distribution\nThere are two choices we need to make for kernel density estimation:\n\nBandwidth: Determines smoothness of distribution, usually data-driven choice\nKernel: Determines how much influence each observation should have on each other during estimation, usually context driven\n\nSeveral other types of density-based displays: violins, ridges, beeswarm plots\n\n\n\n\n\nHW2 is due TONIGHT and you have Lab 4 on Friday\nNext time: Graphical inference for 1D quantitative data\nRecommended reading: CW Chapter 7 Visualizing distributions: Histograms and density plots"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#announcements-previously-and-today",
    "href": "lectures/09-compare-distr-power.html#announcements-previously-and-today",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW3 is due TONIGHT by 11:59 PM and you have Lab 5 again on Friday!\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Tuesdays @ 11 AM\n\n\n\n\nFinished discussed density based visualizations\nIntroduced KS test for testing if distribution follows a particular distribution\nGraphics are extremely useful because human eyes can quickly compare and contrast distributions…\n\n\n\n\nTODAY:\n\nUnderstanding the statistical power of tests and graphics"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#flipper-length-example",
    "href": "lectures/09-compare-distr-power.html#flipper-length-example",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Flipper length example",
    "text": "Flipper length example"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#statistical-tests-for-comparing-distributions",
    "href": "lectures/09-compare-distr-power.html#statistical-tests-for-comparing-distributions",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Statistical Tests for Comparing Distributions",
    "text": "Statistical Tests for Comparing Distributions\n\n\nWe’ve focused on assessing if a single quantitative variable follows a particular distribution\n\nLogic of one-sample KS test: Compare empirical distribution to theoretical distribution\n\n\n\n\n\nHow do we compare multiple empirical distributions?\nVery common scenario: Determine if a quantitative variable depends on a categorical variable, examples:\n\nClinical trials with multiple treatments\nAssessing differences across race, gender, socioeconomic status\nIndustrial experiments, A/B testing\nComparing song duration across different genres?\n\nCan use overlayed densities, side-by-side violin plots, facetted histograms\nRemember: plotting conditional distributions… but when are differences in a graphic statistically significant?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre",
    "href": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre",
    "title": "Comparing Distributions and Statistical Power",
    "section": "TidyTuesday Spotify Songs - Duration by Genre",
    "text": "TidyTuesday Spotify Songs - Duration by Genre"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre-1",
    "href": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "TidyTuesday Spotify Songs - Duration by Genre",
    "text": "TidyTuesday Spotify Songs - Duration by Genre"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#kolmogorov-smirnov-test-again",
    "href": "lectures/09-compare-distr-power.html#kolmogorov-smirnov-test-again",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Kolmogorov-Smirnov Test… Again",
    "text": "Kolmogorov-Smirnov Test… Again\nKS test can also be used to compare two empirical distributions \\(\\hat{F}_A(x)\\) and \\(\\hat{F}_B\\), via test statistic for two samples:\n\\[\n\\text{max}_x |\\hat{F}_A(x) - \\hat{F}_B(x)|\n\\]\n\nspotify_songs &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')\nrap_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"rap\") |&gt; pull(duration_ms)\nrock_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"rock\") |&gt; pull(duration_ms)\n\nks.test(rap_duration, y = rock_duration)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  rap_duration and rock_duration\nD = 0.22386, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#kolmogorov-smirnov-test-again-1",
    "href": "lectures/09-compare-distr-power.html#kolmogorov-smirnov-test-again-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Kolmogorov-Smirnov Test… Again",
    "text": "Kolmogorov-Smirnov Test… Again\nKS test can also be used to compare two empirical distributions \\(\\hat{F}_A(x)\\) and \\(\\hat{F}_B\\), via test statistic for two samples:\n\\[\n\\text{max}_x |\\hat{F}_A(x) - \\hat{F}_B(x)|\n\\]"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#statistical-tests-for-comparing-distributions-1",
    "href": "lectures/09-compare-distr-power.html#statistical-tests-for-comparing-distributions-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Statistical Tests for Comparing Distributions",
    "text": "Statistical Tests for Comparing Distributions\n\nInfinite number of ways that you can compare multiple quantitative distributions, 3 common ways:\n\nAny difference at all? \nDifference in means?\n\n\nNull hypothesis: \\(H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_K\\) (use t.test or oneway.test() functions)\nCan assume the variances are all the same or differ\nIf reject, can only conclude not all means are equal\n\n\nDifference in variances?\n\n\nNull hypothesis: \\(H_0: \\sigma^2_1 = \\sigma^2_2 = \\cdots = \\sigma^2_K\\) (use bartlett.test() function)\nIf reject, can only conclude not all variances are equal\n\nUnlike the KS test, difference in means and variances are sensitive to non-Normality\n\nDifferent distributions can yield insignificant results"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock-1",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock-2",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-rap-and-rock-2",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between rap and rock?",
    "text": "Test difference between rap and rock?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-pop-and-rap",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-pop-and-rap",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between pop and rap?",
    "text": "Test difference between pop and rap?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#recap-one-sample-ks-test",
    "href": "lectures/09-compare-distr-power.html#recap-one-sample-ks-test",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Recap: One-Sample KS Test",
    "text": "Recap: One-Sample KS Test\n\n\nHave a single sample \\(\\mathbf{X} = (X_1,\\dots,X_n)\\)\nWant to test: Does \\(\\mathbf{X}\\) follow a particular distribution?\nCompares the empirical CDF of \\(\\mathbf{X}\\) to the theoretical CDF of a particular distribution:\n\n\\[\\underbrace{F(x) = P(X \\leq x)}_{\\text{theoretical CDF}}, \\hspace{0.2in} \\underbrace{\\hat{F}(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(X_i \\leq x)}_{\\text{empirical CDF}}\\]\n\nNull hypothesis: \\(\\mathbf{X}\\) follows a distribution with CDF \\(F(x)\\)\nAlternative hypothesis: \\(\\mathbf{X}\\) does not follow this distribution\nTest statistic: \\(\\max_x |\\hat{F}(x) - F(x)|\\)\nIf \\(\\hat{F}(x)\\) is far away from \\(F(x)\\) \\(\\rightarrow\\) reject null"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#recap-two-sample-ks-test",
    "href": "lectures/09-compare-distr-power.html#recap-two-sample-ks-test",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Recap: Two-Sample KS Test",
    "text": "Recap: Two-Sample KS Test\n\n\nHave two samples \\(\\mathbf{X} = (X_1,\\dots,X_m)\\), \\(\\mathbf{Y} = (Y_1,\\dots,Y_n)\\)\nWant to test: Do \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) follow the same distribution?\nCompares the empirical CDFs of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\):\n\n\\[\\underbrace{\\hat{F}_X(z) = \\frac{1}{m} \\sum_{i=1}^m \\mathbb{I}(X_i \\leq z)}_{\\text{empirical CDF of } \\mathbf{X}} \\hspace{0.2in} \\underbrace{\\hat{F}_Y(z) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(Y_i \\leq z)}_{\\text{empirical CDF of } \\mathbf{Y}}\\]\n\nNull hypothesis: \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) follow the same distribution.\nAlternative hypothesis: \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) do not follow the same distribution\nTest statistic: \\(\\max_z |\\hat{F}_X(z) - \\hat{F}_Y(z)|\\)\nIf \\(\\hat{F}_X\\) and \\(\\hat{F}_Y\\) are far away from each other \\(\\rightarrow\\) reject null"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre-2",
    "href": "lectures/09-compare-distr-power.html#tidytuesday-spotify-songs---duration-by-genre-2",
    "title": "Comparing Distributions and Statistical Power",
    "section": "TidyTuesday Spotify Songs - Duration by Genre",
    "text": "TidyTuesday Spotify Songs - Duration by Genre"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#what-about-the-difference-between-pop-and-rap",
    "href": "lectures/09-compare-distr-power.html#what-about-the-difference-between-pop-and-rap",
    "title": "Comparing Distributions and Statistical Power",
    "section": "What about the difference between pop and rap?",
    "text": "What about the difference between pop and rap?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#what-about-the-difference-between-pop-and-rap-1",
    "href": "lectures/09-compare-distr-power.html#what-about-the-difference-between-pop-and-rap-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "What about the difference between pop and rap?",
    "text": "What about the difference between pop and rap?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#significant-difference-with-large-sample-size",
    "href": "lectures/09-compare-distr-power.html#significant-difference-with-large-sample-size",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Significant difference with large sample size",
    "text": "Significant difference with large sample size\n\nspotify_songs &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')\ntable(spotify_songs$playlist_genre)\n\n\n  edm latin   pop   r&b   rap  rock \n 6043  5155  5507  5431  5746  4951 \n\nrap_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"rap\") |&gt; pull(duration_ms)\npop_duration &lt;- spotify_songs |&gt; filter(playlist_genre == \"pop\") |&gt; pull(duration_ms)\n\nks.test(rap_duration, y = pop_duration)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  rap_duration and pop_duration\nD = 0.14569, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#what-happens-if-we-had-a-smaller-sample",
    "href": "lectures/09-compare-distr-power.html#what-happens-if-we-had-a-smaller-sample",
    "title": "Comparing Distributions and Statistical Power",
    "section": "What happens if we had a smaller sample?",
    "text": "What happens if we had a smaller sample?\n\nset.seed(2017)\nsample_songs &lt;- spotify_songs |&gt;\n  group_by(playlist_genre) |&gt; \n  slice_sample(n = 100)\n\ntable(sample_songs$playlist_genre)\n\n\n  edm latin   pop   r&b   rap  rock \n  100   100   100   100   100   100 \n\nsample_rap_duration &lt;- sample_songs |&gt; filter(playlist_genre == \"rap\") |&gt; pull(duration_ms)\nsample_pop_duration &lt;- sample_songs |&gt; filter(playlist_genre == \"pop\") |&gt; pull(duration_ms)\n\nks.test(sample_rap_duration, y = sample_pop_duration)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  sample_rap_duration and sample_pop_duration\nD = 0.16, p-value = 0.1545\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#but-it-still-looks-different",
    "href": "lectures/09-compare-distr-power.html#but-it-still-looks-different",
    "title": "Comparing Distributions and Statistical Power",
    "section": "But it still looks different???",
    "text": "But it still looks different???"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-means-and-variances",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-means-and-variances",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between means and variances?",
    "text": "Test difference between means and variances?\nCan test difference in means using t.test():\n\nt.test(sample_rap_duration, sample_pop_duration)\n\n\n    Welch Two Sample t-test\n\ndata:  sample_rap_duration and sample_pop_duration\nt = 0.83091, df = 172.78, p-value = 0.4072\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -8766.158 21512.638\nsample estimates:\nmean of x mean of y \n 221645.7  215272.5"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#test-difference-between-means-and-variances-1",
    "href": "lectures/09-compare-distr-power.html#test-difference-between-means-and-variances-1",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Test difference between means and variances?",
    "text": "Test difference between means and variances?\nCan test difference in variances using bartlett.test():\n\nbartlett.test(list(sample_rap_duration, sample_pop_duration))\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  list(sample_rap_duration, sample_pop_duration)\nBartlett's K-squared = 15.54, df = 1, p-value = 8.08e-05\n\n\nRejects at \\(\\alpha = 0.05\\) even with this smaller sample size!\n\n\nWhy did the KS test say they weren’t different when the graph were clearly different? Two possible reasons:\n\nThe sample size might be too small to detect a difference\nThe KS test is known to have low power"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#statistical-power",
    "href": "lectures/09-compare-distr-power.html#statistical-power",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Statistical power",
    "text": "Statistical power\nStatistical power is key to really understanding graphics - you need to know when you’re looking at real effects versus noise\nHere are two definitions of power (one in English, one in math):\n\nEnglish: The probability that we reject the null hypothesis when the null hypothesis is false.\nMath: \\(P(\\text{p-value} \\leq \\alpha | H_0\\) is false)"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#toy-example-for-understanding-statistical-power",
    "href": "lectures/09-compare-distr-power.html#toy-example-for-understanding-statistical-power",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Toy example for understanding statistical power",
    "text": "Toy example for understanding statistical power\n\nConsider two samples:\n\\[(X_1,\\dots,X_n) \\sim N(0, 1)\\] \\[(Y_1,\\dots,Y_n) \\sim N(\\delta, 1)\\]\nLet’s say we use t.test(x, y)\nWe’ll simulate \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) 1000 times for some \\(n\\) and \\(\\delta &gt; 0\\)\nWe’ll count the number of times we reject\n\\[\\text{Power} = P(\\text{p-value} \\leq \\alpha | H_0 \\text{ false}) \\\\\n            = P(\\text{p-value} \\leq \\alpha | \\delta &gt; 0) \\\\\n            \\approx \\frac{\\text{# times reject}}{1000}\\]\nWe’ll consider \\(n = 10, 20, \\dots, 1000\\) and \\(\\delta = 0.1\\) or \\(\\delta = 0.25\\)"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#toy-example-power-of-t-test",
    "href": "lectures/09-compare-distr-power.html#toy-example-power-of-t-test",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Toy example: power of \\(t\\)-test",
    "text": "Toy example: power of \\(t\\)-test"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#another-toy-example",
    "href": "lectures/09-compare-distr-power.html#another-toy-example",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Another toy example",
    "text": "Another toy example\nConsider two samples:\n\\[(X_1,\\dots,X_n) \\sim N(0, 1)\\] \\[(Y_1,\\dots,Y_n) \\sim N(0, 1.5)\\]\nLet’s consider three ways to test differences:\n\nt.test(x, y)\nbartlett.test(list(x, y))\nks.test(x,y)\n\nWe’ll simulate \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) 1000 times for samples sizes \\(n = 10, 20, \\dots, 1000\\)\nWhat do you think the power curves will look like for these methods?"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#comparison-of-power-for-the-different-tests",
    "href": "lectures/09-compare-distr-power.html#comparison-of-power-for-the-different-tests",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Comparison of power for the different tests",
    "text": "Comparison of power for the different tests"
  },
  {
    "objectID": "lectures/09-compare-distr-power.html#recap-and-next-steps",
    "href": "lectures/09-compare-distr-power.html#recap-and-next-steps",
    "title": "Comparing Distributions and Statistical Power",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nGraphics should be paired with statistical analyses to determine if what you see is a true effect versus noise\nEven if there is a true effect, you may have limited power to detect it (some effects are easier to detect than others)\nRemember: Power is the probability you reject when the null is false. Things that increase statistical power:\n\nIncrease sample size\nReduce variance/error\nIncrease differences / effects\nChoose appropriate tests!\n\n\n\n\n\nHW3 is due TONIGHT and you have Lab 5 on Friday\nNext time: 2D Quantitative Data - Scatterplots and Linear Regression"
  },
  {
    "objectID": "lectures/10-2dquant.html#announcements-previously-and-today",
    "href": "lectures/10-2dquant.html#announcements-previously-and-today",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW4 is due Wednesday by 11:59 PM and you have Lab 6 again on Friday!\nTake-home exam is next week Wednesday Feb 26th\nHere’s how the exam will work:\n\nI’ll post the exam Monday evening, and it’s due Wednesday by 11:59 PM EDT (Feb 26th)\nExam will cover material from HW 1-4 and Labs 1-6\nQuestions will be similar to homeworks but more open-ended, e.g, instead of “make a side-by-side violin plot…” I’ll ask “Make a plot that compares the conditional distributions…”\nThere will NOT be class on Wednesday Feb 26th\nConflict Feb 26th? Let me know ASAP! Day-of accommodations will NOT be made, late submissions will NOT be accepted\n\n\n\n\nDiscussed power in the context of visualizations and statistical tests\nEven if there is a true effect, you may have limited power to detect it\nSeveral ways to formally compare distributions:\n\n\\(t\\)-test: Compare means\nBartlett’s test: Compare variances\nKS test: Compare distributions\n\n\n\n\n\nTODAY: 2D quantitative data, scatterplots, and linear regression"
  },
  {
    "objectID": "lectures/10-2dquant.html#d-quantitative-data",
    "href": "lectures/10-2dquant.html#d-quantitative-data",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "2D quantitative data",
    "text": "2D quantitative data\n\nWe’re working with two variables: \\((X, Y) \\in \\mathbb{R}^2\\), i.e., dataset with \\(n\\) rows and 2 columns\n\n\n\nGoals:\n\ndescribing the relationships between two variables\ndescribing the conditional distribution \\(Y | X\\) via regression analysis\ndescribing the joint distribution \\(X,Y\\) via contours, heatmaps, etc.\n\n\n\n\n\nFew big picture ideas to keep in mind:\n\nscatterplots are by far the most common visual\nregression analysis is by far the most popular analysis (you have a whole class on this…)\nrelationships may vary across other variables, e.g., categorical variables"
  },
  {
    "objectID": "lectures/10-2dquant.html#making-scatterplots-with-geom_point",
    "href": "lectures/10-2dquant.html#making-scatterplots-with-geom_point",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Making scatterplots with geom_point()",
    "text": "Making scatterplots with geom_point()\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()"
  },
  {
    "objectID": "lectures/10-2dquant.html#always-adjust-the-alpha",
    "href": "lectures/10-2dquant.html#always-adjust-the-alpha",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "ALWAYS adjust the alpha",
    "text": "ALWAYS adjust the alpha\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#map-categorical-variable-to-color",
    "href": "lectures/10-2dquant.html#map-categorical-variable-to-color",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Map categorical variable to color",
    "text": "Map categorical variable to color\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g,\n             color = species)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#map-continuous-variable-to-color",
    "href": "lectures/10-2dquant.html#map-continuous-variable-to-color",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Map continuous variable to color",
    "text": "Map continuous variable to color\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g,\n             color = body_mass_g)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#map-continuous-variable-to-color-1",
    "href": "lectures/10-2dquant.html#map-continuous-variable-to-color-1",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Map continuous variable to color",
    "text": "Map continuous variable to color\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g,\n             color = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  scale_color_gradient(low = \"darkblue\", high = \"darkorange\")"
  },
  {
    "objectID": "lectures/10-2dquant.html#map-continuous-variable-to-size",
    "href": "lectures/10-2dquant.html#map-continuous-variable-to-size",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Map continuous variable to size",
    "text": "Map continuous variable to size\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g,\n             size = body_mass_g)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#map-categorical-variable-to-shape",
    "href": "lectures/10-2dquant.html#map-categorical-variable-to-shape",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Map categorical variable to shape",
    "text": "Map categorical variable to shape\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g,\n             shape = species)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#all-at-once",
    "href": "lectures/10-2dquant.html#all-at-once",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "ALL AT ONCE!",
    "text": "ALL AT ONCE!\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm,\n             color = species, shape = island, size = body_mass_g)) +\n  geom_point(alpha = 0.5)"
  },
  {
    "objectID": "lectures/10-2dquant.html#displaying-trend-lines-linear-regression",
    "href": "lectures/10-2dquant.html#displaying-trend-lines-linear-regression",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Displaying trend lines: linear regression",
    "text": "Displaying trend lines: linear regression\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "lectures/10-2dquant.html#setup-and-motivation-for-linear-regression",
    "href": "lectures/10-2dquant.html#setup-and-motivation-for-linear-regression",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Setup and motivation for linear regression",
    "text": "Setup and motivation for linear regression\n\nConsider an outcome \\(Y \\in \\mathbb{R}\\) and covariate \\(X \\in \\mathbb{R}\\)\n\nWe have \\(n\\) observations: \\((X_1, Y_1), \\dots, (X_n, Y_n)\\)\n\nPurpose of regression is to model \\(\\mathbb{E}[Y | X]\\)\nConsider the case where \\(X\\) takes on discrete values \\(c_1, \\dots, c_k\\)\nThen most straightforward way to estimate \\(\\mathbb{E}[Y | X = c_j]\\) is to use the sample mean for subgroup \\(X_i = c_j\\):\n\\[\\hat{\\mathbb{E}}[Y|X = c_j] = \\frac{1}{N_j} \\sum_{i: X_i = c_j} Y_i\\]\n\nGraphs like side-by-side violin plots, facetted histograms, and overlaid density plots essentially compare \\(\\hat{\\mathbb{E}}[Y|X = c_j]\\) for different categories\n\nBut when \\(X\\) is quantitative, what do we do?\n\nUse statistical model to “guess” \\(\\mathbb{E}[Y|X = x]\\), even when we don’t observe \\(X = x\\)"
  },
  {
    "objectID": "lectures/10-2dquant.html#statistical-model-for-linear-regression",
    "href": "lectures/10-2dquant.html#statistical-model-for-linear-regression",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Statistical Model for Linear Regression",
    "text": "Statistical Model for Linear Regression\nLinear regression assumes \\(Y_i \\overset{iid}{\\sim} N(\\beta_0 + \\beta_1 X_i, \\sigma^2)\\)\n\n\\(\\beta_0\\): intercept - population mean outcome when \\(X = 0\\); i.e., \\(\\mathbb{E}[Y | X = 0]\\)\n\\(\\beta_1\\): slope - population mean change in \\(Y\\) when \\(X\\) increases by 1\n\\(\\beta_0\\) and \\(\\beta_1\\) are parameters that must be estimated"
  },
  {
    "objectID": "lectures/10-2dquant.html#assessing-assumptions-of-linear-regression",
    "href": "lectures/10-2dquant.html#assessing-assumptions-of-linear-regression",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Assessing assumptions of linear regression",
    "text": "Assessing assumptions of linear regression\nLinear regression assumes \\(Y_i \\overset{iid}{\\sim} N(\\beta_0 + \\beta_1 X_i, \\sigma^2)\\)\n\nIf this is true, then \\(Y_i - \\hat{Y}_i \\overset{iid}{\\sim} N(0, \\sigma^2)\\)\n\n\nPlot residuals against \\(\\hat{Y}_i\\), residuals vs fit plot\n\nUsed to assess linearity, any divergence from mean 0\nUsed to assess equal variance, i.e., if \\(\\sigma^2\\) is homogenous across predictions/fits \\(\\hat{Y}_i\\)\n\n\n\nMore difficult to assess the independence and fixed \\(X\\) assumptions\n\nMake these assumptions based on subject-matter knowledge"
  },
  {
    "objectID": "lectures/10-2dquant.html#residual-vs-fit-plots",
    "href": "lectures/10-2dquant.html#residual-vs-fit-plots",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Residual vs fit plots",
    "text": "Residual vs fit plots\n\nlin_reg &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\n\ntibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")"
  },
  {
    "objectID": "lectures/10-2dquant.html#residual-vs-fit-plots-1",
    "href": "lectures/10-2dquant.html#residual-vs-fit-plots-1",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Residual vs fit plots",
    "text": "Residual vs fit plots\n\ntibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth()"
  },
  {
    "objectID": "lectures/10-2dquant.html#examples-of-residual-vs-fit-plots",
    "href": "lectures/10-2dquant.html#examples-of-residual-vs-fit-plots",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Examples of Residual-vs-Fit Plots",
    "text": "Examples of Residual-vs-Fit Plots"
  },
  {
    "objectID": "lectures/10-2dquant.html#recap-and-next-steps",
    "href": "lectures/10-2dquant.html#recap-and-next-steps",
    "title": "2D Quantitative Data: Scatterplots and Linear Regression",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nScatterplots are the most common visual for 2D quantitative variables\n\nMany ways to incorporate additional dimensions in scatterplots, e.g., color and shape\n\nLinear regression is by far the most common model for describing the relationship between 2+ quantitative variables\n\nCan also: transform the outcome, transform the covariates, do nonparametric “smoothing”\n\nUse graphs to assess linear regression assumptions, i.e., residual diagnostic plots\n\n\n\n\nHW4 due Wednesday and you have Lab 6 on Friday\nGraphics critique due Feb 28th!\nNext time: Inference with Linear Regression\nRecommended reading: CW Chapter 12 Visualizing associations among two or more quantitative variables"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Lecture\nDate\nTitle\nMaterials\n\n\n\n\n1\nJan 13\nIntroduction and Grammar of Graphics\nslides\n\n\n2\nJan 15\n1D Categorical Data\nslides\n\n\n3\nJan 22\nStatistical Inference for 1D Categorical Data\nslides\n\n\n4\nJan 27\nPower and Multiple Testing\nslides\n\n\n5\nJan 29\nVisualizations and Inference for 2D Categorical Data\nslides\n\n\n6\nFeb 3\nVisualizing 1D Quantitative Data\nslides\n\n\n7\nFeb 5\nDensity Estimation\nslides\n\n\n8\nFeb 10\nGraphical Inference for 1D Quantitative Data\nslides\n\n\n9\nFeb 12\nComparing Distributions and Statistical Power\nslides\n\n\n10\nFeb 17\nScatterplots and Linear Regression\nslides\n\n\n11\nFeb 19\nInference with Linear Regression\nslides\n\n\n12\nFeb 24\nMidsemester Review\nslides\n\n\n13\nMar 10\nNonlinear Regression and Pairs Plots\nslides\n\n\n14\nMar 12\nContour Plots, Heat Maps, and Into High-Dimensional Data\nslides\n\n\n15\nMar 17\nVisualizing Distances for High-Dimensional Data\nslides\n\n\n16\nMar 19\nDendrograms for Visualizing Distances and Clusters\nslides\n\n\n17\nMar 24\nPrincipal Component Analysis\nslides\n\n\n18\nMar 26\nVisualizing Trends\nslides\n\n\n19\nMar 31\nTime Series, Autocorrelation, and Seasonal Decomposition\nslides",
    "crumbs": [
      "Lectures"
    ]
  },
  {
    "objectID": "demos/02-plot-2dquant.html",
    "href": "demos/02-plot-2dquant.html",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs. Throughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")"
  },
  {
    "objectID": "demos/02-plot-2dquant.html#colors",
    "href": "demos/02-plot-2dquant.html#colors",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Colors",
    "text": "Colors\nWe can color by a third variable (e.g., different color for each category).\nNote that, just like the x and y aesthetics, you can put color = inside ggplot or geom_point - both display the same visualization:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm, color = species)) +\n  geom_point(alpha = 0.5)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(color = species))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nYou can also color by a quantitative variable using a color scale/gradient:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(color = body_mass_g))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe default color gradient is not the most appealing, while there are a number of possibilities - blue to orange is a good choice since these colors are opposites on the color spectrum:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(color = body_mass_g)) +\n  scale_color_gradient(low = \"darkblue\", high = \"darkorange\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#point-size-size",
    "href": "demos/02-plot-2dquant.html#point-size-size",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Point size (size)",
    "text": "Point size (size)\nWe can also map variables to other aesthetics, e.g. size:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(size = body_mass_g))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#point-type-shape",
    "href": "demos/02-plot-2dquant.html#point-type-shape",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Point type (shape)",
    "text": "Point type (shape)\nOr the type (shape) of points:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm)) +\n  geom_point(alpha = 0.5, aes(shape = species))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#combining-aesthetics",
    "href": "demos/02-plot-2dquant.html#combining-aesthetics",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Combining aesthetics",
    "text": "Combining aesthetics\nWe can even do several of these at once:\n\npenguins |&gt;\n  ggplot(aes(x = bill_depth_mm, y = bill_length_mm,\n             color = species, shape = island,\n             size = body_mass_g)) +\n  geom_point(alpha = 0.5)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe above graph may be a bit difficult to read, but it contains a lot of information in the sense that it is a 5-dimensional graphic:\n\nx = bill depth (mm)\ny = bill length (mm)\ncolor = species\nsize = body mass (g)\nshape = island\n\n\nBut be careful!\nThe more complications you add, the more difficult your graph is to explain."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#linear-regression",
    "href": "demos/02-plot-2dquant.html#linear-regression",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Linear regression",
    "text": "Linear regression\nTo do this, we can use + geom_smooth(method = lm):\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n##Linear regression (with error bars)\nAbove, I added se = FALSE so that the standard error bars do not show up in the graph. Setting this parameter to TRUE produces (by default) 95% confidence intervals.\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = TRUE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nYou can change the level of the confidence intervals using the level argument:\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = TRUE, level = 0.99)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHowever, from these graphs, it’s not clear if the linear regression is a good fit. We can “eyeball” this by looking at the fitted versus residuals, or we can make a residual-versus-fit plot.\nWhat’s a residual-versus-fit plot? In short, the “fits” are the estimated y-values from the linear regression (i.e., the y-values along the linear regression line). Meanwhile, the “residuals” are the distance between the actual y-values and the fits. A residual-versus-fit plot is itself a scatterplot, with fits on the x-axis and residuals on the y-axis.\n\nlin_reg &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\nfitted_vals &lt;- fitted(lin_reg)\nres_vals &lt;- residuals(lin_reg)\ntibble(fits = fitted_vals, \n       residuals = res_vals) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")\n\n\n\n\n\n\n\n\nWe look for two things when looking at residual-versus-fit plots:\n\nIs there any trend around the 0 horizontal line? If so, that might be a violation of the linearity assumption (more on this next class).\nDo the points have equal vertical spread from left to right? If not, that might be a violation of the equal variance assumption.\n\nWe’ll talk about these assumptions more next class."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#transformations-of-the-outcome",
    "href": "demos/02-plot-2dquant.html#transformations-of-the-outcome",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Transformations of the outcome",
    "text": "Transformations of the outcome\nWe can transform variables as well – again, within the plot. First, we will focus on the outcome y; in particular, we will focus on log transformations. This can be done through the y argument…\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = log(body_mass_g))) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = TRUE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#transformations-of-covariates",
    "href": "demos/02-plot-2dquant.html#transformations-of-covariates",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Transformations of Covariates",
    "text": "Transformations of Covariates\nIt’s also possible to include transformations of the covariates instead of (or in addition to) transformations of the outcome. For example, the following plots a quadratic regression model (i.e., plots y ~ x + x^2).\nNote that the formula argument in geom_smooth requires you to write in terms of y and x, NOT the variable names!\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", \n              formula = y ~ x + I(x^2))\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nTo assess if this is a better fit, we can again make a fitted-versus-residual plot (this looks better!):\n\nquad_lin_reg &lt;- lm(body_mass_g ~ flipper_length_mm + I(flipper_length_mm^2), \n                   data = penguins)\nquad_fitted_vals &lt;- fitted(quad_lin_reg)\nquad_res_vals &lt;- residuals(quad_lin_reg)\ntibble(fits = quad_fitted_vals, \n       residuals = quad_res_vals) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")"
  },
  {
    "objectID": "demos/02-plot-2dquant.html#non-linear-trends",
    "href": "demos/02-plot-2dquant.html#non-linear-trends",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Non-Linear Trends",
    "text": "Non-Linear Trends\nWe can also do other types of modeling, e.g. local regression / loess smoothing:\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"loess\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nCheck the help documentation for geom_smooth() and stat_smooth() to see what methods are available and how to use them. The most common choices are “lm”, “glm”, “gam”, and “loess”.\nNote that if you don’t put anything in geom_smooth, it will select “auto”, which typically uses loess for small datasets and gam for large datasets. However, it uses a particular form of smoothing splines, so in practice I recommend you specify a particular statistical method (e.g., “lm”, “loess”) so you actually know what you’re plotting."
  },
  {
    "objectID": "demos/02-plot-2dquant.html#useful-for-residual-diagnostics",
    "href": "demos/02-plot-2dquant.html#useful-for-residual-diagnostics",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Useful for residual diagnostics",
    "text": "Useful for residual diagnostics\nConvenient to add + geom_smooth() to residual plots to help display any trends:\n\ntibble(fits = fitted_vals, \n       residuals = res_vals) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nwhich appears to be alleviated with the quadratic transformation:\n\ntibble(fits = quad_fitted_vals, \n       residuals = quad_res_vals) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "demos/02-plot-2dquant.html#leave-the-points-take-the-regression-model-this-is-a-bad-idea",
    "href": "demos/02-plot-2dquant.html#leave-the-points-take-the-regression-model-this-is-a-bad-idea",
    "title": "Demo 02: Scatterplots and Linear Regression",
    "section": "Leave The Points, take The Regression Model? (This is a bad idea…)",
    "text": "Leave The Points, take The Regression Model? (This is a bad idea…)\nWe don’t even need to plot the points to do this – you can plot the regression model by itself by simply omitting geom_point():\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_smooth(method = \"loess\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\nAs always, you can adjust some parameters (like color, alpha, etc.):\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_smooth(method = \"loess\", se = TRUE, fill = \"darkorange\", \n              color = \"darkblue\", size = 2, alpha = 0.2) +\n  theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\nHowever, in general I don’t recommend doing this, because it hides the data entirely, making it unclear which data points are influencing the regression line."
  },
  {
    "objectID": "demos/09-trends.html",
    "href": "demos/09-trends.html",
    "title": "Demo 09: Visualizing trends",
    "section": "",
    "text": "In this demo, we’ll first work with a dataset on the number of PhD degrees awarded in the US from TidyTuesday.\n\n# Read in the tidytuesday data\nlibrary(tidyverse)\nphd_field &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv\")\nphd_field\n\n# A tibble: 3,370 × 5\n   broad_field   major_field                                 field   year n_phds\n   &lt;chr&gt;         &lt;chr&gt;                                       &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Life sciences Agricultural sciences and natural resources Agric…  2008    111\n 2 Life sciences Agricultural sciences and natural resources Agric…  2008     28\n 3 Life sciences Agricultural sciences and natural resources Agric…  2008      3\n 4 Life sciences Agricultural sciences and natural resources Agron…  2008     68\n 5 Life sciences Agricultural sciences and natural resources Anima…  2008     41\n 6 Life sciences Agricultural sciences and natural resources Anima…  2008     18\n 7 Life sciences Agricultural sciences and natural resources Anima…  2008     77\n 8 Life sciences Agricultural sciences and natural resources Envir…  2008    182\n 9 Life sciences Agricultural sciences and natural resources Fishi…  2008     52\n10 Life sciences Agricultural sciences and natural resources Food …  2008     96\n# ℹ 3,360 more rows\n\n\nLet’s start by grabbing the rows corresponding to Statistics PhDs. While there are a number of ways to do this, we can grab field containing “statistics” (including biostatistics) with the str_detect() function.\n\nstats_phds &lt;- phd_field |&gt;\n  filter(str_detect(tolower(field), \"statistics\"))\n\nWhat are the different fields that were captured?\n\ntable(stats_phds$field)\n\n\n                       Biometrics and biostatistics \n                                                 10 \n           Educational statistics, research methods \n                                                 10 \nManagement information systems, business statistics \n                                                 10 \n                Mathematics and statistics, general \n                                                 10 \n                  Mathematics and statistics, other \n                                                 10 \n                           Statistics (mathematics) \n                                                 10 \n                       Statistics (social sciences) \n                                                 10 \n\n\nTo start, let’s just summarize the number of PhDs by year:\n\nstat_phd_year_summary &lt;- stats_phds |&gt;\n  group_by(year) |&gt;\n  summarize(n_phds = sum(n_phds))\n\nNow, we’ll make the typical scatterplot display with n_phds on the y-axis and year on the x-axis:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nWe should fix our x-axis here and make the breaks more informative. In this case, I’ll change it so each year is labeled (that may not be appropriate for every visual but it works out here).\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  # Modify the x-axis to make the axis breaks at the unique years and show their\n  # respective labels\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nTo emphasize the ordering of the year along the x-axis, we’ll add a line connecting the points to emphasize the order:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nWe can drop the points, leaving only the connecting lines to emphasize trends:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nAnother common way to display trends is by filling in the area under the line. However, this is only appropriate when the y-axis starts at 0! It’s also redundant use of ink so just be careful when deciding whether or not to fill the area. We can fill the area under the line with the geom_area() aesthetic - but note that it changes the y-axis by default to start at 0:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  # Fill the area under the line\n  geom_area(fill = \"darkblue\", alpha = 0.5) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nYou can also make this plot using the ggridges package."
  },
  {
    "objectID": "demos/09-trends.html#phds-awarded-by-field-over-time",
    "href": "demos/09-trends.html#phds-awarded-by-field-over-time",
    "title": "Demo 09: Visualizing trends",
    "section": "",
    "text": "In this demo, we’ll first work with a dataset on the number of PhD degrees awarded in the US from TidyTuesday.\n\n# Read in the tidytuesday data\nlibrary(tidyverse)\nphd_field &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv\")\nphd_field\n\n# A tibble: 3,370 × 5\n   broad_field   major_field                                 field   year n_phds\n   &lt;chr&gt;         &lt;chr&gt;                                       &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Life sciences Agricultural sciences and natural resources Agric…  2008    111\n 2 Life sciences Agricultural sciences and natural resources Agric…  2008     28\n 3 Life sciences Agricultural sciences and natural resources Agric…  2008      3\n 4 Life sciences Agricultural sciences and natural resources Agron…  2008     68\n 5 Life sciences Agricultural sciences and natural resources Anima…  2008     41\n 6 Life sciences Agricultural sciences and natural resources Anima…  2008     18\n 7 Life sciences Agricultural sciences and natural resources Anima…  2008     77\n 8 Life sciences Agricultural sciences and natural resources Envir…  2008    182\n 9 Life sciences Agricultural sciences and natural resources Fishi…  2008     52\n10 Life sciences Agricultural sciences and natural resources Food …  2008     96\n# ℹ 3,360 more rows\n\n\nLet’s start by grabbing the rows corresponding to Statistics PhDs. While there are a number of ways to do this, we can grab field containing “statistics” (including biostatistics) with the str_detect() function.\n\nstats_phds &lt;- phd_field |&gt;\n  filter(str_detect(tolower(field), \"statistics\"))\n\nWhat are the different fields that were captured?\n\ntable(stats_phds$field)\n\n\n                       Biometrics and biostatistics \n                                                 10 \n           Educational statistics, research methods \n                                                 10 \nManagement information systems, business statistics \n                                                 10 \n                Mathematics and statistics, general \n                                                 10 \n                  Mathematics and statistics, other \n                                                 10 \n                           Statistics (mathematics) \n                                                 10 \n                       Statistics (social sciences) \n                                                 10 \n\n\nTo start, let’s just summarize the number of PhDs by year:\n\nstat_phd_year_summary &lt;- stats_phds |&gt;\n  group_by(year) |&gt;\n  summarize(n_phds = sum(n_phds))\n\nNow, we’ll make the typical scatterplot display with n_phds on the y-axis and year on the x-axis:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nWe should fix our x-axis here and make the breaks more informative. In this case, I’ll change it so each year is labeled (that may not be appropriate for every visual but it works out here).\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  # Modify the x-axis to make the axis breaks at the unique years and show their\n  # respective labels\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nTo emphasize the ordering of the year along the x-axis, we’ll add a line connecting the points to emphasize the order:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nWe can drop the points, leaving only the connecting lines to emphasize trends:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nAnother common way to display trends is by filling in the area under the line. However, this is only appropriate when the y-axis starts at 0! It’s also redundant use of ink so just be careful when deciding whether or not to fill the area. We can fill the area under the line with the geom_area() aesthetic - but note that it changes the y-axis by default to start at 0:\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  # Fill the area under the line\n  geom_area(fill = \"darkblue\", alpha = 0.5) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\n\n\n\n\n\n\n\nYou can also make this plot using the ggridges package."
  },
  {
    "objectID": "demos/09-trends.html#plotting-and-labeling-several-lines",
    "href": "demos/09-trends.html#plotting-and-labeling-several-lines",
    "title": "Demo 09: Visualizing trends",
    "section": "Plotting and labeling several lines",
    "text": "Plotting and labeling several lines\nWe’ll now switch to displaying the different Statistics fields separately with the stats_phds dataset. First, we should NOT display multiple time series with just points as follows:\n\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_point() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  theme(legend.position = \"bottom\",\n        # Adjust the size of the legend's text\n        legend.text = element_text(size = 5),\n        legend.title = element_text(size = 6)) +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\",\n       color = \"Field\")\n\n\n\n\n\n\n\n\nIt’s much simpler to just display the lines to compare the trends:\n\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  theme(legend.position = \"bottom\",\n        # Adjust the size of the legend's text\n        legend.text = element_text(size = 5),\n        legend.title = element_text(size = 6)) +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\",\n       color = \"Field\")\n\n\n\n\n\n\n\n\nThe legend is pretty cluttered though, instead we can directly label the displayed lines using the ggrepel package. We first need to create a dataset with just the final values (which in this case corresponds to year == 2017), and then add labels for these values. To make the labels visible, we need to increase our x-axis limits. Note that this is a “hack”, but you will rely on hacks to customize visuals in the future… The following code chunk demonstrates how to do this:\n\nstats_phds_2017 &lt;- stats_phds |&gt;\n  filter(year == 2017)\n\n# Access the ggrepel package:\n# install.packages(\"ggrepel\")\nlibrary(ggrepel)\n\nWarning: package 'ggrepel' was built under R version 4.2.3\n\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  # Add the labels:\n  geom_text_repel(data = stats_phds_2017,\n                  aes(label = field),\n                  size = 2, \n                  # Drop the segment connection:\n                  segment.color = NA, \n                  # Move labels up or down based on overlap\n                  direction = \"y\",\n                  # Try to align the labels horizontally on the left hand side\n                  hjust = \"left\") +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year),\n                     # Update the limits so that there is some padding on the\n                     # x-axis but don't label the new maximum\n                     limits = c(min(stat_phd_year_summary$year),\n                                max(stat_phd_year_summary$year) + 3)) +\n  theme_bw() +\n  # Drop the legend\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\",\n       color = \"Field\")\n\n\n\n\n\n\n\n\nAn alternative approach is to use the gghighlight package:\n\nlibrary(gghighlight)\n\nWarning: package 'gghighlight' was built under R version 4.2.3\n\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  gghighlight()  +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")\n\nlabel_key: field\n\n\n\n\n\n\n\n\n\nNext, let’s switch to back to the original dataset phd_field. What happens if we plot a line for every field attempting to use the color aesthetic to separate them?\n\nphd_field |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\",\n       color = \"Field\")\n\nWarning: Removed 270 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThe plot above is obviously a disaster… When we are dealing with potentially way too many categories, we can instead highlight lines of interest while setting the background lines to gray, so we can still see background trends. We need to use the group aesthetic to split the gray lines from each other. Plus, we should adjust the alpha due to the overlap. The following code chunk demonstrates how to do this for highlighting the “Statistics (mathematics)” and “Biometrics and biostatistics” lines. We essentially create separate plot layers by filtering on the field variable:\n\n# First display the background lines using the full dataset with those two fields \n# filtered out:\nphd_field |&gt;\n  # The following line says: NOT (field in c(\"Biometrics and biostatistics\", \"Statistics (mathematics)\"))\n  filter(!(field %in% c(\"Biometrics and biostatistics\", \n                        \"Statistics (mathematics)\"))) |&gt;\n  ggplot() +\n  # Add the background lines - need to specify the group to be the field\n  geom_line(aes(x = year, y = n_phds, group = field),\n            color = \"gray\", size = .5, alpha = .5) +\n  # Now add the layer with the lines of interest:\n  geom_line(data = filter(phd_field,\n                          # Note this is just the opposite of the above since ! is removed\n                          field %in% c(\"Biometrics and biostatistics\", \n                                       \"Statistics (mathematics)\")),\n            aes(x = year, y = n_phds, color = field),\n            # Make the size larger\n            size = .75, alpha = 1) +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_bw() +\n  theme(legend.position = \"bottom\", \n        # Drop the panel lines making the gray difficult to see\n        panel.grid = element_blank()) +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\",\n       color = \"Field\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 270 rows containing missing values or values outside the scale range\n(`geom_line()`)."
  },
  {
    "objectID": "demos/09-trends.html#florence-nightingales-rose-diagrams",
    "href": "demos/09-trends.html#florence-nightingales-rose-diagrams",
    "title": "Demo 09: Visualizing trends",
    "section": "Florence Nightingale’s rose diagrams",
    "text": "Florence Nightingale’s rose diagrams\nAnother way to visualize time series data is to display it in a cycle pattern, using polar coordinates, as done by Florence Nightingale’s famous rose diagram. We can recreate the rose diagram by accessing the data in the HistData package. We’ll first load and print out the first so many rows of the data below:\n\nlibrary(HistData)\nhead(Nightingale)\n\n        Date Month Year  Army Disease Wounds Other Disease.rate Wounds.rate\n1 1854-04-01   Apr 1854  8571       1      0     5          1.4         0.0\n2 1854-05-01   May 1854 23333      12      0     9          6.2         0.0\n3 1854-06-01   Jun 1854 28333      11      0     6          4.7         0.0\n4 1854-07-01   Jul 1854 28722     359      0    23        150.0         0.0\n5 1854-08-01   Aug 1854 30246     828      1    30        328.5         0.4\n6 1854-09-01   Sep 1854 30290     788     81    70        312.2        32.1\n  Other.rate\n1        7.0\n2        4.6\n3        2.5\n4        9.6\n5       11.9\n6       27.7\n\n\nTo recreate the plot, we’ll need to first make a longer version of the dataset with the Disease, Wounds, and Other columns separated into three rows. To do that, we’ll use the pivot_longer() function after just selecting the columns of interest for our plot:\n\ncrimean_war_data &lt;- Nightingale |&gt;\n  dplyr::select(Date, Month, Year, Disease, Wounds, Other) |&gt;\n  # Now pivot those columns to take up separate rows:\n  pivot_longer(Disease:Other,\n               names_to = \"cause\", values_to = \"count\")\n\nNext, we’ll make a label column matching Nightingale’s plot based on the Date column. We’ll talk about dates more below, but we can condition on being above or below certain dates in a natural way:\n\ncrimean_war_data &lt;- crimean_war_data |&gt;\n  mutate(time_period = ifelse(Date &lt;= as.Date(\"1855-03-01\"),\n                              \"April 1854 to March 1855\", \n                              \"April 1855 to March 1856\"))\n\nAnd finally we can go ahead and display the rose diagram facetted by the time period (using similar colors to Nightingale):\n\ncrimean_war_data |&gt; \n  ggplot(aes(x = Month, y = count)) + \n  geom_col(aes(fill = cause), width = 1, \n           position = \"identity\", alpha = 0.5) + \n  coord_polar() + \n  facet_wrap(~ time_period, ncol = 2) +\n  scale_fill_manual(values = c(\"skyblue3\", \"grey30\", \"firebrick\")) +\n  scale_y_sqrt() +\n  theme_void() +\n  # All of this below is to just customize the theme in a way that we are\n  # close to resembling the original plot (ie lets make it look old!)\n  theme(axis.text.x = element_text(size = 9),\n        strip.text = element_text(size = 11),\n        legend.position = \"bottom\",\n        plot.background = element_rect(fill = alpha(\"cornsilk\", 0.5)),\n        plot.margin = unit(c(10, 10, 10, 10), \"pt\"),\n        plot.title = element_text(vjust = 5)) +\n  labs(title = \"Diagram of the Causes of Mortality in the Army in the East\")\n\n\n\n\n\n\n\n\nThis looks pretty close to the original diagram, except the order of the months does not match the original. We can of course change that by reordering the factor variable:\n\ncrimean_war_data |&gt; \n  # Manually relevel it to match the original plot\n  mutate(Month = fct_relevel(Month, \n                             \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\",\n                             \"Dec\", \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\")) |&gt;\n  ggplot(aes(x = Month, y = count)) + \n  geom_col(aes(fill = cause), width = 1, \n           position = \"identity\", alpha = 0.5) + \n  coord_polar() + \n  facet_wrap(~ time_period, ncol = 2) +\n  scale_fill_manual(values = c(\"skyblue3\", \"grey30\", \"firebrick\")) +\n  scale_y_sqrt() +\n  theme_void() +\n  # All of this below is to just customize the theme in a way that we are\n  # close to resembling the original plot (ie lets make it look old!)\n  theme(axis.text.x = element_text(size = 9),\n        strip.text = element_text(size = 11),\n        legend.position = \"bottom\",\n        plot.background = element_rect(fill = alpha(\"cornsilk\", 0.5)),\n        plot.margin = unit(c(10, 10, 10, 10), \"pt\"),\n        plot.title = element_text(vjust = 5)) +\n  labs(title = \"Diagram of the Causes of Mortality in the Army in the East\")\n\n\n\n\n\n\n\n\nHow does this compare to just a simple line graph?\n\ncrimean_war_data |&gt; \n  ggplot(aes(x = Date, y = count, color = cause)) + \n  geom_line() +\n  # Add a reference line at the cutoff point\n  geom_vline(xintercept = as.Date(\"1855-03-01\"), linetype = \"dashed\",\n             color = \"gray\") +\n  scale_color_manual(values = c(\"skyblue3\", \"grey30\", \"firebrick\")) +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Diagram of the Causes of Mortality in the Army in the East\",\n       y = \"Counts\", x = \"Date\")\n\n\n\n\n\n\n\n\nWe can customize the x-axis further using scale_x_date():\n\ncrimean_war_data |&gt; \n  ggplot(aes(x = Date, y = count, color = cause)) + \n  geom_line() +\n  # Add a reference line at the cutoff point\n  geom_vline(xintercept = as.Date(\"1855-03-01\"), linetype = \"dashed\",\n             color = \"gray\") +\n  scale_color_manual(values = c(\"skyblue3\", \"grey30\", \"firebrick\")) +\n  # Format to use abbreviate month %b with year %Y\n  scale_x_date(date_labels = \"%b %Y\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Diagram of the Causes of Mortality in the Army in the East\",\n       y = \"Counts\", x = \"Date\")\n\n\n\n\n\n\n\n\nWhich one do you prefer? Maybe filling the area under the lines would be better here…"
  },
  {
    "objectID": "demos/06-mds.html",
    "href": "demos/06-mds.html",
    "title": "Demo 06: Visualizing Distances for High-Dimensional Data",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs.\nThroughout this demo we use a dataset about Starbucks drinks available in the #TidyTuesday project.\nYou can read in and manipulate various columns in the dataset with the following code:\nlibrary(tidyverse)\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g),\n         fiber_g = as.numeric(fiber_g))"
  },
  {
    "objectID": "demos/06-mds.html#so-far",
    "href": "demos/06-mds.html#so-far",
    "title": "Demo 06: Visualizing Distances for High-Dimensional Data",
    "section": "So far…",
    "text": "So far…\nWe’ve been working with “tidy data” – data that has \\(n\\) rows and \\(p\\) columns, where each row is an observation, and each column is a variable describing some feature of each observation.\nNow we’ll discuss more complicated data structures."
  },
  {
    "objectID": "demos/06-mds.html#distance-matrices",
    "href": "demos/06-mds.html#distance-matrices",
    "title": "Demo 06: Visualizing Distances for High-Dimensional Data",
    "section": "Distance matrices",
    "text": "Distance matrices\nA distance matrix is a data structure that specifies the “distance” between each pair of observations in the original \\(n\\)-row, \\(p\\)-column dataset. For each pair of observations (e.g. \\(x_i, x_j\\)) in the original dataset, we compute the distance between those observations, denoted as \\(d(x_i, x_j)\\) or \\(d_{ij}\\) for short.\nA variety of approaches for calculating the distance between a pair of observations can be used. The most commonly used approach (when we have quantitative variables) is called “Euclidean Distance”. The Euclidean distance between observations \\(x_i\\) and \\(x_j\\) is defined as follows: \\(d(x_i, x_j) = \\sqrt{\\sum_{l = 1}^p (x_{i,l} - x_{j,l}) ^ 2}\\). That is, it is the square root of the sum of squared differences between each column (\\(l \\in \\{1, ..., p\\}\\)) of \\(x_i\\) and \\(x_j\\) (remember, there are \\(p\\) original columns / variables).\nNote that if some variables in our dataset have substantially higher variance than others, the high-variance variables will dominate the calculation of distance, skewing our resulting distances towards the differences in these variables. As such, it’s common to scale the original dataset before calculating the distance, so that each variable is on the same scale."
  },
  {
    "objectID": "demos/06-mds.html#starbucks-drinks-dataset",
    "href": "demos/06-mds.html#starbucks-drinks-dataset",
    "title": "Demo 06: Visualizing Distances for High-Dimensional Data",
    "section": "Starbucks drinks dataset",
    "text": "Starbucks drinks dataset\nIn this R demo we will look at Starbucks drinks (courtesy of the #TidyTuesday project). In short, this is a dataset containing nutritional information about Starbucks drinks. We’re going to consider all of the quantitative variables in this dataset, starting with fifth column serv_size_m_l to the final column caffeine_mg. You can read about the columns in the dataset here. After selecting the desired columns, the first thing we’re going to do is use the scale() function to ensure each variable on the same scale, i.e., variances are equal to 1.\n\n# Select the variables of interest:\nstarbucks_quant_data &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg)\n# Now scale each column so that the variance is 1 using the scale function:\n# We specify here to not center the data and need to follow the directions in\n# the help page of scale to ensure we are properly standardizing the variance\nstarbucks_scaled_quant_data &lt;- \n  scale(starbucks_quant_data, center = FALSE, \n        scale = apply(starbucks_quant_data,\n                      2, sd, na.rm = TRUE)) \n\n# Just for reference - this is equivalent to the following commented out code:\n# starbucks_quant_data &lt;- starbucks |&gt;\n#   dplyr::select(serv_size_m_l:caffeine_mg)\n# starbucks_scaled_quant_data &lt;- apply(starbucks_quant_data, MARGIN = 2,\n#                                      FUN = function(x) x / sd(x))\n\nThe most common way to compute distances in R is to use the dist function. This takes in a dataset and returns the distance matrix for that dataset. By default this computes the euclidean distance (method = \"euclidean\"), but other distance metrics can be used.\n\n# Calculate distance matrix.\n# As an example, we'll just look at the first five rows:\ndist(starbucks_scaled_quant_data[1:5,])\n\n         1        2        3        4\n2 1.059790                           \n3 2.160501 1.101588                  \n4 3.388562 2.331643 1.232345         \n5 1.472299 2.380300 3.425819 4.643997\n\n# You can also include the diagonal if you want:\n# (the diagonal will always be 0s)\ndist(starbucks_scaled_quant_data[1:5,], diag = T)\n\n         1        2        3        4        5\n1 0.000000                                    \n2 1.059790 0.000000                           \n3 2.160501 1.101588 0.000000                  \n4 3.388562 2.331643 1.232345 0.000000         \n5 1.472299 2.380300 3.425819 4.643997 0.000000\n\n# You can also include the \"upper triangle\" if you want:\ndist(starbucks_scaled_quant_data[1:5,], upper = T)\n\n         1        2        3        4        5\n1          1.059790 2.160501 3.388562 1.472299\n2 1.059790          1.101588 2.331643 2.380300\n3 2.160501 1.101588          1.232345 3.425819\n4 3.388562 2.331643 1.232345          4.643997\n5 1.472299 2.380300 3.425819 4.643997         \n\n# Can also include both:\n# (this is the full distance matrix)\ndist(starbucks_scaled_quant_data[1:5,], diag = T, upper = T)\n\n         1        2        3        4        5\n1 0.000000 1.059790 2.160501 3.388562 1.472299\n2 1.059790 0.000000 1.101588 2.331643 2.380300\n3 2.160501 1.101588 0.000000 1.232345 3.425819\n4 3.388562 2.331643 1.232345 0.000000 4.643997\n5 1.472299 2.380300 3.425819 4.643997 0.000000\n\n# Can also consider other distance metrics\n# The default is euclidean, as you can see below:\n# (compare to what you see above)\ndist(starbucks_scaled_quant_data[1:5,], method = \"euclidean\", diag = T, upper = T)\n\n         1        2        3        4        5\n1 0.000000 1.059790 2.160501 3.388562 1.472299\n2 1.059790 0.000000 1.101588 2.331643 2.380300\n3 2.160501 1.101588 0.000000 1.232345 3.425819\n4 3.388562 2.331643 1.232345 0.000000 4.643997\n5 1.472299 2.380300 3.425819 4.643997 0.000000\n\n# For example, can consider the Manhattan distance:\ndist(starbucks_scaled_quant_data[1:5,], method = \"manhattan\", diag = T, upper = T)\n\n         1        2        3        4        5\n1 0.000000 1.552865 3.109038 4.818573 1.472299\n2 1.552865 0.000000 1.556173 3.265708 3.025165\n3 3.109038 1.556173 0.000000 1.709535 4.581337\n4 4.818573 3.265708 1.709535 0.000000 6.290872\n5 1.472299 3.025165 4.581337 6.290872 0.000000\n\n\nFor the purposes of this class, we’ll mostly focus on the Euclidean distance, so let’s define that here:\n\ndist_euc &lt;- dist(starbucks_scaled_quant_data)"
  },
  {
    "objectID": "demos/06-mds.html#implementing-multi-dimensional-scaling",
    "href": "demos/06-mds.html#implementing-multi-dimensional-scaling",
    "title": "Demo 06: Visualizing Distances for High-Dimensional Data",
    "section": "Implementing multi-dimensional scaling",
    "text": "Implementing multi-dimensional scaling\nNow we will implement multi-dimensional scaling (MDS) in R. As a reminder, MDS tries to find the “best” \\(k\\)-dimensional projection of the original \\(p\\)-dimensional dataset (\\(k &lt; p\\)).\nAs such, MDS tries to preserve the order of the pairwise distances. That is, pairs of observations with low distances in the original \\(p\\)-column dataset will still be have low distances in the smaller \\(k\\)-column dataset. Similarly, pairs of observations with high distances in the original \\(p\\)-column dataset will still be have high distances in the smaller \\(k\\)-column dataset.\nMDS can be implemented in R using the cmdscale function. This function takes a distance matrix (not a dataset!!):\n\nstarbucks_mds &lt;- cmdscale(d = dist_euc, k = 2)\n\nNote that you can change \\(k\\) to be greater than 2 if you want, but usually we want \\(k = 2\\) so that we can plot the (projected) distances in a scatterplot; see below.\nFor the purposes of plotting, let’s add the two coordinates of mds to our original dataset:\n\nstarbucks &lt;- starbucks |&gt;\n  mutate(mds1 = starbucks_mds[,1], mds2 = starbucks_mds[,2])\n\nThen, we can make a plot with ggplot:\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\")\n\n\n\n\n\n\n\n\nIt can be helpful to add colors and/or shapes of the plot according to categorical variables. For example, here’s the plot colored by size:\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2, color = size)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\")\n\n\n\n\n\n\n\n\nTo get some insight into the contributions by the different continous variables, we could also map them to various aesthetics. For example, the following plots displays points colored by sugar_g:\n\nstarbucks |&gt;\n  ggplot(aes(x = mds1, y = mds2, color = sugar_g)) +\n  geom_point(alpha = 0.5) +\n  scale_color_gradient(low = \"darkblue\", high = \"darkorange\") +\n  labs(x = \"Coordinate 1\", y = \"Coordinate 2\")\n\n\n\n\n\n\n\n\nWhat do these two colored plots tell us about the data?"
  },
  {
    "objectID": "demos/05-heatmap-highdim.html",
    "href": "demos/05-heatmap-highdim.html",
    "title": "Demo 05: Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs.\nFor the first part of this demo we’ll use a dataset of shots by LeBron James. For people that are interested, this was constructed using the hoopR package.\nYou can read in the dataset with the following code:\n\nlibrary(tidyverse)\nlebron_shots &lt;- read_csv(\"https://raw.githubusercontent.com/ryurko/DataViz-Class-Data/main/lebron_shots.csv\")\n\n\n\nWe’re all experts in 1D kernel density estimation by now. Let’s move on to 2D kernel density estimation.\n\n\nHere let’s focus on plotting the joint distribution of coordinate_x and coordinate_y, which are both quantitative (i.e., observing the joint distribution of shots). We’re already very familiar with how to make scatterplots:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = .5)\n\n\n\n\n\n\n\n\nIt’s really easy to add a two-dimensional density (via contour lines) to the plot: we just use geom_density2d():\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = .5) +\n  geom_density2d()\n\n\n\n\n\n\n\n\nSimilar to the contour lines on a topological map, the inner lines denote the “peaks” of the density. Note that the contour lines won’t necessarily encapsulate every data point.\nWe can also plot the contour lines without the points if you’d like (see below), but this is a bit misleading, because it automatically throws out areas of the plot where there were points but the density was low. To see this, compare the plot below to the plot above.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_density2d()\n\n\n\n\n\n\n\n\n\n\n\nFor example, we can change the fill type, which gives two benefits: (1) It looks cooler, and (2) Now we can see what the actual density values are.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(level)), geom = \"polygon\") + \n  geom_point(alpha = .5) #+\n\n\n\n\n\n\n\n  #scale_fill_gradient(low = \"darkblue\", high = \"darkorange\")\n\nNote: To change the color, you can uncomment the code above. This uses the scale_fill_gradient() function, which we’ve seen before in previous homeworks.\nWe might also want to change the bandwidth. In 2D kernel density estimation, we must specify two bandwidths – one for the x-direction, one for the y-direction. We’ll see how to do this in homework.\n\n\n\nHeat maps: Divide the space into a grid and color the grid according to high/low values.\nTo do this with densities, include fill = after_stat(density), geom = \"tile\", contour = FALSE in your call to stat_density2d, as below:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(density)), geom = \"tile\",\n                 contour = FALSE) + \n  geom_point(alpha = .5)\n\n\n\n\n\n\n\n\nAgain, I recommend changing the default color scheme (it’s pretty awful…), as below:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(density)), geom = \"tile\",\n                 contour = FALSE) + \n  geom_point(alpha = .5) + \n  scale_fill_gradient(low = \"white\", high = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nWe make hexagonal heatmap plots using geom_hex(), can specify binwidth in both directions. This avoids limitations and issues with smoothing and challenges with multivariate density estimation. Note: You need to have the hexbin package installed prior to creating these visuals.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_hex() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nUnrelated to 2D density estimation and viewing the joint frequency of points, we can alternatively view some statistical summary within various hexagonal bins displayed on two axes of interest. For example, the following graph displays the percentage of shots made within each hexagonal bin. We do this by mapping as.numeric(scoring_play) to the z aesthetic (since scoring_play is a boolean TRUE/FALSE and as.numeric() converts it to 1/0) and using the stat_summary_hex() layer with a specified function via fun = mean.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y, \n             z = as.numeric(scoring_play))) +\n  stat_summary_hex(fun = mean) +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "demos/05-heatmap-highdim.html#d-density-estimates",
    "href": "demos/05-heatmap-highdim.html#d-density-estimates",
    "title": "Demo 05: Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "",
    "text": "We’re all experts in 1D kernel density estimation by now. Let’s move on to 2D kernel density estimation.\n\n\nHere let’s focus on plotting the joint distribution of coordinate_x and coordinate_y, which are both quantitative (i.e., observing the joint distribution of shots). We’re already very familiar with how to make scatterplots:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = .5)\n\n\n\n\n\n\n\n\nIt’s really easy to add a two-dimensional density (via contour lines) to the plot: we just use geom_density2d():\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_point(alpha = .5) +\n  geom_density2d()\n\n\n\n\n\n\n\n\nSimilar to the contour lines on a topological map, the inner lines denote the “peaks” of the density. Note that the contour lines won’t necessarily encapsulate every data point.\nWe can also plot the contour lines without the points if you’d like (see below), but this is a bit misleading, because it automatically throws out areas of the plot where there were points but the density was low. To see this, compare the plot below to the plot above.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_density2d()\n\n\n\n\n\n\n\n\n\n\n\nFor example, we can change the fill type, which gives two benefits: (1) It looks cooler, and (2) Now we can see what the actual density values are.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(level)), geom = \"polygon\") + \n  geom_point(alpha = .5) #+\n\n\n\n\n\n\n\n  #scale_fill_gradient(low = \"darkblue\", high = \"darkorange\")\n\nNote: To change the color, you can uncomment the code above. This uses the scale_fill_gradient() function, which we’ve seen before in previous homeworks.\nWe might also want to change the bandwidth. In 2D kernel density estimation, we must specify two bandwidths – one for the x-direction, one for the y-direction. We’ll see how to do this in homework.\n\n\n\nHeat maps: Divide the space into a grid and color the grid according to high/low values.\nTo do this with densities, include fill = after_stat(density), geom = \"tile\", contour = FALSE in your call to stat_density2d, as below:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(density)), geom = \"tile\",\n                 contour = FALSE) + \n  geom_point(alpha = .5)\n\n\n\n\n\n\n\n\nAgain, I recommend changing the default color scheme (it’s pretty awful…), as below:\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  stat_density2d(aes(fill = after_stat(density)), geom = \"tile\",\n                 contour = FALSE) + \n  geom_point(alpha = .5) + \n  scale_fill_gradient(low = \"white\", high = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nWe make hexagonal heatmap plots using geom_hex(), can specify binwidth in both directions. This avoids limitations and issues with smoothing and challenges with multivariate density estimation. Note: You need to have the hexbin package installed prior to creating these visuals.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y)) +\n  geom_hex() +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "demos/05-heatmap-highdim.html#bonus-statistical-summaries-within-hexagonal-bins",
    "href": "demos/05-heatmap-highdim.html#bonus-statistical-summaries-within-hexagonal-bins",
    "title": "Demo 05: Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "",
    "text": "Unrelated to 2D density estimation and viewing the joint frequency of points, we can alternatively view some statistical summary within various hexagonal bins displayed on two axes of interest. For example, the following graph displays the percentage of shots made within each hexagonal bin. We do this by mapping as.numeric(scoring_play) to the z aesthetic (since scoring_play is a boolean TRUE/FALSE and as.numeric() converts it to 1/0) and using the stat_summary_hex() layer with a specified function via fun = mean.\n\nlebron_shots |&gt;\n  ggplot(aes(x = coordinate_x, y = coordinate_y, \n             z = as.numeric(scoring_play))) +\n  stat_summary_hex(fun = mean) +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "demos/05-heatmap-highdim.html#correlograms-with-ggcorrplot",
    "href": "demos/05-heatmap-highdim.html#correlograms-with-ggcorrplot",
    "title": "Demo 05: Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Correlograms with ggcorrplot",
    "text": "Correlograms with ggcorrplot\nWe can visualize the correlation matrix for the variables in a dataset using the ggcorrplot package. You need to install the package:\n\ninstall.packages(\"ggcorrplot\")\n\nNext, we’ll load the package and create a correlogram using only the continuous variables. To do this, we first need to compute the correlation matrix for these variables:\n\npenguins_cor_matrix &lt;- penguins |&gt;\n  dplyr::select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) |&gt;\n  cor(use = \"complete.obs\")\npenguins_cor_matrix\n\n                  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\nbill_length_mm         1.0000000    -0.2350529         0.6561813   0.5951098\nbill_depth_mm         -0.2350529     1.0000000        -0.5838512  -0.4719156\nflipper_length_mm      0.6561813    -0.5838512         1.0000000   0.8712018\nbody_mass_g            0.5951098    -0.4719156         0.8712018   1.0000000\n\n\nNOTE: Since there are missing values in the penguins data we need to indicate in the cor() function how to handle missing values using the use argument. By default, the correlations are returned as NA, which is not what we want. Instead, we can change this to only use observations without NA values for the considered columns (see help(cor) for more options).\nNow, we can create the correlogram using ggcorrplot() using this correlation matrix:\n\nlibrary(ggcorrplot)\nggcorrplot(penguins_cor_matrix)\n\n\n\n\n\n\n\n\nThere are several ways we can improve this correlogram:\n\nwe can avoid redundancy by only using one half of matrix by changing the type input: the default is full, we can make it lower or upper instead:\n\n\nggcorrplot(penguins_cor_matrix, type = \"lower\")\n\n\n\n\n\n\n\n\n\nwe can rearrange the variables using hierarchical clustering so that variables displaying stronger levels of correlation are closer together along the diagonal by setting hc.order = TRUE:\n\n\nggcorrplot(penguins_cor_matrix, type = \"lower\", hc.order = TRUE)\n\n\n\n\n\n\n\n\n\nif we want to add the correlation values directly to the plot, we can include those labels setting lab = TRUE - but we should round the correlation values first using the round() function:\n\n\nggcorrplot(round(penguins_cor_matrix, digits = 4), \n           type = \"lower\", hc.order = TRUE, lab = TRUE)\n\n\n\n\n\n\n\n\n\nif we want to place more stress on the correlation magnitude, we can change the method input to circle so that the size of the displayed circles is mapped to the absolute value of the correlation value:\n\n\nggcorrplot(penguins_cor_matrix, type = \"lower\", hc.order = TRUE,\n           method = \"circle\")\n\n\n\n\n\n\n\n\nYou can ignore the Warning message that is displayed - just from the differences in ggplot implementation."
  },
  {
    "objectID": "demos/05-heatmap-highdim.html#parallel-coordinates-plot-with-ggally",
    "href": "demos/05-heatmap-highdim.html#parallel-coordinates-plot-with-ggally",
    "title": "Demo 05: Contour Plots, Heat Maps, and Into High-Dimensional Data",
    "section": "Parallel coordinates plot with GGally",
    "text": "Parallel coordinates plot with GGally\nIn a parallel coordinates plot, we create an axis for each varaible and align these axes side-by-side, drawing lines between observations from one axis to the next. This can be useful for visualizing structure among both the variables and observations in our dataset. These are useful when working with a moderate number of observations and variables - but can be overwhelming with too many.\nWe use the ggparcoord() function from the GGally package to make parallel coordinates plots:\n\nlibrary(GGally)\n\nWarning: package 'GGally' was built under R version 4.2.3\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\npenguins |&gt;\n  ggparcoord(columns = 3:6)\n\n\n\n\n\n\n\n\nThere are several ways we can modify this parallel coordinates plot:\n\nwe should always adjust the transparency of the lines using the alphaLines input to help handle overlap:\n\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2)\n\n\n\n\n\n\n\n\n\nwe can color each observation’s lines by a categorical variable, which can be useful for revealing group structure:\n\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2, groupColumn = \"species\")\n\n\n\n\n\n\n\n\n\nwe can change how the y-axis is constructed by modifying the scale input, which by default is std that is simply subtracting the mean and dividing by the standard deviation. We could instead use uniminmax so that minimum of the variable is zero and the maximum is one:\n\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2, groupColumn = \"species\",\n             scale = \"uniminmax\")\n\n\n\n\n\n\n\n\n\nwe can also reorder the variables a number of different ways with the order input (see help(ggparcoord) for details). There appears to be some weird errors however with the different options, but you can still manually provide the order of indices as follows:\n\n\npenguins |&gt;\n  ggparcoord(columns = 3:6, alphaLines = .2, groupColumn = \"species\",\n             order = c(6, 5, 3, 4))"
  },
  {
    "objectID": "demos/08-pca.html",
    "href": "demos/08-pca.html",
    "title": "Demo 08: Principal Component Analysis",
    "section": "",
    "text": "Throughout this demo we will again use the dataset about Starbucks drinks available in the #TidyTuesday project.\nYou can read in and manipulate various columns in the dataset with the following code:\n\nlibrary(tidyverse)\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g),\n         fiber_g = as.numeric(fiber_g))\n\nWe will apply principal component analysis (PCA) to the quantitative variables in this dataset:\n\n# Select the variables of interest:\nstarbucks_quant_data &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg)\ndim(starbucks_quant_data)\n\n[1] 1147   11\n\n\nAs seen above, there are 11 quantitative variables in the dataset, and it’s difficult to visualize 11 quantitative variables simultaneously. Maybe we can “get away” with just plotting two dimensions that make up the majority of the variation among these 11 variables (i.e., the first two principal components).\nTo conduct PCA, you must center and standardize your variables. We can either do that manually with the scale() function:\n\nscaled_starbucks_quant_data &lt;- scale(starbucks_quant_data)\n\nOr we can tell R do that for us before performing PCA using the prcomp() function:\n\n# perform PCA\nstarbucks_pca &lt;- prcomp(starbucks_quant_data, \n                        # Center and scale variables:\n                        center = TRUE, scale. = TRUE)\n# This is equivalent to the following commented out code:\n# starbucks_pca &lt;- prcomp(scaled_starbucks_quant_data, \n#                         center = FALSE, scale. = FALSE)\n# View the summary\nsummary(starbucks_pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000\n\n\nThere are 11 variables in this dataset, and thus there are 11 principal components. However, we can see that the first principal component accounts for over half of the variation in the dataset(!), while the second accounts for about 15% of the variation. As we can see, the variation accounted by each component adds up to the total variation in the data (i.e., the “cumulative proportion” equals 100% in the PC11 column). Also, in the first row, we can see that \\(\\text{Var}(Z_1) &gt; \\text{Var}(Z_2) &gt; \\cdots &gt; \\text{Var}(Z_{11})\\), as expected given what we talked about in lecture.\nWe haven’t actually computed the principal components \\(Z_1,\\dots,Z_{11}\\) yet. In brief, PCA provides a \\(p \\times p\\) “rotation matrix,” and the matrix \\(\\boldsymbol{Z} = (Z_1,\\dots,Z_{11})\\) is equal to the original data matrix \\(X\\) times the rotation matrix. The prcomp() function returns us the result of this matrix multiplication: the matrix of the principal component scores \\(\\boldsymbol{Z} = (Z_1,\\dots,Z_{11})\\) which can be accessed in the following way:\n\nstarbucks_pc_matrix &lt;- starbucks_pca$x\nhead(starbucks_pc_matrix)\n\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9        PC10         PC11\n[1,] -0.02812472 0.006489978 0.05145094 -0.06678083 -0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 -0.08080545 -0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 -0.09389227 -0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 -0.11582260 -0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 -0.03631676 -0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 -0.03497690 -0.002469611\n\n\nWe could have manually computed this using the returned rotation matrix and the original data (but centered and scaled). You perform matrix multiplication in R using the %*% operator:\n\nmanual_starbucks_pc_matrix &lt;- \n  as.matrix(scaled_starbucks_quant_data) %*% starbucks_pca$rotation\nhead(manual_starbucks_pc_matrix)\n\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9        PC10         PC11\n[1,] -0.02812472 0.006489978 0.05145094 -0.06678083 -0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 -0.08080545 -0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 -0.09389227 -0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 -0.11582260 -0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 -0.03631676 -0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 -0.03497690 -0.002469611\n\n\nAs you can see from just the first so many rows, these matrices match. If we view the dimensionality of this matrix (just the one returned to us by prcomp), we can seee that it matches the dimensionality of the original dataset:\n\ndim(starbucks_pc_matrix)\n\n[1] 1147   11\n\n\nIndeed, it is literally an 11-dimensional rotation of our dataset. However, the first column of this matrix accounts for over half of the variation in the data and the second column accounts for over 15% of the variation, so maybe we can “get away” with plotting just those first two dimensions.\nTo recreate what the summary output of prcomp function gave us above, the following line of code computes the standard deviation of each \\(Z\\) (the numbers match what’s given in the first row of numbers above):\n\napply(starbucks_pc_matrix, MARGIN = 2, FUN = sd)\n\n       PC1        PC2        PC3        PC4        PC5        PC6        PC7 \n2.47478380 1.30742010 1.05712064 0.97918632 0.67836258 0.56399067 0.44130936 \n       PC8        PC9       PC10       PC11 \n0.28122634 0.16874262 0.08701525 0.04048139 \n\n\nThis corresponds to the singular values, i.e., \\(\\sqrt{\\lambda_j}\\). We can then compute the proportion of variance explained by each component (also displayed in the summary output) by squaring these values and dividing by the number of columns:\n\n# Note that I can just replace the sd function above with the var function\napply(starbucks_pc_matrix, MARGIN = 2, FUN = var) / \n  ncol(starbucks_pc_matrix)\n\n         PC1          PC2          PC3          PC4          PC5          PC6 \n0.5567777142 0.1553952108 0.1015912775 0.0871641674 0.0418341630 0.0289168610 \n         PC7          PC8          PC9         PC10         PC11 \n0.0177049043 0.0071898413 0.0025885519 0.0006883321 0.0001489766 \n\n\nThe plot below displays the first two PCs \\(Z_1\\) and \\(Z_2\\):\n\n# First add these columns to the original dataset:\nstarbucks &lt;- starbucks |&gt;\n  mutate(pc1 = starbucks_pc_matrix[,1], \n         pc2 = starbucks_pc_matrix[,2])\nstarbucks |&gt;\n  ggplot(aes(x = pc1, y = pc2)) +\n  geom_point(alpha = 0.25) +\n  labs(x = \"PC 1\", y = \"PC 2\")\n\n\n\n\n\n\n\n\nThis matches what we saw returned by MDS!"
  },
  {
    "objectID": "demos/08-pca.html#principal-components-of-starbucks",
    "href": "demos/08-pca.html#principal-components-of-starbucks",
    "title": "Demo 08: Principal Component Analysis",
    "section": "",
    "text": "Throughout this demo we will again use the dataset about Starbucks drinks available in the #TidyTuesday project.\nYou can read in and manipulate various columns in the dataset with the following code:\n\nlibrary(tidyverse)\nstarbucks &lt;- \n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv\") |&gt;\n  # Convert columns to numeric that were saved as character\n  mutate(trans_fat_g = as.numeric(trans_fat_g),\n         fiber_g = as.numeric(fiber_g))\n\nWe will apply principal component analysis (PCA) to the quantitative variables in this dataset:\n\n# Select the variables of interest:\nstarbucks_quant_data &lt;- starbucks |&gt;\n  dplyr::select(serv_size_m_l:caffeine_mg)\ndim(starbucks_quant_data)\n\n[1] 1147   11\n\n\nAs seen above, there are 11 quantitative variables in the dataset, and it’s difficult to visualize 11 quantitative variables simultaneously. Maybe we can “get away” with just plotting two dimensions that make up the majority of the variation among these 11 variables (i.e., the first two principal components).\nTo conduct PCA, you must center and standardize your variables. We can either do that manually with the scale() function:\n\nscaled_starbucks_quant_data &lt;- scale(starbucks_quant_data)\n\nOr we can tell R do that for us before performing PCA using the prcomp() function:\n\n# perform PCA\nstarbucks_pca &lt;- prcomp(starbucks_quant_data, \n                        # Center and scale variables:\n                        center = TRUE, scale. = TRUE)\n# This is equivalent to the following commented out code:\n# starbucks_pca &lt;- prcomp(scaled_starbucks_quant_data, \n#                         center = FALSE, scale. = FALSE)\n# View the summary\nsummary(starbucks_pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000\n\n\nThere are 11 variables in this dataset, and thus there are 11 principal components. However, we can see that the first principal component accounts for over half of the variation in the dataset(!), while the second accounts for about 15% of the variation. As we can see, the variation accounted by each component adds up to the total variation in the data (i.e., the “cumulative proportion” equals 100% in the PC11 column). Also, in the first row, we can see that \\(\\text{Var}(Z_1) &gt; \\text{Var}(Z_2) &gt; \\cdots &gt; \\text{Var}(Z_{11})\\), as expected given what we talked about in lecture.\nWe haven’t actually computed the principal components \\(Z_1,\\dots,Z_{11}\\) yet. In brief, PCA provides a \\(p \\times p\\) “rotation matrix,” and the matrix \\(\\boldsymbol{Z} = (Z_1,\\dots,Z_{11})\\) is equal to the original data matrix \\(X\\) times the rotation matrix. The prcomp() function returns us the result of this matrix multiplication: the matrix of the principal component scores \\(\\boldsymbol{Z} = (Z_1,\\dots,Z_{11})\\) which can be accessed in the following way:\n\nstarbucks_pc_matrix &lt;- starbucks_pca$x\nhead(starbucks_pc_matrix)\n\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9        PC10         PC11\n[1,] -0.02812472 0.006489978 0.05145094 -0.06678083 -0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 -0.08080545 -0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 -0.09389227 -0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 -0.11582260 -0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 -0.03631676 -0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 -0.03497690 -0.002469611\n\n\nWe could have manually computed this using the returned rotation matrix and the original data (but centered and scaled). You perform matrix multiplication in R using the %*% operator:\n\nmanual_starbucks_pc_matrix &lt;- \n  as.matrix(scaled_starbucks_quant_data) %*% starbucks_pca$rotation\nhead(manual_starbucks_pc_matrix)\n\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9        PC10         PC11\n[1,] -0.02812472 0.006489978 0.05145094 -0.06678083 -0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 -0.08080545 -0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 -0.09389227 -0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 -0.11582260 -0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 -0.03631676 -0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 -0.03497690 -0.002469611\n\n\nAs you can see from just the first so many rows, these matrices match. If we view the dimensionality of this matrix (just the one returned to us by prcomp), we can seee that it matches the dimensionality of the original dataset:\n\ndim(starbucks_pc_matrix)\n\n[1] 1147   11\n\n\nIndeed, it is literally an 11-dimensional rotation of our dataset. However, the first column of this matrix accounts for over half of the variation in the data and the second column accounts for over 15% of the variation, so maybe we can “get away” with plotting just those first two dimensions.\nTo recreate what the summary output of prcomp function gave us above, the following line of code computes the standard deviation of each \\(Z\\) (the numbers match what’s given in the first row of numbers above):\n\napply(starbucks_pc_matrix, MARGIN = 2, FUN = sd)\n\n       PC1        PC2        PC3        PC4        PC5        PC6        PC7 \n2.47478380 1.30742010 1.05712064 0.97918632 0.67836258 0.56399067 0.44130936 \n       PC8        PC9       PC10       PC11 \n0.28122634 0.16874262 0.08701525 0.04048139 \n\n\nThis corresponds to the singular values, i.e., \\(\\sqrt{\\lambda_j}\\). We can then compute the proportion of variance explained by each component (also displayed in the summary output) by squaring these values and dividing by the number of columns:\n\n# Note that I can just replace the sd function above with the var function\napply(starbucks_pc_matrix, MARGIN = 2, FUN = var) / \n  ncol(starbucks_pc_matrix)\n\n         PC1          PC2          PC3          PC4          PC5          PC6 \n0.5567777142 0.1553952108 0.1015912775 0.0871641674 0.0418341630 0.0289168610 \n         PC7          PC8          PC9         PC10         PC11 \n0.0177049043 0.0071898413 0.0025885519 0.0006883321 0.0001489766 \n\n\nThe plot below displays the first two PCs \\(Z_1\\) and \\(Z_2\\):\n\n# First add these columns to the original dataset:\nstarbucks &lt;- starbucks |&gt;\n  mutate(pc1 = starbucks_pc_matrix[,1], \n         pc2 = starbucks_pc_matrix[,2])\nstarbucks |&gt;\n  ggplot(aes(x = pc1, y = pc2)) +\n  geom_point(alpha = 0.25) +\n  labs(x = \"PC 1\", y = \"PC 2\")\n\n\n\n\n\n\n\n\nThis matches what we saw returned by MDS!"
  },
  {
    "objectID": "demos/08-pca.html#one-biplot-to-rule-them-all",
    "href": "demos/08-pca.html#one-biplot-to-rule-them-all",
    "title": "Demo 08: Principal Component Analysis",
    "section": "One Biplot to Rule Them All",
    "text": "One Biplot to Rule Them All\nHowever, the components by themselves aren’t very interpretable - how do they relate to original variables? At this point, it’s important to remember that the principal components are linear combinations of the original variables. So, there is a (deterministic) linear relationship between the original variables and the principal components that we are plotting here.\nUsing the popular R package factoextra, we can plot these linear relationships on top of the scatterplot. We can do so using what’s called a biplot, which is essentially just a fancy expression for “scatterplots with arrows on top”. After installing the factoextra package, we can create the biplot using the fviz_pca_biplot() function on the prcomp output directly (but with the observation labels turned off!):\n\n# install.packages(\"factoextra\")\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n# Designate to only label the variables:\nfviz_pca_biplot(starbucks_pca, label = \"var\", \n                # Change the alpha for the observations - \n                # which is represented by ind\n                alpha.ind = .25,\n                # Modify the alpha for the variables (var):\n                alpha.var = .75,\n                # Modify the color of the variables\n                col.var = \"darkblue\")\n\n\n\n\n\n\n\n\nThe above plot tells us a lot of information:\n\nThe direction of a particular arrow is indicative of “as this variable increases….” For example, the far left arrow for caffeine_mg suggests that, as caffeine_mg increases, \\(Z_1\\) and \\(Z_2\\) tend to decrease (in other words, within the definition of \\(Z_1\\) and \\(Z_2\\), the coefficient for caffeine_mg is negative; this is verified below). You can contrast this with serv_size_m_l which is pointing to the upper right, indicating that as serv_size_m_l increases then both \\(Z_1\\) and \\(Z_2\\) tend to increase.\nThe angle of the different vectors is also indicative of the correlation between different variables. If two vectors are at a right angle (90 degrees), that suggests that they are uncorrelated, e.g., serv_size_m_l and saturated_fag_g. If two vectors are in similar directions (i.e., their angle is less than 90 degrees), that suggests that they are positively correlated, e.g., sugar_g and total_carbs_g. If two vectors are in different directions (i.e., their angle is greater than 90 degrees), that suggests that they are negatively correlated, e.g., caffeine_mg and calories.\nThe length of the lines also indicate how strongly related the principal components are with the individual variables. For example, serv_size_m_l has a fairly long line because it has a large positive coefficient for \\(Z_1\\) in the rotation matrix (see below). Meanwhile, caffeine_mg has a relatively short arrow because its coefficients are relatively small.\n\nFor reference, the below code shows the rotation matrix we used to create the \\(Z\\)s. You’ll see that the directions of the vectors in the above plot are the first two columns of this matrix.\n\nstarbucks_pca$rotation\n\n                        PC1         PC2         PC3          PC4         PC5\nserv_size_m_l    0.20078297  0.44103545  0.35053466 -0.117331692 -0.71633828\ncalories         0.39488151  0.10314156 -0.01048587  0.055814030  0.11487225\ntotal_fat_g      0.35254969 -0.31687231  0.06598414  0.046196797  0.07677253\nsaturated_fat_g  0.33929914 -0.30565133  0.05310592 -0.003731227  0.16145662\ntrans_fat_g      0.29974182 -0.39855899  0.01855869 -0.092804122 -0.35695525\ncholesterol_mg   0.33049434 -0.37077805  0.01219867 -0.105617624 -0.18815364\nsodium_mg        0.33573598  0.24647412 -0.09107538 -0.083512068  0.34969486\ntotal_carbs_g    0.34858318  0.34483762 -0.09623296  0.002842153  0.12386718\nfiber_g          0.11351848  0.04137855  0.17814948  0.956078124 -0.04719036\nsugar_g          0.34234584  0.35100839 -0.13314389 -0.109371714  0.12108189\ncaffeine_mg     -0.03085327 -0.01056235  0.89572768 -0.167846419  0.35265479\n                        PC6         PC7         PC8         PC9         PC10\nserv_size_m_l    0.30806678  0.13668394  0.04039275  0.01194522 -0.001076764\ncalories        -0.01331210 -0.18521073  0.09836135 -0.45551398  0.744248239\ntotal_fat_g      0.37698224 -0.03833030  0.03871096 -0.58859673 -0.518643989\nsaturated_fat_g  0.57285718 -0.06553378 -0.26369346  0.56257742  0.209355859\ntrans_fat_g     -0.50043224  0.15197176 -0.58086994 -0.05398876  0.032105721\ncholesterol_mg  -0.26449384 -0.04594580  0.74615325  0.27703165 -0.032124871\nsodium_mg       -0.06228905  0.82317144  0.06292570  0.04230447 -0.037304757\ntotal_carbs_g   -0.17619489 -0.34490217 -0.08588651  0.12501079 -0.148886253\nfiber_g         -0.11365528  0.06192955  0.01207815  0.10654914 -0.061378250\nsugar_g         -0.16729497 -0.33345131 -0.10758116  0.14408095 -0.321644156\ncaffeine_mg     -0.19600402 -0.06671121 -0.02122274  0.01530108 -0.020691492\n                         PC11\nserv_size_m_l    0.0053899973\ncalories        -0.1070327163\ntotal_fat_g      0.0489644534\nsaturated_fat_g -0.0152794817\ntrans_fat_g      0.0069417249\ncholesterol_mg  -0.0004710159\nsodium_mg        0.0185545403\ntotal_carbs_g    0.7347049650\nfiber_g         -0.0730283725\nsugar_g         -0.6635335478\ncaffeine_mg     -0.0094861578\n\n\nIn the above example, we plotted the first two principal components; thus, implicitly, we have chosen \\(k = 2\\), the only reason being that it is easy to visualize. However, how many principal components should we actually be using?"
  },
  {
    "objectID": "demos/08-pca.html#creating-and-interpreting-scree-plots",
    "href": "demos/08-pca.html#creating-and-interpreting-scree-plots",
    "title": "Demo 08: Principal Component Analysis",
    "section": "Creating and Interpreting Scree Plots",
    "text": "Creating and Interpreting Scree Plots\nThere is a common visual used to answer this question, but first let’s build some intuition. We already know that \\(Z_1\\) accounts for the most variation in our data, \\(Z_2\\) accounts for the next most, and so on. Thus, each time we add a new principal component dimension, we capture a “higher proportion of the information in the data,” but that increase in proportion decreases for each new dimension we add. (You may have to read those last two sentences a few times to get what I mean.) Thus, in practice, it is recommended to keep adding principal components until the marginal gain “levels off,” i.e., decreases to the point that it isn’t too beneficial to add another dimension to the data.\nThis trade-off between dimensions and marginal gain in information is often inspected visually using a scree plot, or what is more commonly known as an elbow plot. In an elbow plot, the x-axis has the numbers \\(1,2,\\dots,p\\) (i.e., the dimensions in the data), and the y-axis has the proportion of variation that the particular principal component \\(Z_j\\) accounts for. We can construct the scree plot using the fviz_screeplot() function from `factoextra\n\nfviz_eig(starbucks_pca, addlabels = TRUE) # Add the labels \n\n\n\n\n\n\n\n\nThe graphical rule-of-thumb is to then look for the “elbow,” i.e., where the proportion of variation starts to become flat. Unfortunately there is not a definitive “this is the elbow for sure” rule, and it is up to your judgment. Another useful rule-of-thumb is to consider drawing a horizontal line at 1 divided by the number of variables in your original dataset. Why do you think that is a useful rule? We easily do this because factoextra generates ggplot objects, so we can add another geometric layer corresponding to our reference:\n\nfviz_eig(starbucks_pca, addlabels = TRUE) +\n  # Have to multiply by 100 to get on percent scale\n  geom_hline(yintercept = 100 * (1 / ncol(starbucks_quant_data)),\n             linetype = \"dashed\", color = \"darkred\")\n\n\n\n\n\n\n\n\nBased on this plot, I think there’s a strong argument to stop at \\(k = 3\\) (but maybe go up to \\(k = 5\\) for another substantial drop in the elbow).\nLet’s say we decide \\(k = 3\\). This means that we should use the first three principal components in our graphics and other analyses in order for a “satisfactory” amount of the variation in the data to be captured. Our above visual only plots the first two principal components, and so we are “hiding” some data information that we are better off plotting in some way if possible (specifically, we are hiding about 30% of the information, i.e., the total amount of information captured by principal components 3, 4, …, 11, and about a third of this remaining information is captured by that third component that we are not plotting). This means that, theoretically, we should plot three quantitative variables, and we’ve discussed a bit about how to do this - you could use the size of points, transparency, or even a 3D scatterplot if you wanted to - but we are not going to explore that further here. Alternatively, you could just make three scatterplots (one for each pair of principal components).\nIf you’re having issues with the factoextra package then you can easily remake the scree plot manually. All we need to do is grab the proportion of variance explained by each component, turn it into a table, and then display it in some way. We already computed these values earlier in the demo, but we also just grab the singular values directly provided to us by R:\n\n# Manual creation of elbow plot, start by computing the eigenvalues and dividing by\n# the total variance in the data:\ntibble(prop_var = (starbucks_pca$sdev)^2 / ncol(starbucks_quant_data)) |&gt;\n  # Add a column for the PC index:\n  mutate(pc_index = 1:n()) |&gt;\n  # Now make the plot!\n  ggplot(aes(x = pc_index, y = prop_var)) +\n  geom_bar(stat = \"identity\", fill = \"blue\", alpha = 0.75) +\n  geom_point(color = \"black\", size = 2) +\n  geom_line() +\n  # Add the horizontal reference line:\n  geom_hline(yintercept = (1 / ncol(starbucks_quant_data)),\n             linetype = \"dashed\", color = \"darkred\") +\n  # And label:\n  labs(x = \"Dimensions\", y = \"Proportion of explained variance\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nMaking a biplot from scratch is much more difficult… Instead, you can try the ggfortify package (which is useful for making model diagnostic plots). The following code demonstrates how to do this (after you installed ggfortify):\n\n# install.packages(\"ggfortify\")\nlibrary(ggfortify)\n\nWarning: package 'ggfortify' was built under R version 4.2.3\n\nautoplot(starbucks_pca, \n         data = starbucks_quant_data,\n         alpha = 0.25,\n         loadings = TRUE, loadings.colour = 'darkblue',\n         loadings.label.colour = 'darkblue',\n         loadings.label = TRUE, loadings.label.size = 3,\n         loadings.label.repel = TRUE) +\n  theme_bw()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Graphics and Visualization 36-315",
    "section": "",
    "text": "This is the companion website for Statistical Graphics and Visualization 36-315. While all of the assignments will be posted on Canvas, this website provides an alternative way to access lecture materials and additional demo files (see the see side-bar).\nLectures are on Mondays and Wednesdays from 12 - 12:50 PM, located in DH A302.\nOffice hours schedule:\n\n\n\n\n\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\nProf Yurko\nWednesdays and Thursdays @ 2 PM\nBaker Hall 132D\n\n\nAnna Rosengart\nMondays @ 2 PM\nZoom (see Canvas)\n\n\nPerry Lin\nWednesdays @ 11 AM\nZoom (see Canvas)\n\n\n\nThere are no required textbooks for this course, but the following are useful free resources that I will sometimes refer to as recommended reading:\n\nR for Data Science\nData Visualization: A Practical Introduction\nFundamentals of Data Visualization\nggplot2: Elegant Graphics for Data Analysis\n\nAnd the following are interesting data visualization websites:\n\nFlowingData\nHistory of Data Visualization\nFriends Don’t Let Friends Make Bad Graphs",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "demos/10-time-series.html",
    "href": "demos/10-time-series.html",
    "title": "Demo 10: Visualizing time series data",
    "section": "",
    "text": "A time series measures a single variable over many points in time. Time intervals may be regularly or irregularly spaced, but we’ll only consider regularly-spaced data today. We’ll focus on visualizing a single variable over time, since there are already so many choices and information to work with.\nFor this demo, we’re going to work with a dataset that’s actually already loaded when you start R: it’s defined under co2, and known as the “Mauna Loa Atmospheric CO2 Concentration” dataset. This dataset contains 468 monthly measurements of CO2 concentration from 1959 to 1997.\nWhen you start R, if you type in co2, this is what you’ll see:\n\nco2\n\n        Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct\n1959 315.42 316.31 316.50 317.56 318.13 318.00 316.39 314.65 313.68 313.18\n1960 316.27 316.81 317.42 318.87 319.87 319.43 318.01 315.74 314.00 313.68\n1961 316.73 317.54 318.38 319.31 320.42 319.61 318.42 316.63 314.83 315.16\n1962 317.78 318.40 319.53 320.42 320.85 320.45 319.45 317.25 316.11 315.27\n1963 318.58 318.92 319.70 321.22 322.08 321.31 319.58 317.61 316.05 315.83\n1964 319.41 320.07 320.74 321.40 322.06 321.73 320.27 318.54 316.54 316.71\n1965 319.27 320.28 320.73 321.97 322.00 321.71 321.05 318.71 317.66 317.14\n1966 320.46 321.43 322.23 323.54 323.91 323.59 322.24 320.20 318.48 317.94\n1967 322.17 322.34 322.88 324.25 324.83 323.93 322.38 320.76 319.10 319.24\n1968 322.40 322.99 323.73 324.86 325.40 325.20 323.98 321.95 320.18 320.09\n1969 323.83 324.26 325.47 326.50 327.21 326.54 325.72 323.50 322.22 321.62\n1970 324.89 325.82 326.77 327.97 327.91 327.50 326.18 324.53 322.93 322.90\n1971 326.01 326.51 327.01 327.62 328.76 328.40 327.20 325.27 323.20 323.40\n1972 326.60 327.47 327.58 329.56 329.90 328.92 327.88 326.16 324.68 325.04\n1973 328.37 329.40 330.14 331.33 332.31 331.90 330.70 329.15 327.35 327.02\n1974 329.18 330.55 331.32 332.48 332.92 332.08 331.01 329.23 327.27 327.21\n1975 330.23 331.25 331.87 333.14 333.80 333.43 331.73 329.90 328.40 328.17\n1976 331.58 332.39 333.33 334.41 334.71 334.17 332.89 330.77 329.14 328.78\n1977 332.75 333.24 334.53 335.90 336.57 336.10 334.76 332.59 331.42 330.98\n1978 334.80 335.22 336.47 337.59 337.84 337.72 336.37 334.51 332.60 332.38\n1979 336.05 336.59 337.79 338.71 339.30 339.12 337.56 335.92 333.75 333.70\n1980 337.84 338.19 339.91 340.60 341.29 341.00 339.39 337.43 335.72 335.84\n1981 339.06 340.30 341.21 342.33 342.74 342.08 340.32 338.26 336.52 336.68\n1982 340.57 341.44 342.53 343.39 343.96 343.18 341.88 339.65 337.81 337.69\n1983 341.20 342.35 342.93 344.77 345.58 345.14 343.81 342.21 339.69 339.82\n1984 343.52 344.33 345.11 346.88 347.25 346.62 345.22 343.11 340.90 341.18\n1985 344.79 345.82 347.25 348.17 348.74 348.07 346.38 344.51 342.92 342.62\n1986 346.11 346.78 347.68 349.37 350.03 349.37 347.76 345.73 344.68 343.99\n1987 347.84 348.29 349.23 350.80 351.66 351.07 349.33 347.92 346.27 346.18\n1988 350.25 351.54 352.05 353.41 354.04 353.62 352.22 350.27 348.55 348.72\n1989 352.60 352.92 353.53 355.26 355.52 354.97 353.75 351.52 349.64 349.83\n1990 353.50 354.55 355.23 356.04 357.00 356.07 354.67 352.76 350.82 351.04\n1991 354.59 355.63 357.03 358.48 359.22 358.12 356.06 353.92 352.05 352.11\n1992 355.88 356.63 357.72 359.07 359.58 359.17 356.94 354.92 352.94 353.23\n1993 356.63 357.10 358.32 359.41 360.23 359.55 357.53 355.48 353.67 353.95\n1994 358.34 358.89 359.95 361.25 361.67 360.94 359.55 357.49 355.84 356.00\n1995 359.98 361.03 361.66 363.48 363.82 363.30 361.94 359.50 358.11 357.80\n1996 362.09 363.29 364.06 364.76 365.45 365.01 363.70 361.54 359.51 359.65\n1997 363.23 364.06 364.61 366.40 366.84 365.68 364.52 362.57 360.24 360.83\n        Nov    Dec\n1959 314.66 315.43\n1960 314.84 316.03\n1961 315.94 316.85\n1962 316.53 317.53\n1963 316.91 318.20\n1964 317.53 318.55\n1965 318.70 319.25\n1966 319.63 320.87\n1967 320.56 321.80\n1968 321.16 322.74\n1969 322.69 323.95\n1970 323.85 324.96\n1971 324.63 325.85\n1972 326.34 327.39\n1973 327.99 328.48\n1974 328.29 329.41\n1975 329.32 330.59\n1976 330.14 331.52\n1977 332.24 333.68\n1978 333.75 334.78\n1979 335.12 336.56\n1980 336.93 338.04\n1981 338.19 339.44\n1982 339.09 340.32\n1983 340.98 342.82\n1984 342.80 344.04\n1985 344.06 345.38\n1986 345.48 346.72\n1987 347.64 348.78\n1988 349.91 351.18\n1989 351.14 352.37\n1990 352.69 354.07\n1991 353.64 354.89\n1992 354.09 355.33\n1993 355.30 356.78\n1994 357.59 359.05\n1995 359.61 360.74\n1996 360.80 362.38\n1997 362.49 364.34\n\n\nThese numbers are expressed in parts per million (ppm) - if you’ve taken a chemistry class (which I haven’t), I assure you know more about this than I do, but ppm is a very common measure for pollutants and contaminants.\nThe co2 object is actually defined with a class we haven’t seen yet, ts (time series):\n\nclass(co2)\n\n[1] \"ts\"\n\n\nAs a result, it contains extra attributes about the times associated with each value, and base R graphs can plot this automatically (we don’t need to specify the time range manually):\n\nplot(co2)\n\n\n\n\n\n\n\n\nThis is typically called a line plot. Here’s how you would plot the same data in the gg style (note that you need the package ggfortify to do this) without constructing the typical long-table format we’re used to:\n\nlibrary(ggfortify)\n\nWarning: package 'ggfortify' was built under R version 4.2.3\n\n\nLoading required package: ggplot2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nautoplot(co2)\n\n\n\n\n\n\n\n\nIn time series, we are interested in checking for trends (does the variable tend to increase or decrease over time?), seasonality (are there tendencies that regularly occur? If so, at what intervals do they occur?), general variability (i.e., variation beyond average trends and seasonality), and outliers (unusual spikes or valleys). Which of these do we see in the co2 data?\nIn order to show you more general-purpose plotting methods, we’ll treat co2 as numeric instead of ts from now on. This is actually the form that many time series data take (they’re usually not in the ts format), so this will also show you some of the formatting/structuring issues you’ll have to work with in the wild.\nIn the code below, we create a dataset with 1:468 as the obs_i variable (x-axis variable) and the CO2 concentration as the co2_val variable (y-axis) variable). Then, we use our usual ggplot to plot the data:\n\nlibrary(tidyverse)\nco2_tbl &lt;- tibble(co2_val = as.numeric(co2)) |&gt;\n  mutate(obs_i = 1:n())\nco2_tbl |&gt;\n  ggplot(aes(x = obs_i, y = co2_val)) + \n  geom_line() + \n  labs(x = \"Time index\", y = \"CO2 (ppm)\")\n\n\n\n\n\n\n\n\nNote the x-axis: This will not be helpful/informative to readers. It’s a common mistake for people to just put something like “index” on the x-axis, but they don’t even tell you what they are indexing. Be sure to not make this mistake! To properly structure time series data and label it correctly, you’ll need to know how to work with Date type objects in R (which we will discuss next…).\n\n\nThe following code redefines creates a new column in our dataset to be the monthly dates 1/1/1959, 2/1/1959, … , 12/1/1997 using the as.Date() function (given the description of the time range in help(co2)):\n\nco2_tbl &lt;- co2_tbl |&gt;\n  # We can use the seq() function with dates which is pretty useful!\n  mutate(obs_date = seq(as.Date(\"1959/1/1\"), by = \"month\",\n                        length.out = n()))\nco2_tbl |&gt;\n  ggplot(aes(x = obs_date, y = co2_val)) + \n  geom_line() + \n  labs(x = \"Year\", y = \"CO2 (ppm)\")\n\n\n\n\n\n\n\n\nUnfortunately, the default format for dates in as.Date() is Year/Month/Day. If you prefer another format, such as the common Month/Day/Year (as I used above in the previous paragraph), you need to include the format argument within as.Date(), as such (note that the Y is capitalized - yes, as.Date() is that picky):\n\nco2_tbl &lt;- co2_tbl |&gt;\n  mutate(obs_date = seq(as.Date(\"1/1/1959\", format = \"%m/%d/%Y\"), \n                        by = \"month\",\n                        length.out = n()))\nco2_tbl |&gt;\n  ggplot(aes(x = obs_date, y = co2_val)) + \n  geom_line() + \n  labs(x = \"Year\", y = \"CO2 (ppm)\")\n\n\n\n\n\n\n\n\nNote that all we needed to do was convert the obs_date variable to a Date class, and then we could use ggplot as is. This is because ggplot knows to use a special date scale for the x-axis when x has class Date. As a result, we can easily play with the breaks on the date axis using scale_x_date(). For example:\n\nFor a subset of the data, maybe we only want ticks every 4 months, using date_breaks.\nWe can specify the format of the date with date_labels. (See Details section of ?strftime for the formatting options. Here, we choose abbreviated month %b and full year %Y.)\n\n\nco2_tbl[1:26,] |&gt;\n  ggplot(aes(x = obs_date, y = co2_val)) + \n  geom_line() + \n  scale_x_date(date_breaks = \"4 months\", date_labels = \"%b %Y\") +\n  labs(x = \"Year\", y = \"CO2 (ppm)\") +\n  # Modify the x-axis text \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "demos/10-time-series.html#formatting-date-data-in-r",
    "href": "demos/10-time-series.html#formatting-date-data-in-r",
    "title": "Demo 10: Visualizing time series data",
    "section": "",
    "text": "The following code redefines creates a new column in our dataset to be the monthly dates 1/1/1959, 2/1/1959, … , 12/1/1997 using the as.Date() function (given the description of the time range in help(co2)):\n\nco2_tbl &lt;- co2_tbl |&gt;\n  # We can use the seq() function with dates which is pretty useful!\n  mutate(obs_date = seq(as.Date(\"1959/1/1\"), by = \"month\",\n                        length.out = n()))\nco2_tbl |&gt;\n  ggplot(aes(x = obs_date, y = co2_val)) + \n  geom_line() + \n  labs(x = \"Year\", y = \"CO2 (ppm)\")\n\n\n\n\n\n\n\n\nUnfortunately, the default format for dates in as.Date() is Year/Month/Day. If you prefer another format, such as the common Month/Day/Year (as I used above in the previous paragraph), you need to include the format argument within as.Date(), as such (note that the Y is capitalized - yes, as.Date() is that picky):\n\nco2_tbl &lt;- co2_tbl |&gt;\n  mutate(obs_date = seq(as.Date(\"1/1/1959\", format = \"%m/%d/%Y\"), \n                        by = \"month\",\n                        length.out = n()))\nco2_tbl |&gt;\n  ggplot(aes(x = obs_date, y = co2_val)) + \n  geom_line() + \n  labs(x = \"Year\", y = \"CO2 (ppm)\")\n\n\n\n\n\n\n\n\nNote that all we needed to do was convert the obs_date variable to a Date class, and then we could use ggplot as is. This is because ggplot knows to use a special date scale for the x-axis when x has class Date. As a result, we can easily play with the breaks on the date axis using scale_x_date(). For example:\n\nFor a subset of the data, maybe we only want ticks every 4 months, using date_breaks.\nWe can specify the format of the date with date_labels. (See Details section of ?strftime for the formatting options. Here, we choose abbreviated month %b and full year %Y.)\n\n\nco2_tbl[1:26,] |&gt;\n  ggplot(aes(x = obs_date, y = co2_val)) + \n  geom_line() + \n  scale_x_date(date_breaks = \"4 months\", date_labels = \"%b %Y\") +\n  labs(x = \"Year\", y = \"CO2 (ppm)\") +\n  # Modify the x-axis text \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "demos/04-nonlinear-pairs.html",
    "href": "demos/04-nonlinear-pairs.html",
    "title": "Demo 04: Nonlinear Regression and Pairs Plots",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs. Throughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")"
  },
  {
    "objectID": "demos/04-nonlinear-pairs.html#adjusting-the-span",
    "href": "demos/04-nonlinear-pairs.html#adjusting-the-span",
    "title": "Demo 04: Nonlinear Regression and Pairs Plots",
    "section": "Adjusting the span",
    "text": "Adjusting the span\nWhen using method = \"loess\", we can control the proportion of observations that are used when estimating the local regression (i.e., the size of the neighborhood around the observation of interest) with the span argument. For span &lt; 1, then the “neighborhood” includes proportion span of all possible points. By default, method = \"loess\" using the tri-cubic weighting, such that the weight is proportional to (1 - (dist / maxdist)^3)^3 (where maxdist refers to the maximum distance from the observations in the considered neighborhood). The default setting is span = 0.75, meaning that 75% of the dataset’s observations are used when fitting the local linear regression with weights. We can change span directly in geom_smooth():\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(span = .2)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nUpdate to use all observations instead:\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(span = 1)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/07-clustering.html",
    "href": "demos/07-clustering.html",
    "title": "Demo 07: More MDS and Creating Dendrograms",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs."
  },
  {
    "objectID": "demos/07-clustering.html#mds-meets-mcu",
    "href": "demos/07-clustering.html#mds-meets-mcu",
    "title": "Demo 07: More MDS and Creating Dendrograms",
    "section": "MDS meets MCU",
    "text": "MDS meets MCU\nWe will work with a dataset about the performance of MCU movies. The dataset was accessed from here with information such as the box office performance and reviews.\nHere is the code to read in the data into R:\n\nlibrary(tidyverse)\nmcu_movies &lt;- read_csv(\"https://raw.githubusercontent.com/ryurko/DataViz-Class-Data/main/mcu_movies.csv\")\n\nFor this demo, we’ll focus just on the columns that contain quantitative variables about the movies’ performance, but we’ll also exclude the year variable:\n\n# Select only the continuous variables excluding the year\nmcu_quant &lt;- mcu_movies |&gt; \n  dplyr::select(-c(film, category, year))\n\nThere are 15 measurements about each movie:\n\nhead(mcu_quant)\n\n# A tibble: 6 × 15\n  worldwide_gross_m percent_budget_recovered critics_percent_score\n              &lt;dbl&gt;                    &lt;dbl&gt;                 &lt;dbl&gt;\n1               518                      398                    83\n2               623                      479                    87\n3              1395                      382                    76\n4              2797                      699                    94\n5              2048                      683                    85\n6              1336                      668                    96\n# ℹ 12 more variables: audience_percent_score &lt;dbl&gt;,\n#   audience_vs_critics_percent_deviance &lt;dbl&gt;, budget &lt;dbl&gt;,\n#   domestic_gross_m &lt;dbl&gt;, international_gross_m &lt;dbl&gt;,\n#   opening_weekend_m &lt;dbl&gt;, second_weekend_m &lt;dbl&gt;,\n#   x1st_vs_2nd_weekend_drop_off &lt;dbl&gt;,\n#   percent_gross_from_opening_weekend &lt;dbl&gt;,\n#   percent_gross_from_domestic &lt;dbl&gt;, …\n\n\nWe’re interested in the questions: which of these movies are most similar, and which are most different?\nWe’re going to follow our workflow from the previous demo and first compute the distance matrix for the movies (based on Euclidean distance), followed by performing multi-dimensional scaling (MDS) to see which movies are “close” and which are “far apart”.\nFirst, scale the data and then compute the distance matrix:\n\n# This is just one way to scale the data - without centering the columns\nmcu_quant &lt;- apply(mcu_quant, MARGIN = 2,\n                   FUN = function(x) x / sd(x))\nmcu_dist &lt;- dist(mcu_quant)\n\nStaring at a distance matrix and trying to find the most/least similar pairs of observations is not practical for large datasets. This is why an approach like MDS can be really useful: We can quickly see which movies are “close” and which are “far apart” by plotting the first two coordinates (using k = 2). In the code chunk below, we run MDS to get the two new coordinates, then add these coordinates as columns to the original dataset for plotting purposes:\n\n# Run MDS\nmcu_mds &lt;- cmdscale(d = dist(mcu_quant), k = 2)\n\n# Add to original dataset\nmcu_movies &lt;- mcu_movies |&gt;\n  mutate(mds1 = mcu_mds[,1], \n         mds2 = mcu_mds[,2])\n\n# Create plot:\nmcu_movies |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  geom_point(alpha = .5) +\n  labs(x = \"MDS Coordinate 1\", y = \"MDS Coordinate 2\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nSince this dataset is relatively small, I can instead plot the film titles directly using geom_text() where I just need to map the film variable to the label aesthetic of the plot:\n\nmcu_movies |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  # Use text labels instead of points:\n  geom_text(aes(label = film),\n            alpha = .75) +\n  labs(x = \"MDS Coordinate 1\", y = \"MDS Coordinate 2\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nNow I can see where the movies fall in this projection. If you know anything about the MCU, you can see that some of the biggest movies are along the right-hand side: Black Panther, The Avengers, Spider-Man: No Way Home, Avengers: Infinity War, and Avengers Endgame. We see the various other movies throughout, including the definitive worst MCU movie on the bottom-left corner, as well as a compact group of several movies together."
  },
  {
    "objectID": "demos/07-clustering.html#visualizing-distance-structure-with-hierarchical-clustering-and-dendrograms",
    "href": "demos/07-clustering.html#visualizing-distance-structure-with-hierarchical-clustering-and-dendrograms",
    "title": "Demo 07: More MDS and Creating Dendrograms",
    "section": "Visualizing distance structure with hierarchical clustering and dendrograms",
    "text": "Visualizing distance structure with hierarchical clustering and dendrograms\nWhile this MDS plot is useful for visualizing how the movies relate to each other. However, it can be difficult to imagine how we could use the MDS plot to identify clusters of movies (unless the points themselves were already clearly clustered, which they are not in the above plot). This is where dendrograms can be a great visual tool for understanding the clustering of observations in your dataset.\nDendrograms are tree-like structures used for visualizing distances. Dendrograms have the following axes:\n\ny-axis: distance (or more generally speaking: dissimilarity) at which a pair of observations are linked\nx-axis: rough grouping of observations (the exact ordering is not necessarily meaningful, other than the fact that pairs of observations near each other are being assigned to the same cluster)\n\nBelow, we implement hierarchical clustering with complete linkage and single linkage (complete linkage is more commonly used), and then plot the results on a dendrogram. We use the hclust function in R to perform hierarchical clustering (the default method is complete linkage) given a distance matrix (or in more general terms, a dissimilarity matrix).\nI’ll first start with complete linkage:\n\nhc_complete &lt;- hclust(mcu_dist, method = \"complete\")\n\nplot(hc_complete, ylab = \"Pairwise Distance\", \n     main = \"Complete Linkage\", xlab = \"MCU Movies\")\n\n\n\n\n\n\n\n\nFrom looking at this dendrogram, we can broadly see a group of movies on the right-hand side that are separate from most of the other movies. However, the movies are labeled by their row numbers which is useless for us! We can update the leaf labels by modifying the labels input for the hclust plot object. For example, I can instead label the movies with the actual film titles:\n\nplot(hc_complete, ylab = \"Pairwise Distance\", \n     labels = mcu_movies$film,\n     main = \"Complete Linkage\", xlab = \"MCU Movies\")\n\n\n\n\n\n\n\n\nThis is much more useful! I can now see that the group of movies on the right-hand side correspond to the biggest one I previously mentioned, as well as Captain Marvel. From this figure, we can now see which pairs of movies were most similar such as Ant-Man and its sequel, along with Thor and its first sequel (both of which are not great…).\nNOTE: This is a small enough dataset that labeling the leaves is worthwhile, but for larger datasets the labels can become problematic and unreadable. Instead, you can also turn off the leaf labels by just setting label to FALSE:\n\nplot(hc_complete, ylab = \"Pairwise Distance\", \n     labels = FALSE,\n     main = \"Complete Linkage\", xlab = \"MCU Movies\")\n\n\n\n\n\n\n\n\nWhile the above was with complete linkage, the following demonstrates the results with single linkage:\n\nhc_single &lt;- hclust(mcu_dist, method = \"single\")\nplot(hc_single, ylab = \"Pairwise Distance\",\n     labels = mcu_movies$film,\n     main = \"Single Linkage\", xlab = \"MCU Movies\")\n\n\n\n\n\n\n\n\nIn this case, we can clearly see a very different looking dendrogram driven by the difference in how we compute distances between clusters. Single linkage results in this chaining effect: where poorly separate but distinct clusters are merged together.\nAlternatively, because base R plotting is pretty annoying at times, we can instead use the ggdendro package to create our dendrogram. This offers more customization and even allows you to extract a dataset constructed by the dendrogram for use - but we won’t focus on that for this demo. Instead the following code uses the ggdendro package to create two dendrograms and then plots them side-by-side using the patchwork package (which I have previously used in various solutions):\n\n# You'll need to run the following lines if you do NOT have ggdendro and patchwork \n# packages installed already:\n# install.packages(\"ggdendro\")\n# install.packages(\"patchwork\")\nlibrary(ggdendro)\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.2.3\n\nhc_single_ggdendro &lt;- ggdendrogram(hc_single, theme_dendro = FALSE) +\n  labs(y = \"Pairwise Distance\", title = \"Single Linkage\") + \n  theme_bw() +\n  # Remove the x-axis title\n  theme(axis.title.x = element_blank())\n\nhc_complete_ggdendro &lt;- ggdendrogram(hc_complete, theme_dendro = FALSE) +\n  labs(y = \"Pairwise Distance\", title = \"Complete Linkage\") + \n  theme_bw() +\n  # Remove the x-axis title\n  theme(axis.title.x = element_blank())\n\nhc_single_ggdendro + hc_complete_ggdendro\n\n\n\n\n\n\n\n\nIf we want to add the movie titles to the ggdendro version of the dendrogram, we unfortunately we need to go back to the beginning of this process! We need to add rownames to our initial dataset that we used for computing the distance matrix. These names will then automatically carry over and serve as the observation labels in the dendrograms above:\n\n# Add the film titles as the row names for mcu_quant\nrownames(mcu_quant) &lt;- mcu_movies$film\n# Recompute the distance matrix\nmcu_dist &lt;- dist(mcu_quant)\n\n# And repeat the dendrogram process\nhc_complete &lt;- hclust(mcu_dist, method = \"complete\")\nggdendrogram(hc_complete, theme_dendro = FALSE) +\n  labs(y = \"Pairwise Distance\", title = \"Complete Linkage\") + \n  theme_bw() +\n  # Remove the x-axis title\n  theme(axis.title.x = element_blank())\n\n\n\n\n\n\n\n\nBut now we can’t read any of the labels! Conveniently, since ggdendro returns a ggplot object, we can just use coord_flip() to make this easier to read:\n\nhc_complete_ggdendro &lt;- ggdendrogram(hc_complete, theme_dendro = FALSE) +\n  labs(y = \"Cluster Dissimilarity (based on complete linkage)\", \n       title = \"Which MCU movies are similar to each other?\") + \n  coord_flip() +\n  theme_bw() +\n  # Remove the y-axis title (changed from x to y since we flipped it!)\n  theme(axis.title.y = element_blank())\n\n# Display this:\nhc_complete_ggdendro\n\n\n\n\n\n\n\n\nIt can also be helpful to put an MDS plot and a dendrogram plot side-by-side (again using patchwork). The MDS plot gives us a better idea of what the dendrogram is doing behind the scenes (think about why).\n\nmcu_mds_plot &lt;- mcu_movies |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  geom_text(aes(label = film),\n            alpha = .75) +\n  labs(x = \"MDS Coordinate 1\", y = \"MDS Coordinate 2\") +\n  theme_bw()\n\nmcu_mds_plot + hc_complete_ggdendro\n\n\n\n\n\n\n\n\nUsing our dendrogram, we can assign MCU movies to clusters by cutting the tree via the cutree function. In terms of code, there are two ways to do this: (1) we pick the height of the tree to cut at or (2) we tell it how many clusters we want and it finds the corresponding height to use.\nLet’s start with cutting based on the height. From looking at the dendrogram above, I decide to break the movies into clusters based on a threshold of 10, i.e., if the complete linkage distance is \\(\\leq 10\\) then the clusters are merged together while anything above that threshold is in a different cluster.\nIf I call this cutree function at height h = 10 , you’ll see how it returns arbitrary labels to each movie:\n\nmcu_clusters &lt;- cutree(hc_complete, h = 10)\nmcu_clusters\n\n                          Ant-Man                Ant-Man & The Wasp \n                                1                                 1 \n          Avengers: Age of Ultron                Avengers: End Game \n                                1                                 2 \n           Avengers: Infinity War                     Black Panther \n                                2                                 2 \n                  Black Panther 2                       Black Widow \n                                1                                 1 \n                  Captain America        Captain America: Civil War \n                                1                                 1 \n  Captain America: Winter Soldier                    Captain Marvel \n                                1                                 2 \n                       Dr Strange Dr Strange: Multiverse of Madness \n                                1                                 1 \n                         Eternals           Guardians of the Galaxy \n                                1                                 1 \n        Guardians of the Galaxy 2                   Incredible Hulk \n                                1                                 1 \n                         Iron Man                        Iron Man 2 \n                                1                                 1 \n                       Iron Man 3                         Shang-Chi \n                                1                                 1 \n        Spider-Man: Far from Home            Spider-Man: Homecoming \n                                1                                 1 \n          Spider-Man: No Way Home                      The Avengers \n                                2                                 2 \n                 Thor: Dark World              Thor: Love & Thunder \n                                1                                 1 \n                   Thor: Ragnarok                              Thor \n                                1                                 1 \n\n\nThis returns a vector of cluster assignments for every movie (where the names of the vector are the film titles). NOTE: The numbers are arbitrary labels without meaninful order! You could replace all the 1s with Zs and all of the 2s with As, and the meaning would be the same. All that matters is which observations have the same labels. I can now add these labels to be another column for my plot above to display what the clusters are via color while denoting the height at which I cut the dendrogram:\n\ncluster_mcu_mds_plot &lt;- mcu_movies |&gt;\n  mutate(cluster = as.factor(mcu_clusters)) |&gt;\n  ggplot(aes(x = mds1, y = mds2,\n             color = cluster)) +\n  geom_text(aes(label = film),\n            alpha = .75) +\n  labs(x = \"MDS Coordinate 1\", y = \"MDS Coordinate 2\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n# Update dendogram with cut:\ncut_dendro &lt;- hc_complete_ggdendro +\n  # This is a horizontal line since its considered before the flip:\n  geom_hline(yintercept = 10, linetype = \"dashed\", \n             color = \"darkred\")\n\ncluster_mcu_mds_plot + cut_dendro"
  },
  {
    "objectID": "demos/07-clustering.html#other-dendrogram-visualization-tools",
    "href": "demos/07-clustering.html#other-dendrogram-visualization-tools",
    "title": "Demo 07: More MDS and Creating Dendrograms",
    "section": "Other dendrogram visualization tools",
    "text": "Other dendrogram visualization tools\n\ndendextend\nWe’ll learn how to make prettier versions of this in a later lab. In particular, we’ll use the dendextend package (which you’ll have to install). This allows you to prespecify \\(k\\) the clusters displayed via color that you want to add to your dendrogram:\n\n# Install dendextend if you do not have it already!\n# install.packages(\"dendextend\")\nlibrary(dendextend)\nhc_dendrogram &lt;- as.dendrogram(hc_complete)\nhc_dendrogram &lt;- set(hc_dendrogram,\n                     \"branches_k_color\", k = 4)\nplot(hc_dendrogram, ylab = \"Pairwise Distance\")\n\n\n\n\n\n\n\n\nCheck this page out for a simple tutorial of customizing dendrograms in R using ggdendro and dendextend.\n\n\nfactoextra\nAnother package with dendrogram visualizations, that we’ll use later on for PCA, is the factoextra package. This package contains many extremely useful functions for creating visualizations without too many steps. We can use the fviz_dend() function to display the dendrogram for an hclust object:\n\n# Install factoextra if you do not have it already!\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nfviz_dend(hc_complete)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\nℹ The deprecated feature was likely used in the factoextra package.\n  Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\n\n\n\n\n\n\n\n\nWe can customize this in pretty easy ways, such as changing the font size via the cex argument and displaying clusters via color:\n\nfviz_dend(hc_complete, cex = 0.25, k = 3, color_labels_by_k = TRUE) \n\n\n\n\n\n\n\n\nCheck out the function documentation for more customization options: https://rpkgs.datanovia.com/factoextra/reference/fviz_dend.html"
  },
  {
    "objectID": "demos/01-into-tidyverse.html",
    "href": "demos/01-into-tidyverse.html",
    "title": "Demo 01: Into the tidyverse",
    "section": "",
    "text": "(Broadly speaking) EDA = questions about data + wrangling + visualization\nR for Data Science: “EDA is a state of mind”, an iterative cycle:\n\ngenerate questions\nanswer via transformations and visualizations\n\nExample of questions?\n\nWhat type of variation do the variables display?\nWhat type of relationships exist between variables?\n\nGoal: develop understanding and become familiar with your data\n\nEDA is NOT a replacement for statistical inference and learning\nEDA is an important and necessary step to build intuition\n\nWe tackle the challenges of EDA with a data science workflow. An example of this according to Hadley Wickham in R for Data Science:\n\n\n\n\n\nAspects of data wrangling:\n\nimport: reading in data (e.g., read_csv())\ntidy: rows = observations, columns = variables (i.e. tabular data)\ntransform: filter observations, create new variables, summarize, etc."
  },
  {
    "objectID": "demos/01-into-tidyverse.html#what-is-exploratory-data-analysis-eda",
    "href": "demos/01-into-tidyverse.html#what-is-exploratory-data-analysis-eda",
    "title": "Demo 01: Into the tidyverse",
    "section": "",
    "text": "(Broadly speaking) EDA = questions about data + wrangling + visualization\nR for Data Science: “EDA is a state of mind”, an iterative cycle:\n\ngenerate questions\nanswer via transformations and visualizations\n\nExample of questions?\n\nWhat type of variation do the variables display?\nWhat type of relationships exist between variables?\n\nGoal: develop understanding and become familiar with your data\n\nEDA is NOT a replacement for statistical inference and learning\nEDA is an important and necessary step to build intuition\n\nWe tackle the challenges of EDA with a data science workflow. An example of this according to Hadley Wickham in R for Data Science:\n\n\n\n\n\nAspects of data wrangling:\n\nimport: reading in data (e.g., read_csv())\ntidy: rows = observations, columns = variables (i.e. tabular data)\ntransform: filter observations, create new variables, summarize, etc."
  },
  {
    "objectID": "demos/01-into-tidyverse.html#working-with-penguins",
    "href": "demos/01-into-tidyverse.html#working-with-penguins",
    "title": "Demo 01: Into the tidyverse",
    "section": "Working with penguins",
    "text": "Working with penguins\nIn R, there are many libraries or packages/groups of programs that are not permanently stored in R, so we have to load them when we want to use them. You can load an R package by typing library(package_name). (Sometimes we need to download/install the package first, as described in HW0.)\nThroughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")\n\nImport the penguins dataset by loading the palmerpenguins package using the library function and then access the data with the data() function:\n\nlibrary(palmerpenguins) \ndata(penguins)\n\nView some basic info about the penguins dataset:\n\n# displays same info as c(nrow(penguins), ncol(penguins))\ndim(penguins) \n\n[1] 344   8\n\nclass(penguins)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\ntbl (pronounced tibble) is the tidyverse way of storing tabular data, like a spreadsheet or data.frame\nI assure you that you’ll run into errors as you code in R; in fact, my attitude as a coder is that something is wrong if I never get any errors while working on a project. When you run into an error, your first reaction may be to panic and post a question to Piazza. However, checking help documentation in R can be a great way to figure out what’s going wrong. (For good or bad, I end up having to read help documentation almost every day of my life - because, well, I regularly make mistakes in R.)\nLook at the help documentation for penguins by typing help(penguins) in the Console. What are the names of the variables in this dataset? How many observations are in this dataset?\n\nhelp(penguins)\n\nYou should always look at your data before doing anything: view the first 6 (by default) rows with head()\n\nhead(penguins) # Try just typing penguins into your console, what happens?\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nIs our penguins dataset tidy?\n\nEach row = a single penguin\nEach column = different measurement about the penguins (can print out column names directly with colnames(penguins) or names(penguins))\n\nWe’ll now explore differences among the penguins using the tidyverse."
  },
  {
    "objectID": "demos/01-into-tidyverse.html#let-the-data-wrangling-begin",
    "href": "demos/01-into-tidyverse.html#let-the-data-wrangling-begin",
    "title": "Demo 01: Into the tidyverse",
    "section": "Let the data wrangling begin…",
    "text": "Let the data wrangling begin…\nFirst, load the tidyverse for exploring the data - and do NOT worry about the warning messages that will pop-up! Warning messages will tell you when other packages that are loaded may have functions replaced with the most recent package you’ve loaded. In general though, you should just be concerned when an error message pops up (errors are different than warnings!).\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWe’ll start by summarizing continuous (e.g., bill_length_mm, flipper_length_mm) and categorical (e.g., species, island) variables in different ways.\nWe can compute summary statistics for continuous variables with the summary() function:\n\nsummary(penguins$bill_length_mm)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  32.10   39.23   44.45   43.92   48.50   59.60       2 \n\n\nCompute counts of categorical variables with table() function:\n\ntable(\"island\" = penguins$island) # be careful it ignores NA values!\n\nisland\n   Biscoe     Dream Torgersen \n      168       124        52 \n\n\nHow do we remove the penguins with missing bill_length_mm values? Within the tidyverse, dplyr is a package with functions for data wrangling (because it’s within the tidyverse that means you do NOT have to load it separately with library(dplyr) after using library(tidyverse)!). It’s considered a “grammar of data manipulation”: dplyr functions are verbs, datasets are nouns.\nWe can filter() our dataset to choose observations meeting conditions:\n\nclean_penguins &lt;- filter(penguins, !is.na(bill_length_mm))\n# Use help(is.na) to see what it returns. And then observe \n# that the ! operator means to negate what comes after it.\n# This means !TRUE == FALSE (i.e., opposite of TRUE is equal to FALSE).\nnrow(penguins) - nrow(clean_penguins) # Difference in rows\n\n[1] 2\n\n\nIf we want to only consider a subset of columns in our data, we can select() variables of interest:\n\nsel_penguins &lt;- select(clean_penguins, species, island, bill_length_mm, flipper_length_mm)\nhead(sel_penguins, n = 3)\n\n# A tibble: 3 × 4\n  species island    bill_length_mm flipper_length_mm\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;             &lt;int&gt;\n1 Adelie  Torgersen           39.1               181\n2 Adelie  Torgersen           39.5               186\n3 Adelie  Torgersen           40.3               195\n\n\nWe can arrange() our dataset to sort observations by variables:\n\nbill_penguins &lt;- arrange(sel_penguins, desc(bill_length_mm)) # use desc() for descending order\nhead(bill_penguins, n = 3)\n\n# A tibble: 3 × 4\n  species   island bill_length_mm flipper_length_mm\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;             &lt;int&gt;\n1 Gentoo    Biscoe           59.6               230\n2 Chinstrap Dream            58                 181\n3 Gentoo    Biscoe           55.9               228\n\n\nWe can summarize() our dataset to one row based on functions of variables:\n\nsummarize(bill_penguins, max(bill_length_mm), median(flipper_length_mm))\n\n# A tibble: 1 × 2\n  `max(bill_length_mm)` `median(flipper_length_mm)`\n                  &lt;dbl&gt;                       &lt;dbl&gt;\n1                  59.6                         197\n\n\nWe can mutate() our dataset to create new variables:\n\nnew_penguins &lt;- mutate(bill_penguins, \n                       bill_flipper_ratio = bill_length_mm / flipper_length_mm,\n                       flipper_bill_ratio = flipper_length_mm / bill_length_mm)\nhead(new_penguins, n = 1)\n\n# A tibble: 1 × 6\n  species island bill_length_mm flipper_length_mm bill_flipper_ratio\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;             &lt;int&gt;              &lt;dbl&gt;\n1 Gentoo  Biscoe           59.6               230              0.259\n# ℹ 1 more variable: flipper_bill_ratio &lt;dbl&gt;\n\n\nHow do we perform several of these actions?\n\nhead(arrange(select(mutate(filter(penguins, !is.na(flipper_length_mm)), bill_flipper_ratio = bill_length_mm / flipper_length_mm), species, island, bill_flipper_ratio), desc(bill_flipper_ratio)), n = 1)\n\n# A tibble: 1 × 3\n  species   island bill_flipper_ratio\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Chinstrap Dream               0.320\n\n\nThat’s awfully annoying to do, and also difficult to read…"
  },
  {
    "objectID": "demos/01-into-tidyverse.html#enter-the-pipeline",
    "href": "demos/01-into-tidyverse.html#enter-the-pipeline",
    "title": "Demo 01: Into the tidyverse",
    "section": "Enter the pipeline",
    "text": "Enter the pipeline\nThe |&gt; (pipe) operator is used in the to chain commands together. Note: you can also use the tidyverse pipe %&gt;% (from magrittr), but |&gt; is the built-in pipe that is native to new versions of R without loading the tidyverse.\n|&gt; directs the data analyis pipeline: output of one function pipes into input of the next function\n\npenguins |&gt;\n  filter(!is.na(flipper_length_mm)) |&gt;\n  mutate(bill_flipper_ratio = bill_length_mm / flipper_length_mm) |&gt;\n  select(species, island, bill_flipper_ratio) |&gt;\n  arrange(desc(bill_flipper_ratio)) |&gt;\n  head(n = 5)\n\n# A tibble: 5 × 3\n  species   island bill_flipper_ratio\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Chinstrap Dream               0.320\n2 Chinstrap Dream               0.275\n3 Chinstrap Dream               0.270\n4 Chinstrap Dream               0.270\n5 Chinstrap Dream               0.268"
  },
  {
    "objectID": "demos/01-into-tidyverse.html#more-pipeline-actions",
    "href": "demos/01-into-tidyverse.html#more-pipeline-actions",
    "title": "Demo 01: Into the tidyverse",
    "section": "More pipeline actions!",
    "text": "More pipeline actions!\nInstead of head(), we can slice() our dataset to choose the observations based on the position\n\npenguins |&gt;\n  filter(!is.na(flipper_length_mm)) |&gt;\n  mutate(bill_flipper_ratio = bill_length_mm / flipper_length_mm) |&gt;\n  select(species, island, bill_flipper_ratio) |&gt;\n  arrange(desc(bill_flipper_ratio)) |&gt;\n  slice(c(1, 2, 10, 100))\n\n# A tibble: 4 × 3\n  species   island bill_flipper_ratio\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Chinstrap Dream               0.320\n2 Chinstrap Dream               0.275\n3 Chinstrap Dream               0.264\n4 Gentoo    Biscoe              0.227"
  },
  {
    "objectID": "demos/01-into-tidyverse.html#grouped-operations",
    "href": "demos/01-into-tidyverse.html#grouped-operations",
    "title": "Demo 01: Into the tidyverse",
    "section": "Grouped operations",
    "text": "Grouped operations\nWe group_by() to split our dataset into groups based on a variable’s values\n\npenguins |&gt;\n  filter(!is.na(flipper_length_mm)) |&gt;\n  group_by(island) |&gt;\n  summarize(n_penguins = n(), #counts number of rows in group\n            ave_flipper_length = mean(flipper_length_mm), \n            sum_bill_depth = sum(bill_depth_mm),\n            .groups = \"drop\") |&gt; # all levels of grouping dropping\n  arrange(desc(n_penguins)) |&gt;\n  slice(1:5)\n\n# A tibble: 3 × 4\n  island    n_penguins ave_flipper_length sum_bill_depth\n  &lt;fct&gt;          &lt;int&gt;              &lt;dbl&gt;          &lt;dbl&gt;\n1 Biscoe           167               210.          2651.\n2 Dream            124               193.          2275.\n3 Torgersen         51               191.           940.\n\n\n\ngroup_by() is only useful in a pipeline (e.g. with summarize()), and pay attention to its behavior\nspecify the .groups field to decide if observations remain grouped or not after summarizing (you can also use ungroup() for this as well)"
  },
  {
    "objectID": "demos/01-into-tidyverse.html#putting-it-all-together",
    "href": "demos/01-into-tidyverse.html#putting-it-all-together",
    "title": "Demo 01: Into the tidyverse",
    "section": "Putting it all together…",
    "text": "Putting it all together…\nAs your own exercise, create a tidy dataset where each row == an island with the following variables:\n\nnumber of penguins,\nnumber of unique species on the island (see help(unique)),\naverage body_mass_g,\nvariance (see help(var)) of bill_depth_mm\n\nPrior to making those variables, make sure to filter missings and also only consider female penguins. Then arrange the islands in order of the average body_mass_g:\n\n# INSERT YOUR CODE HERE"
  },
  {
    "objectID": "demos/03-regression.html",
    "href": "demos/03-regression.html",
    "title": "Demo 03: More Regression with Penguins",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs. Throughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")\n\nWe load the penguins data in the same way as before:\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "demos/03-regression.html#regression-with-penguins",
    "href": "demos/03-regression.html#regression-with-penguins",
    "title": "Demo 03: More Regression with Penguins",
    "section": "",
    "text": "The graphs below don’t have proper titles, axis labels, legends, etc. Please take care to do this on your own graphs. Throughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")\n\nWe load the penguins data in the same way as before:\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "demos/03-regression.html#simple-linear-regression-based-only-on-bill-length",
    "href": "demos/03-regression.html#simple-linear-regression-based-only-on-bill-length",
    "title": "Demo 03: More Regression with Penguins",
    "section": "Simple Linear Regression (based only on bill length)",
    "text": "Simple Linear Regression (based only on bill length)\nFirst, we can run a simple linear regression (the first model) based only on bill length. We can display this line via geom_smooth():\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE)\n\n\n\n\n\n\n\n\nAnd display the regression model output using summary():\n\nsummary(lm(bill_depth_mm ~ bill_length_mm, data = penguins))\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1381 -1.4263  0.0164  1.3841  4.5255 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    20.88547    0.84388  24.749  &lt; 2e-16 ***\nbill_length_mm -0.08502    0.01907  -4.459 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.922 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.05525,   Adjusted R-squared:  0.05247 \nF-statistic: 19.88 on 1 and 340 DF,  p-value: 1.12e-05\n\n\nWe can write this regression model as:\n\\[\\text{depth} \\sim N(\\beta_0 + \\beta_L \\cdot \\text{length}, \\sigma^2)\\]\nNote that \\(\\beta_0\\) is the intercept and \\(\\beta_L\\) is the slope.\nThus, our estimates are:\n\n\\(\\hat{\\beta}_0 = 20.88547\\)\n\\(\\hat{\\beta}_L = 12.43\\)\n\\(\\hat{\\sigma}^2 = 1.922^2\\)"
  },
  {
    "objectID": "demos/03-regression.html#multiple-linear-regression-additive",
    "href": "demos/03-regression.html#multiple-linear-regression-additive",
    "title": "Demo 03: More Regression with Penguins",
    "section": "Multiple Linear Regression (Additive)",
    "text": "Multiple Linear Regression (Additive)\nWe can also run the second model, which is based on length and species, but with only additive effects. First, we’ll check the counts of the species variable to ensure that the species with the highest number of observations if the reference level (i.e., the first level for a factor variable):\n\ntable(penguins$species)\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n\nLooks like we’re lucky and that the Adelie species is the most popular and is already first due to alphabetical order. What function would we need to do to re-order the variable?\nNext, we’ll fit the regression that accounts for species without an interaction - so it’s just an additive effect:\n\ndepth_lm_species_add &lt;- lm(bill_depth_mm ~ bill_length_mm + species,\n                           data = penguins)\nsummary(depth_lm_species_add)\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm + species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4529 -0.6864 -0.0508  0.5519  3.5915 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      10.59218    0.68302  15.508  &lt; 2e-16 ***\nbill_length_mm    0.19989    0.01749  11.427  &lt; 2e-16 ***\nspeciesChinstrap -1.93319    0.22416  -8.624 2.55e-16 ***\nspeciesGentoo    -5.10602    0.19142 -26.674  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9533 on 338 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.769, Adjusted R-squared:  0.7669 \nF-statistic: 375.1 on 3 and 338 DF,  p-value: &lt; 2.2e-16\n\n\nThis tells us that Chinstrap is different from Adelie and Gentoo is different from Adelie, but it does NOT tell us Chinstrap is different from Gentoo. That would require another model with a reordered species variable. Exercise: Reorder species so that Gentoo is the reference level and compare to the results above.\nWe can manually extract intercepts and coefficients to use for plotting (read the code comments!):\n\n# Calculate species-specific intercepts in order:\nintercepts &lt;- # First for `Adelie` it's just the initial intercept\n  c(coef(depth_lm_species_add)[\"(Intercept)\"],\n    # Next for `Chinstrap` it's the intercept plus the `Chinstrap` term:\n    coef(depth_lm_species_add)[\"(Intercept)\"] + \n      coef(depth_lm_species_add)[\"speciesChinstrap\"],\n    # And finally for `Gentoo` it's again the intercept plus the `Gentoo` term\n    coef(depth_lm_species_add)[\"(Intercept)\"] + \n      coef(depth_lm_species_add)[\"speciesGentoo\"])\n\n# Create a small table to store the intercept, slopes, and species:\nlines_tbl &lt;- tibble(\"intercepts\" = intercepts,\n                    # Slopes are the same for each, thus use rep()\n                    \"slopes\" = rep(coef(depth_lm_species_add)[\"bill_length_mm\"],\n                                   3),\n                    # And the levels of species:\n                    \"species\" = levels(penguins$species))\n\nWe can now plot this model by specifying the regression lines with geom_abline() using the newly constructed lines_tbl as the data for this layer:\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(data = lines_tbl,\n              aes(intercept = intercepts, slope = slopes,\n                  color = species)) +\n  labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\", \n       title = \"Bill depth versus weight by species\")\n\n\n\n\n\n\n\n\nThis is a great example of Simpson’s Paradox! We originally observed a negative linear relationship between depth and length, but now observe a positive linear relationship within species!"
  },
  {
    "objectID": "demos/03-regression.html#multiple-linear-regression-interactive",
    "href": "demos/03-regression.html#multiple-linear-regression-interactive",
    "title": "Demo 03: More Regression with Penguins",
    "section": "Multiple Linear Regression (Interactive)",
    "text": "Multiple Linear Regression (Interactive)\nNext, we can run the third model, which is based on length and species, including interaction effects. This is the default type of model displayed when we map species to the color aesthetic for the geom_smooth() layer. In the plot below, we display across both layers, geom_point() and geom_smooth() by mapping species to color in the initial ggplot canvas construction:\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\", \n       title = \"Bill depth versus weight by species\")\n\n\n\n\n\n\n\n\nWhat about the summary of this model? Is the inclusion of interaction terms relevant? Note that by default, multiplying two variables in the lm() formula below includes both the additive AND interaction terms.\n\ndepth_lm_species_int &lt;- lm(bill_depth_mm ~ bill_length_mm * species,\n                           data = penguins)\nsummary(depth_lm_species_int)\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm * species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6574 -0.6675 -0.0524  0.5383  3.5032 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     11.40912    1.13812  10.025  &lt; 2e-16 ***\nbill_length_mm                   0.17883    0.02927   6.110 2.76e-09 ***\nspeciesChinstrap                -3.83998    2.05398  -1.870 0.062419 .  \nspeciesGentoo                   -6.15812    1.75451  -3.510 0.000509 ***\nbill_length_mm:speciesChinstrap  0.04338    0.04558   0.952 0.341895    \nbill_length_mm:speciesGentoo     0.02601    0.04054   0.642 0.521590    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9548 on 336 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7662 \nF-statistic: 224.5 on 5 and 336 DF,  p-value: &lt; 2.2e-16\n\n\nThe interaction terms do NOT appear to be necessary to include. This is justified by both the lack of significance and the slight drop in adjusted R-squared."
  },
  {
    "objectID": "demos/03-regression.html#what-about-the-intercept",
    "href": "demos/03-regression.html#what-about-the-intercept",
    "title": "Demo 03: More Regression with Penguins",
    "section": "What about the intercept?",
    "text": "What about the intercept?\nRemember the meaning of the intercept term… that is not reasonable in this setting because penguins will never have bills with length of 0mm! We should update the additive model (since we found the interaction terms to not be meaningful) to remove the intercept. This can be done by adding a 0 term to the lm() formula:\n\ndepth_lm_remove_b0 &lt;- lm(bill_depth_mm ~ 0 + bill_length_mm + species,\n                         data = penguins)\nsummary(depth_lm_remove_b0)\n\n\nCall:\nlm(formula = bill_depth_mm ~ 0 + bill_length_mm + species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4529 -0.6864 -0.0508  0.5519  3.5915 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \nbill_length_mm    0.19989    0.01749  11.427  &lt; 2e-16 ***\nspeciesAdelie    10.59218    0.68302  15.508  &lt; 2e-16 ***\nspeciesChinstrap  8.65899    0.86207  10.044  &lt; 2e-16 ***\nspeciesGentoo     5.48616    0.83547   6.567 1.94e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9533 on 338 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.997, Adjusted R-squared:  0.997 \nF-statistic: 2.795e+04 on 4 and 338 DF,  p-value: &lt; 2.2e-16\n\n\nWhat changed in the summary output? Why did that occur?\nWe can copy-and-paste our code from above to add these appropriate regression lines:\n\n# Calculate species-specific intercepts in order:\nnew_intercepts &lt;- # First for `Adelie` \n  c(coef(depth_lm_remove_b0)[\"speciesAdelie\"],\n    # Next for `Chinstrap` \n    coef(depth_lm_remove_b0)[\"speciesChinstrap\"],\n    # And finally for `Gentoo` \n    coef(depth_lm_remove_b0)[\"speciesGentoo\"])\n\n# Create a small table to store the intercept, slopes, and species:\nnew_lines_tbl &lt;- \n  tibble(\"intercepts\" = new_intercepts,\n         # Slopes are the same for each, thus use rep()\n         \"slopes\" = rep(coef(depth_lm_remove_b0)[\"bill_length_mm\"],\n                        3),\n         # And the levels of species:\n         \"species\" = levels(penguins$species))\n\nAgain, create the display:\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(data = new_lines_tbl,\n              aes(intercept = intercepts, slope = slopes,\n                  color = species)) +\n  labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\", \n       title = \"Bill depth versus weight by species\")\n\n\n\n\n\n\n\n\nWhy is this the same display as before? Here’s a great description of why we observe a higher R-squared with the intercept-term excluded from the model."
  },
  {
    "objectID": "demos.html",
    "href": "demos.html",
    "title": "Demos",
    "section": "",
    "text": "Demo\nDate\nTitle\nDemo file\n\n\n\n\n1\nJan 13\nInto the tidyverse\nHTML\n\n\n2\nFeb 17\nScatterplots and Linear Regression\nHTML\n\n\n3\nFeb 19\nMore Regression with Penguins\nHTML\n\n\n4\nMar 10\nNonlinear Regression and Pairs Plots\nHTML\n\n\n5\nMar 12\nContour Plots, Heat Maps, and Into High-Dimensional Data\nHTML\n\n\n6\nMar 17\nVisualizing Distances for High-Dimensional Data\nHTML\n\n\n7\nMar 19\nMore MDS and Creating Dendrograms\nHTML\n\n\n8\nMar 24\nPrincipal Component Analysis\nHTML\n\n\n9\nMar 26\nVisualizing Trends\nHTML\n\n\n10\nMar 31\nVisualizing time series data\nHTML",
    "crumbs": [
      "Demos"
    ]
  },
  {
    "objectID": "lectures/18-trends.html#announcements-previously-and-today",
    "href": "lectures/18-trends.html#announcements-previously-and-today",
    "title": "Visualizing Trends",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW6 is due Wednesday March 26th by 11:59 PM ET\nYou have Lab 8 this Friday\nWe do NOT have in-class lecture on Wednesday! I will post a recording\n\n\n\nCommon workflow:\n\nReduce the data to a few “useful” dimensions\nPlot those “useful” dimensions\n\nLast two classes:\n\nReduce the data by summarizing pairs of subjects with one distance.\nVisualize distances using multi-dimensional scaling or dendrograms.\n\nHow can we reduce the data without distances?\nPrincipal Component Analysis (PCA) is by far the most popular way"
  },
  {
    "objectID": "lectures/18-trends.html#longitudinal-data-and-time-series-structure",
    "href": "lectures/18-trends.html#longitudinal-data-and-time-series-structure",
    "title": "Visualizing Trends",
    "section": "Longitudinal data and time series structure",
    "text": "Longitudinal data and time series structure\n\nConsider a single observation measured across time\n\n\n\n\nVariable\n\\(T_1\\)\n\\(T_2\\)\n\\(\\dots\\)\n\\(T_J\\)\n\n\n\n\n\\(X_1\\)\n\\(x_{11}\\)\n\\(x_{12}\\)\n\\(\\dots\\)\n\\(x_{1J}\\)\n\n\n\\(X_2\\)\n\\(x_{21}\\)\n\\(x_{22}\\)\n\\(\\dots\\)\n\\(x_{2J}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\dots\\)\n\\(\\vdots\\)\n\n\n\\(X_P\\)\n\\(x_{P1}\\)\n\\(x_{P2}\\)\n\\(\\dots\\)\n\\(x_{PJ}\\)\n\n\n\n\nWith \\(N\\) observations we have \\(N\\) of these matrices\nTime may consist of regularly spaced intervals\n\nFor example, \\(T_1 = t\\), \\(T_2 = t + h\\), \\(T_3 = t + 2h\\), etc.\n\nIrregularly spaced intervals, then work with the raw \\(T_1,T_2,...\\)"
  },
  {
    "objectID": "lectures/18-trends.html#example-statistics-phds-by-year",
    "href": "lectures/18-trends.html#example-statistics-phds-by-year",
    "title": "Visualizing Trends",
    "section": "Example: Statistics PhDs by year",
    "text": "Example: Statistics PhDs by year\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  theme_light() +\n  labs(x = \"Year\", y = \"Number of PhDs\", title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#example-statistics-phds-by-year-1",
    "href": "lectures/18-trends.html#example-statistics-phds-by-year-1",
    "title": "Visualizing Trends",
    "section": "Example: Statistics PhDs by year",
    "text": "Example: Statistics PhDs by year\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year), \n                     labels = unique(stat_phd_year_summary$year)) + \n  theme_light() +\n  labs(x = \"Year\", y = \"Number of PhDs\", title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#add-lines-to-emphasize-order",
    "href": "lectures/18-trends.html#add-lines-to-emphasize-order",
    "title": "Visualizing Trends",
    "section": "Add lines to emphasize order",
    "text": "Add lines to emphasize order\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#drop-points-to-emphasize-trends",
    "href": "lectures/18-trends.html#drop-points-to-emphasize-trends",
    "title": "Visualizing Trends",
    "section": "Drop points to emphasize trends",
    "text": "Drop points to emphasize trends\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#can-fill-the-area-under-the-line",
    "href": "lectures/18-trends.html#can-fill-the-area-under-the-line",
    "title": "Visualizing Trends",
    "section": "Can fill the area under the line",
    "text": "Can fill the area under the line\n\nstat_phd_year_summary |&gt;\n  ggplot(aes(x = year, y = n_phds)) +\n  geom_area(fill = \"darkblue\", alpha = 0.5) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#several-time-series-do-not-only-use-points",
    "href": "lectures/18-trends.html#several-time-series-do-not-only-use-points",
    "title": "Visualizing Trends",
    "section": "Several time series? Do NOT only use points",
    "text": "Several time series? Do NOT only use points\n\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_point() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"bottom\", legend.text = element_text(size = 7)) +\n  labs(x = \"Year\", y = \"Number of PhDs\",\n       title = \"Number of Statistics-related PhDs awarded over time\",\n       color = \"Field\")"
  },
  {
    "objectID": "lectures/18-trends.html#several-time-series-use-lines",
    "href": "lectures/18-trends.html#several-time-series-use-lines",
    "title": "Visualizing Trends",
    "section": "Several time series? Use lines!",
    "text": "Several time series? Use lines!\n\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#using-ggrepel-to-directly-label-lines",
    "href": "lectures/18-trends.html#using-ggrepel-to-directly-label-lines",
    "title": "Visualizing Trends",
    "section": "Using ggrepel to directly label lines",
    "text": "Using ggrepel to directly label lines\n\nstats_phds_2017 &lt;- stats_phds |&gt; filter(year == 2017)\n\nlibrary(ggrepel)\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  # Add the labels:\n  geom_text_repel(data = stats_phds_2017, aes(label = field),\n                  size = 3, \n                  # Drop the segment connection:\n                  segment.color = NA, \n                  # Move labels up or down based on overlap\n                  direction = \"y\",\n                  # Try to align the labels horizontally on the left hand side\n                  hjust = \"left\") +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year),\n                     # Update the limits so that there is some padding on the\n                     # x-axis but don't label the new maximum\n                     limits = c(min(stat_phd_year_summary$year),\n                                max(stat_phd_year_summary$year) + 3)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#using-ggrepel-to-directly-label-lines-output",
    "href": "lectures/18-trends.html#using-ggrepel-to-directly-label-lines-output",
    "title": "Visualizing Trends",
    "section": "Using ggrepel to directly label lines",
    "text": "Using ggrepel to directly label lines"
  },
  {
    "objectID": "lectures/18-trends.html#using-gghighlight-instead",
    "href": "lectures/18-trends.html#using-gghighlight-instead",
    "title": "Visualizing Trends",
    "section": "Using gghighlight instead",
    "text": "Using gghighlight instead\n\nlibrary(gghighlight)\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  gghighlight()  +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#using-gghighlight-instead-output",
    "href": "lectures/18-trends.html#using-gghighlight-instead-output",
    "title": "Visualizing Trends",
    "section": "Using gghighlight instead",
    "text": "Using gghighlight instead"
  },
  {
    "objectID": "lectures/18-trends.html#using-gghighlight-instead-1",
    "href": "lectures/18-trends.html#using-gghighlight-instead-1",
    "title": "Visualizing Trends",
    "section": "Using gghighlight instead",
    "text": "Using gghighlight instead\n\nlibrary(gghighlight)\nstats_phds |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  gghighlight(line_label_type = \"sec_axis\")  +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#using-gghighlight-instead-1-output",
    "href": "lectures/18-trends.html#using-gghighlight-instead-1-output",
    "title": "Visualizing Trends",
    "section": "Using gghighlight instead",
    "text": "Using gghighlight instead"
  },
  {
    "objectID": "lectures/18-trends.html#how-do-we-plot-many-lines-not-like-this",
    "href": "lectures/18-trends.html#how-do-we-plot-many-lines-not-like-this",
    "title": "Visualizing Trends",
    "section": "How do we plot many lines? NOT LIKE THIS!",
    "text": "How do we plot many lines? NOT LIKE THIS!\n\nphd_field |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#how-do-we-plot-many-lines-not-like-this-output",
    "href": "lectures/18-trends.html#how-do-we-plot-many-lines-not-like-this-output",
    "title": "Visualizing Trends",
    "section": "How do we plot many lines? NOT LIKE THIS!",
    "text": "How do we plot many lines? NOT LIKE THIS!"
  },
  {
    "objectID": "lectures/18-trends.html#instead-we-highlight-specific-lines",
    "href": "lectures/18-trends.html#instead-we-highlight-specific-lines",
    "title": "Visualizing Trends",
    "section": "Instead we highlight specific lines",
    "text": "Instead we highlight specific lines\n\nphd_field |&gt;\n  filter(!(field %in% c(\"Biometrics and biostatistics\", \"Statistics (mathematics)\"))) |&gt;\n  ggplot() +\n  # Add the background lines - need to specify the group to be the field\n  geom_line(aes(x = year, y = n_phds, group = field),\n            color = \"gray\", size = .5, alpha = .5) +\n  # Now add the layer with the lines of interest:\n  geom_line(data = filter(phd_field,\n                          # Note this is just the opposite of the above since ! is removed\n                          field %in% c(\"Biometrics and biostatistics\", \n                                       \"Statistics (mathematics)\")),\n            aes(x = year, y = n_phds, color = field),\n            # Make the size larger\n            size = .75, alpha = 1) +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"bottom\", \n        # Drop the panel lines making the gray difficult to see\n        panel.grid = element_blank()) +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#instead-we-highlight-specific-lines-output",
    "href": "lectures/18-trends.html#instead-we-highlight-specific-lines-output",
    "title": "Visualizing Trends",
    "section": "Instead we highlight specific lines",
    "text": "Instead we highlight specific lines"
  },
  {
    "objectID": "lectures/18-trends.html#or-you-can-use-gghighlight-instead",
    "href": "lectures/18-trends.html#or-you-can-use-gghighlight-instead",
    "title": "Visualizing Trends",
    "section": "Or you can use gghighlight instead",
    "text": "Or you can use gghighlight instead\n\nphd_field |&gt;\n  ggplot(aes(x = year, y = n_phds, color = field)) +\n  geom_line() +\n  gghighlight(field %in% c(\"Biometrics and biostatistics\", \"Statistics (mathematics)\"),\n              line_label_type = \"sec_axis\") +\n  scale_x_continuous(breaks = unique(stat_phd_year_summary$year),\n                     labels = unique(stat_phd_year_summary$year)) +\n  theme_light() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Year\", y = \"Number of PhDs\", color = \"Field\",\n       title = \"Number of Statistics-related PhDs awarded over time\")"
  },
  {
    "objectID": "lectures/18-trends.html#or-you-can-use-gghighlight-instead-output",
    "href": "lectures/18-trends.html#or-you-can-use-gghighlight-instead-output",
    "title": "Visualizing Trends",
    "section": "Or you can use gghighlight instead",
    "text": "Or you can use gghighlight instead"
  },
  {
    "objectID": "lectures/18-trends.html#what-about-nightingales-rose-diagram",
    "href": "lectures/18-trends.html#what-about-nightingales-rose-diagram",
    "title": "Visualizing Trends",
    "section": "What about Nightingale’s rose diagram?",
    "text": "What about Nightingale’s rose diagram?"
  },
  {
    "objectID": "lectures/18-trends.html#what-about-nightingales-rose-diagram-1",
    "href": "lectures/18-trends.html#what-about-nightingales-rose-diagram-1",
    "title": "Visualizing Trends",
    "section": "What about Nightingale’s rose diagram?",
    "text": "What about Nightingale’s rose diagram?"
  },
  {
    "objectID": "lectures/18-trends.html#what-about-displaying-lines-instead",
    "href": "lectures/18-trends.html#what-about-displaying-lines-instead",
    "title": "Visualizing Trends",
    "section": "What about displaying lines instead?",
    "text": "What about displaying lines instead?"
  },
  {
    "objectID": "lectures/18-trends.html#recap-and-next-steps",
    "href": "lectures/18-trends.html#recap-and-next-steps",
    "title": "Visualizing Trends",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nDiscussed various aspects of visualizing trends\nWhen visualizing many lines, often useful to consider highlighting a small subset\n\n\n\nHW6 is due TONIGHT and you have lab on Friday!\nI will not have office hours today\n\n\n\n\nNext time: Time series, autocorrelation, and seasonal decomposition\nRecommended reading: CW CH 13 Visualizing time series and other functions of an independent variable, CW CH 14 Visualizing trends"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#announcements-previously-and-today",
    "href": "lectures/04-power-multiple-testing.html#announcements-previously-and-today",
    "title": "Power and multiple testing",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW1 is due this Wednesday Jan 29 by 11:59 PM\nYou have Lab 3 again on Friday\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Wednesdays @ 11 AM\n\n\n\n\n\nMain estimators: \\(\\underbrace{\\hat{p}_1,\\dots,\\hat{p}_K}_{\\text{proportions}}\\) for \\(K\\)-many categories\nChi-square test is the main statistical test for 1D categorical data, tests \\(H_0: p_1 = \\cdots = p_K\\)\nCan also make confidence intervals for \\(\\hat{p}_1,\\dots,\\hat{p}_K\\) (just multiply CIs by \\(n\\))\n\n\n\n\n\nTODAY:\n\nInterpreting CIs on graphs is tricky and have to be careful with multiple testing"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#graphics-versus-statistical-inference",
    "href": "lectures/04-power-multiple-testing.html#graphics-versus-statistical-inference",
    "title": "Power and multiple testing",
    "section": "Graphics versus Statistical Inference",
    "text": "Graphics versus Statistical Inference\n\nReminder Anscombe’s Quartet: where statistical inference was the same but the graphics were very different\n\n\n\nThe opposite can be true! Graphics are the same, but statistical inference is very different…"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14",
    "href": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14",
    "title": "Power and multiple testing",
    "section": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)",
    "text": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-1",
    "href": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-1",
    "title": "Power and multiple testing",
    "section": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)",
    "text": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-2",
    "href": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-2",
    "title": "Power and multiple testing",
    "section": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)",
    "text": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-3",
    "href": "lectures/04-power-multiple-testing.html#example-3-categories-p_a-12-p_b-p_c-14-3",
    "title": "Power and multiple testing",
    "section": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)",
    "text": "Example: 3 categories, \\(p_A = 1/2,\\ p_B = p_C = 1/4\\)"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#power-under-this-scenario-2n4-n4-n4",
    "href": "lectures/04-power-multiple-testing.html#power-under-this-scenario-2n4-n4-n4",
    "title": "Power and multiple testing",
    "section": "Power under this scenario: (2n/4, n/4, n/4)",
    "text": "Power under this scenario: (2n/4, n/4, n/4)"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#how-do-we-combine-graphs-with-inference",
    "href": "lectures/04-power-multiple-testing.html#how-do-we-combine-graphs-with-inference",
    "title": "Power and multiple testing",
    "section": "How do we combine graphs with inference?",
    "text": "How do we combine graphs with inference?\n\nSimply add \\(p\\)-values (or other info) to graph via text\nAdd confidence intervals to the graph\n\n\nNeed to remember what each CI is for!\nOur CIs on previous slides are for each \\(\\hat{p}_j\\) marginally, NOT jointly\nHave to be careful with multiple testing…"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#cis-will-visually-capture-uncertainty-in-estimates",
    "href": "lectures/04-power-multiple-testing.html#cis-will-visually-capture-uncertainty-in-estimates",
    "title": "Power and multiple testing",
    "section": "CIs will visually capture uncertainty in estimates",
    "text": "CIs will visually capture uncertainty in estimates"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#rough-rules-for-comparing-cis-on-bar-charts",
    "href": "lectures/04-power-multiple-testing.html#rough-rules-for-comparing-cis-on-bar-charts",
    "title": "Power and multiple testing",
    "section": "Rough rules for comparing CIs on bar charts",
    "text": "Rough rules for comparing CIs on bar charts\n\n\nComparing overlap of two CIs is NOT exactly the same as directly testing for a significant difference…\n\nReally you want CI( \\(\\hat{p}_1 - \\hat{p}_2\\) ), not CI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) )\nCI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) ) not overlapping implies \\(0 \\notin\\) CI( \\(\\hat{p}_1 - \\hat{p}_2\\) )\nHowever CI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) ) overlapping DOES NOT imply \\(0 \\in\\) CI( \\(\\hat{p}_1 - \\hat{p}_2\\) )\n\n\nRoughly speaking:\n\nIf CIs don’t overlap \\(\\rightarrow\\) significant difference\nIf CIs overlap a little \\(\\rightarrow\\) ambiguous\nIf CIs overlap a lot \\(\\rightarrow\\) no significant difference\n\n\n\n\nBut if we’re comparing more than two CIs simultaneously, we need to account for multiple testing!\n\nWhen you look for all non-overlapping CIs: making \\(\\binom{K}{2} = \\frac{K!}{2!(K-2)!}\\) pairwise tests in your head!"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#corrections-for-multiple-testing",
    "href": "lectures/04-power-multiple-testing.html#corrections-for-multiple-testing",
    "title": "Power and multiple testing",
    "section": "Corrections for multiple testing",
    "text": "Corrections for multiple testing\n\n\nIn those bar plots, when we determine whether CIs overlap we make 3 comparisons:\n\nA vs B\nA vs C\nB vs C\n\n\nThis is a multiple testing issue\n\n\n\n\nIn short: we will make Type 1 errors (chance of false rejecting) more than 5% of the time!\nReminder: Type 1 error = Rejecting \\(H_0\\) when \\(H_0\\) is true\ne.g., CIs don’t overlap but actually \\(H_0: p_A = p_B\\) is true\nIf only interested in A vs B and nothing else, then just construct 95% CI for A vs B and control error rate at 5%\nHowever, if we construct several CIs, where A vs B is just one comparison we make, our Type 1 error rate &gt; 5%!"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#corrections-for-multiple-testing-1",
    "href": "lectures/04-power-multiple-testing.html#corrections-for-multiple-testing-1",
    "title": "Power and multiple testing",
    "section": "Corrections for multiple testing",
    "text": "Corrections for multiple testing\n\nVast literature on corrections for multiple testing (beyond the scope of this class… but in my thesis!)\nBut you should understand the following:\n\nCorrections for multiple testing inflate \\(p\\)-values (i.e., make them bigger)\nEquivalently, they inflate CIs (i.e., make them wider)\nPurpose of these corrections is to control Type 1 error rate \\(\\leq 5\\%\\)\n\n\n\n\nWe’ll focus on the Bonferroni correction, which inflates \\(p\\)-values the most but is easy to implement and very popular:\n\nWe usually reject null hypothesis when \\(p\\)-value \\(\\leq .05\\)\nBonferroni: if making \\(K\\) comparisons, reject only if \\(p\\)-value \\(\\leq .05/K\\)\nFor CIs: instead of plotting 95% CIs, we plot (1 - \\(0.05/K\\))% CIs\n\ne.g., for \\(K = 3\\) then plot 98.3% CIs"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#impact-of-bonferroni-correction-on-cis",
    "href": "lectures/04-power-multiple-testing.html#impact-of-bonferroni-correction-on-cis",
    "title": "Power and multiple testing",
    "section": "Impact of Bonferroni correction on CIs…",
    "text": "Impact of Bonferroni correction on CIs…"
  },
  {
    "objectID": "lectures/04-power-multiple-testing.html#recap-and-next-steps",
    "href": "lectures/04-power-multiple-testing.html#recap-and-next-steps",
    "title": "Power and multiple testing",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nGraphs with the same trends can display very different statistical significance (largely due to sample size)\nCan visualize CIs for each \\(\\hat{p}_1\\), \\(\\dots\\), \\(\\hat{p}_K\\), but need to deal with multiple testing\n\n\n\n\n\nHW1 is due Wednesday and you have Lab 3 on Friday!\nNext time: Visualizations and inference for 2D categorical data\nRecommended reading: CW Chapter 11 Visualizing nested proportions"
  },
  {
    "objectID": "lectures/11-regression-inference.html#announcements-previously-and-today",
    "href": "lectures/11-regression-inference.html#announcements-previously-and-today",
    "title": "Inference with Linear Regression",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW4 is due TONIGHT by 11:59 PM and you have Lab 6 again on Friday!\nTake-home exam is next week Wednesday Feb 26th\nHere’s how the exam will work:\n\nI’ll post the exam Monday evening, and it’s due Wednesday by 11:59 PM EDT (Feb 26th)\nExam will cover material from HW 1-4 and Labs 1-6\nQuestions will be similar to homeworks but more open-ended, e.g, instead of “make a side-by-side violin plot…” I’ll ask “Make a plot that compares the conditional distributions…”\nThere will NOT be class on Wednesday Feb 26th\nConflict Feb 26th? Let me know ASAP! Day-of accommodations will NOT be made, late submissions will NOT be accepted\n\n\n\n\nScatterplots are the most common visual for 2D quantitative variables\n\nMany ways to incorporate additional dimensions in scatterplots, e.g., color and shape\n\nLinear regression is by far the most common model for describing the relationship between 2+ quantitative variables\n\nCan also: transform the outcome, transform the covariates, do nonparametric “smoothing”\n\nUse graphs to assess linear regression assumptions, i.e., residual diagnostic plots\n\n\n\n\nTODAY: More linear regression and inference with linear regression"
  },
  {
    "objectID": "lectures/11-regression-inference.html#displaying-trend-lines-linear-regression",
    "href": "lectures/11-regression-inference.html#displaying-trend-lines-linear-regression",
    "title": "Inference with Linear Regression",
    "section": "Displaying trend lines: linear regression",
    "text": "Displaying trend lines: linear regression\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "lectures/11-regression-inference.html#assessing-assumptions-of-linear-regression",
    "href": "lectures/11-regression-inference.html#assessing-assumptions-of-linear-regression",
    "title": "Inference with Linear Regression",
    "section": "Assessing assumptions of linear regression",
    "text": "Assessing assumptions of linear regression\nLinear regression assumes \\(Y_i \\overset{iid}{\\sim} N(\\beta_0 + \\beta_1 X_i, \\sigma^2)\\)\n\nIf this is true, then \\(Y_i - \\hat{Y}_i \\overset{iid}{\\sim} N(0, \\sigma^2)\\)\n\n\nPlot residuals against \\(\\hat{Y}_i\\), residuals vs fit plot\n\nUsed to assess linearity, any divergence from mean 0\nUsed to assess equal variance, i.e., if \\(\\sigma^2\\) is homogenous across predictions/fits \\(\\hat{Y}_i\\)\n\n\n\nMore difficult to assess the independence and fixed \\(X\\) assumptions\n\nMake these assumptions based on subject-matter knowledge"
  },
  {
    "objectID": "lectures/11-regression-inference.html#residual-vs-fit-plots",
    "href": "lectures/11-regression-inference.html#residual-vs-fit-plots",
    "title": "Inference with Linear Regression",
    "section": "Residual vs fit plots",
    "text": "Residual vs fit plots\n\nlin_reg &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\n\ntibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")"
  },
  {
    "objectID": "lectures/11-regression-inference.html#residual-vs-fit-plots-1",
    "href": "lectures/11-regression-inference.html#residual-vs-fit-plots-1",
    "title": "Inference with Linear Regression",
    "section": "Residual vs fit plots",
    "text": "Residual vs fit plots\n\ntibble(fits = fitted(lin_reg), residuals = residuals(lin_reg)) |&gt;\n  ggplot(aes(x = fits, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth()"
  },
  {
    "objectID": "lectures/11-regression-inference.html#examples-of-residual-vs-fit-plots",
    "href": "lectures/11-regression-inference.html#examples-of-residual-vs-fit-plots",
    "title": "Inference with Linear Regression",
    "section": "Examples of Residual-vs-Fit Plots",
    "text": "Examples of Residual-vs-Fit Plots"
  },
  {
    "objectID": "lectures/11-regression-inference.html#more-fun-with-penguins",
    "href": "lectures/11-regression-inference.html#more-fun-with-penguins",
    "title": "Inference with Linear Regression",
    "section": "More fun with penguins…",
    "text": "More fun with penguins…\nDemo 03: Walk through an example of plotting/running different linear regression models\n\nOutcome: bill depth (in mm)\nCovariates: bill length (in mm) and species\n\n\nLinear regression models we will consider:\n\nbill_depth_mm ~ bill_length_mm\nbill_depth_mm ~ bill_length_mm + species\nbill_depth_mm ~ bill_length_mm + species + bill_length_mm \\(\\times\\) species"
  },
  {
    "objectID": "lectures/11-regression-inference.html#model-1-bill_depth_mm-bill_length_mm",
    "href": "lectures/11-regression-inference.html#model-1-bill_depth_mm-bill_length_mm",
    "title": "Inference with Linear Regression",
    "section": "Model 1: bill_depth_mm ~ bill_length_mm",
    "text": "Model 1: bill_depth_mm ~ bill_length_mm"
  },
  {
    "objectID": "lectures/11-regression-inference.html#model-1-bill_depth_mm-bill_length_mm-1",
    "href": "lectures/11-regression-inference.html#model-1-bill_depth_mm-bill_length_mm-1",
    "title": "Inference with Linear Regression",
    "section": "Model 1: bill_depth_mm ~ bill_length_mm",
    "text": "Model 1: bill_depth_mm ~ bill_length_mm\n\nsummary(lm(bill_depth_mm ~ bill_length_mm, data = penguins))\n\n\nCall:\nlm(formula = bill_depth_mm ~ bill_length_mm, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1381 -1.4263  0.0164  1.3841  4.5255 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    20.88547    0.84388  24.749  &lt; 2e-16 ***\nbill_length_mm -0.08502    0.01907  -4.459 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.922 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.05525,   Adjusted R-squared:  0.05247 \nF-statistic: 19.88 on 1 and 340 DF,  p-value: 1.12e-05"
  },
  {
    "objectID": "lectures/11-regression-inference.html#how-are-the-intercept-and-slope-estimated",
    "href": "lectures/11-regression-inference.html#how-are-the-intercept-and-slope-estimated",
    "title": "Inference with Linear Regression",
    "section": "How are the intercept and slope estimated?",
    "text": "How are the intercept and slope estimated?\n\nWe have data \\((X_i, Y_i)\\). Want to estimate \\(\\beta_0\\) and \\(\\beta_1\\), where \\(\\mathbb{E}[Y | X] = \\beta_0 + \\beta_1 X\\)\nIf we had \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), then \\(\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\)\nThe estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are obtained by solving\n\\[\\arg \\min_{\\beta_0,\\beta_1} \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\\]\n\nRemember that \\(\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\), so the above is saying: “Give me the \\(\\hat{Y}_i\\) such that \\((Y_i - \\hat{Y}_i)^2\\) is minimized, on average”\n\nThe estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are:\n\\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}\\]\n\\[\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X}\\]"
  },
  {
    "objectID": "lectures/11-regression-inference.html#assessing-the-fit-of-linear-regression",
    "href": "lectures/11-regression-inference.html#assessing-the-fit-of-linear-regression",
    "title": "Inference with Linear Regression",
    "section": "Assessing the Fit of Linear Regression",
    "text": "Assessing the Fit of Linear Regression\n\nIntuitively, the more \\(X\\) and \\(Y\\) are correlated, the better the fit of the linear regression\nCorrelation is defined as\n\\[\\rho = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^n (X_i - \\bar{X})^2 \\cdot \\sum_{i=1}^n (Y_i - \\bar{Y})^2}} = \\frac{\\text{Cov}(X,Y)}{ \\sqrt{\\text{Var}(X) \\cdot \\text{Var}(Y)} }\\]\n\nCorrelation is just a standardized covariance, where \\(-1 \\leq \\rho \\leq 1\\).\nMore generally, \\(R^2\\) measures the fraction of variability in the outcome accounted by the covariates:\n\n\\[R^2 = 1 - \\frac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2} = 1 - \\frac{\\text{SS}_{\\text{residuals}}}{\\text{SS}_{\\text{total}}}\\]\nThe higher \\(R^2\\), the more the association. When linear regression has one covariate, \\(R = \\rho\\)"
  },
  {
    "objectID": "lectures/11-regression-inference.html#multiple-linear-regression",
    "href": "lectures/11-regression-inference.html#multiple-linear-regression",
    "title": "Inference with Linear Regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nLet’s say we have a bunch of covariates \\(X_1,X_2,\\dots,X_p\\)\nThe statistical model for multiple linear regression is\n\\[Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_1 X_{i1} + \\cdots + \\beta_k X_{ip}, \\sigma^2), \\hspace{0.1in} \\text{for all } i=1,\\dots,n\\]\n\nCovariates can be quadratic, cubic, etc. forms of other covariates, so this is quite flexible\nHow do we know when we’ve included the “right” covariates?\nThe higher \\(R^2\\), the more the association. So, maximize \\(R^2\\)?\n\n\n\nHowever, adding more covariates always increases \\(R^2\\). Better to look at “adjusted \\(R^2\\)”, which accounts for this\nAlso common: AIC and BIC (smaller is better)"
  },
  {
    "objectID": "lectures/11-regression-inference.html#special-case---categorical-variables",
    "href": "lectures/11-regression-inference.html#special-case---categorical-variables",
    "title": "Inference with Linear Regression",
    "section": "Special Case - Categorical Variables",
    "text": "Special Case - Categorical Variables\nCan include categorical variables in multiple linear regression, but need to code them as “dummy variables” (i.e., indicator variables)\nSay a categorical variable has \\(k \\geq 2\\) levels. Need to create \\((k-1)\\) indicator variables, equal to 1 for one category and 0 otherwise\nImportant: Categorical variable may be coded numerically (e.g., Agree = 1, Disagree = -1, Not Sure = 0)\n\nIf you put this variable straight into lm(), it will fit a very different model!"
  },
  {
    "objectID": "lectures/11-regression-inference.html#understanding-the-categorical-variables-example",
    "href": "lectures/11-regression-inference.html#understanding-the-categorical-variables-example",
    "title": "Inference with Linear Regression",
    "section": "Understanding the Categorical Variables Example",
    "text": "Understanding the Categorical Variables Example\nExample: Penguins species: Adelie, Chinstrap, Gentoo. There are \\(k = 3\\) levels.\nCreate an indicator for Chinstrap and Gentoo: \\(I_C\\) and \\(I_G\\).\n\nIf \\(I_C = I_G = 0\\), then the penguin must be Adelie\n\nThe statistical model would be \\(Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_C I_C + \\beta_G I_G, \\sigma^2)\\)\n\n\\(\\beta_0\\): \n\\(\\beta_0 + \\beta_C\\): \n\\(\\beta_0 + \\beta_G\\): \nSignificant \\(\\beta_C\\) \\(\\rightarrow\\) \nSignificant \\(\\beta_G\\) \\(\\rightarrow\\) \nHow to compare Chinstrap and Gentoo?"
  },
  {
    "objectID": "lectures/11-regression-inference.html#understanding-interactions-categorical-example",
    "href": "lectures/11-regression-inference.html#understanding-interactions-categorical-example",
    "title": "Inference with Linear Regression",
    "section": "Understanding Interactions (Categorical Example)",
    "text": "Understanding Interactions (Categorical Example)\n\nSay we also have a quantitative variable \\(X\\) (bill length). Consider two statistical models:\n\n\\(Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_X X + \\beta_C I_C + \\beta_G I_G, \\sigma^2)\\)\n\\(Y_i \\stackrel{iid}{\\sim} N(\\beta_0 + \\beta_X X + \\beta_C I_C + \\beta_G I_G + \\beta_{CX} I_C X + \\beta_{GX} I_G X, \\sigma^2)\\)\n\n\n\n\nFor Model 1…\n\nThe intercept for Adelie is \\(\\beta_0\\); for Chinstrap it is \\(\\beta_0 + \\beta_C\\); for Gentoo it is \\(\\beta_0 + \\beta_G\\)\nThe slope for all species is \\(\\beta_X\\).\n\n\n\n\n\nFor Model 2…\n\nThe intercept for Adelie is \\(\\beta_0\\); for Chinstrap it is \\(\\beta_0 + \\beta_C\\); for Gentoo it is \\(\\beta_0 + \\beta_G\\).\nThe slope for Adelie is \\(\\beta_X\\); for Chinstrap it is \\(\\beta_X + \\beta_{CX}\\); for Gentoo it is \\(\\beta_X + \\beta_{GX}\\)\n\n\n\n\n\nSignificant coefficient for categorical variables by themselves? Significantly different intercepts\nSignificant coefficient for interactions with categorical variables? Significantly different slopes"
  },
  {
    "objectID": "lectures/11-regression-inference.html#model-2-bill_depth_mm-bill_length_mm-species",
    "href": "lectures/11-regression-inference.html#model-2-bill_depth_mm-bill_length_mm-species",
    "title": "Inference with Linear Regression",
    "section": "Model 2: bill_depth_mm ~ bill_length_mm + species",
    "text": "Model 2: bill_depth_mm ~ bill_length_mm + species"
  },
  {
    "objectID": "lectures/11-regression-inference.html#model-3-bill_depth_mm-bill_length_mm-species-bill_length_mm-times-species",
    "href": "lectures/11-regression-inference.html#model-3-bill_depth_mm-bill_length_mm-species-bill_length_mm-times-species",
    "title": "Inference with Linear Regression",
    "section": "Model 3: bill_depth_mm ~ bill_length_mm + species + bill_length_mm \\(\\times\\) species",
    "text": "Model 3: bill_depth_mm ~ bill_length_mm + species + bill_length_mm \\(\\times\\) species"
  },
  {
    "objectID": "lectures/11-regression-inference.html#a-few-linear-regression-warnings",
    "href": "lectures/11-regression-inference.html#a-few-linear-regression-warnings",
    "title": "Inference with Linear Regression",
    "section": "A Few Linear Regression Warnings",
    "text": "A Few Linear Regression Warnings\n\nSimpson’s Paradox\n\nThere is a negative linear relationship between two variables but a positive linear relationship within every subpopulation\nIn these cases, subgroup analysis is especially important\n\n\n\n\nIs the intercept meaningful?\n\nThink about whether \\(X = 0\\) makes scientific sense for a particular variable before you interpret the intercept\n\n\n\n\n\nInterpolation versus Extrapolation\n\nInterpolation is defined as prediction within the range of a variable\nExtrapolation is defined as prediction outside the range of a variable\nGenerally speaking, interpolation is more reliable than extrapolation (less sensitive to model misspecification)"
  },
  {
    "objectID": "lectures/11-regression-inference.html#extrapolation-example",
    "href": "lectures/11-regression-inference.html#extrapolation-example",
    "title": "Inference with Linear Regression",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/11-regression-inference.html#extrapolation-example-1",
    "href": "lectures/11-regression-inference.html#extrapolation-example-1",
    "title": "Inference with Linear Regression",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/11-regression-inference.html#extrapolation-example-2",
    "href": "lectures/11-regression-inference.html#extrapolation-example-2",
    "title": "Inference with Linear Regression",
    "section": "Extrapolation Example",
    "text": "Extrapolation Example"
  },
  {
    "objectID": "lectures/11-regression-inference.html#recap-and-next-steps",
    "href": "lectures/11-regression-inference.html#recap-and-next-steps",
    "title": "Inference with Linear Regression",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nUse graphs to assess linear regression assumptions, i.e., residual diagnostic plots\nDiscussed interpretation of coefficients, the role of categorical variables, and interactions\nHighlighted common problems to consider: Simpson’s Paradox, intercept meaning, and extrapolation\n\n\n\n\nHW4 is due TONIGHT and you have Lab 6 on Friday\nGraphics critique due Feb 28th!\nNext time: Midsemester Review (take-home exam on Feb 26th)\nRecommended reading: CW Chapter 12 Visualizing associations among two or more quantitative variables"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#announcements-previously-and-today",
    "href": "lectures/13-nonlinear-pairs.html#announcements-previously-and-today",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW5 is due next Wednesday March 19th by 11:59 PM ET\nAs part of Homework 5, you’ll form groups for final projects\n\nTeams should be 3-4 students, you can pick your teammates or be randomized to a team\nGoal of the project: create and interpet hiqh-quality visualizations for a dataset of your choice\nProject requirements and rubrics are available on Canvas\nEDA reports are graded as as group, while presentations are graded individually\nHW5 is short so you have time to form teams and explore datasets\n\nYou do NOT have lab this week\nTODAY: How does LOESS (nonlinear regression) work? And maybe pairs plots"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#beyond-linear-regression",
    "href": "lectures/13-nonlinear-pairs.html#beyond-linear-regression",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Beyond Linear Regression",
    "text": "Beyond Linear Regression\nMany kinds of regression methods - we’ll focus on local linear regression for now.\nLet’s say: Still willing to assume Normality, but not linearity where \\(f(x)\\) is some unknown function\n\\[Y_i \\stackrel{iid}{\\sim} N(\\underbrace{f(X_i)}, \\sigma^2)\\]\nIntuition: Any nonlinear function is locally linear\nWe saw this in the extrapolation example\n\nLocal linear regressions fits a bunch of, well, local linear regressions, and then glues them together\nLocal linear regression is basically weighted linear regression, where only “local units” get weight"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#weighted-linear-regression",
    "href": "lectures/13-nonlinear-pairs.html#weighted-linear-regression",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Weighted Linear Regression",
    "text": "Weighted Linear Regression\nRemember that in typical linear regression, we solve the following:\n\\[\\arg \\min_{\\beta_0,\\beta_1} \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i)^2\\]\nIn weighted linear regression, we solve the following:\n\\[\\arg \\min_{\\beta_0,\\beta_1} \\sum_{i=1}^n w_i \\cdot (Y_i - \\beta_0 - \\beta_1 X_i)^2\\]\nLocal linear regression is exactly the same, except the weights depend on which \\(x\\) we want to estimate \\(f(x)\\)."
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#local-linear-regression-via-loess",
    "href": "lectures/13-nonlinear-pairs.html#local-linear-regression-via-loess",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Local linear regression via LOESS",
    "text": "Local linear regression via LOESS\n\\(Y_i \\overset{iid}{\\sim} N(f(x), \\sigma^2)\\), where \\(f(x)\\) is some unknown function\n\nIn local linear regression, we estimate \\(f(X_i)\\):\n\\[\\text{arg }\\underset{\\beta_0, \\beta_1}{\\text{min}} \\sum_i^n w_i(x) \\cdot \\big(Y_i - \\beta_0 - \\beta_1 X_i \\big)^2\\]\n\n\ngeom_smooth() uses tri-cubic weighting:\n\\[w_i(d_i) = \\begin{cases} (1 - |d_i|^3)^3, \\text{ if } i \\in \\text{neighborhood of  } x, \\\\\n0 \\text{ if } i \\notin \\text{neighborhood of  } x \\end{cases}\\]\n\n\\(d_i\\) is the distance between \\(x\\) and \\(X_i\\) scaled to be between 0 and 1\nspan: decides proportion of observations in neighborhood (default is 0.75)"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#animation-example",
    "href": "lectures/13-nonlinear-pairs.html#animation-example",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Animation example",
    "text": "Animation example"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#animation-example---changing-the-span",
    "href": "lectures/13-nonlinear-pairs.html#animation-example---changing-the-span",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Animation example - changing the span",
    "text": "Animation example - changing the span"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess",
    "href": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Displaying trend lines: LOESS",
    "text": "Displaying trend lines: LOESS\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth()\n\n\nFor \\(n &gt; 1000\\), mgcv::gam() is used with formula = y ~ s(x, bs = \"cs\") and method = \"REML\""
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess-1",
    "href": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess-1",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Displaying trend lines: LOESS",
    "text": "Displaying trend lines: LOESS\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(span = .1)"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess-2",
    "href": "lectures/13-nonlinear-pairs.html#displaying-trend-lines-loess-2",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Displaying trend lines: LOESS",
    "text": "Displaying trend lines: LOESS\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(span = 1)"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#back-to-the-penguins",
    "href": "lectures/13-nonlinear-pairs.html#back-to-the-penguins",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Back to the penguins…",
    "text": "Back to the penguins…\nPretend I give you this penguins dataset and I ask you to make a plot for every pairwise comparison…\n\npenguins |&gt; slice(1:3)\n\n# A tibble: 3 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nWe can create a pairs plot to see all pairwise relationships in one plot\nPairs plot can include the various kinds of pairwise plots we’ve seen:\n\nTwo quantitative variables: scatterplot\nOne categorical, one quantitative: side-by-side violins, stacked histograms, overlaid densities\nTwo categorical: stacked bars, side-by-side bars, mosaic plots"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#create-pairs-plots-with-ggally",
    "href": "lectures/13-nonlinear-pairs.html#create-pairs-plots-with-ggally",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Create pairs plots with GGally",
    "text": "Create pairs plots with GGally\n\nlibrary(GGally)\npenguins |&gt; ggpairs(columns = 3:6)"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#create-pairs-plots-with-ggally-1",
    "href": "lectures/13-nonlinear-pairs.html#create-pairs-plots-with-ggally-1",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Create pairs plots with GGally",
    "text": "Create pairs plots with GGally\n\npenguins |&gt; ggpairs(columns = 3:6,\n                    mapping = aes(alpha = 0.5))"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#flexibility-in-customization",
    "href": "lectures/13-nonlinear-pairs.html#flexibility-in-customization",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Flexibility in customization",
    "text": "Flexibility in customization\n\npenguins |&gt; \n  ggpairs(columns = c(\"bill_length_mm\", \"body_mass_g\", \"island\"),\n          mapping = aes(alpha = 0.5, color = species), \n          lower = list(\n            continuous = \"smooth_lm\", \n            combo = \"facetdensitystrip\"\n          ),\n          upper = list(\n            continuous = \"cor\",\n            combo = \"facethist\"\n          )\n  )"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#flexibility-in-customization-output",
    "href": "lectures/13-nonlinear-pairs.html#flexibility-in-customization-output",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Flexibility in customization",
    "text": "Flexibility in customization"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#see-demo-for-more",
    "href": "lectures/13-nonlinear-pairs.html#see-demo-for-more",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "See demo for more!",
    "text": "See demo for more!"
  },
  {
    "objectID": "lectures/13-nonlinear-pairs.html#recap-and-next-steps",
    "href": "lectures/13-nonlinear-pairs.html#recap-and-next-steps",
    "title": "Nonlinear Regression and Pairs Plots",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nLOESS may seem like magic, but it’s just a bunch of little linear regressions glued together\nPairs plots: Nice way to see all pairwise relationships in a dataset\n\n\n\n\nHW5 is due Wednesday March 19th and you do NOT have lab this Friday!\nNext time: Contour Plots and Heat Maps"
  },
  {
    "objectID": "lectures/17-pca.html#announcements-previously-and-today",
    "href": "lectures/17-pca.html#announcements-previously-and-today",
    "title": "Principal Component Analysis",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW6 is due Wednesday March 26th by 11:59 PM ET\nYou have Lab 8 this Friday\nWe do NOT have in-class lecture on Wednesday! I will post a recording\n\n\n\nCommon workflow:\n\nReduce the data to a few “useful” dimensions\nPlot those “useful” dimensions\n\nLast two classes:\n\nReduce the data by summarizing pairs of subjects with one distance.\nVisualize distances using multi-dimensional scaling or dendrograms.\n\nHow can we reduce the data without distances?\nPrincipal Component Analysis (PCA) is by far the most popular way"
  },
  {
    "objectID": "lectures/17-pca.html#dimension-reduction---searching-for-variance",
    "href": "lectures/17-pca.html#dimension-reduction---searching-for-variance",
    "title": "Principal Component Analysis",
    "section": "Dimension reduction - searching for variance",
    "text": "Dimension reduction - searching for variance\nGOAL: Focus on reducing dimensionality of feature space while retaining most of the information in a lower dimensional space\n\n\\(n \\times p\\) matrix \\(\\rightarrow\\) dimension reduction technique \\(\\rightarrow\\) \\(n \\times k\\) matrix\n\n\nSpecial case we just discussed: MDS\n\n\\(n \\times n\\) distance matrix \\(\\rightarrow\\) MDS \\(\\rightarrow\\) \\(n \\times k\\) matrix (usually \\(k = 2\\))\n\n\nReduce data to a distance matrix\nReduce distance matrix to \\(k = 2\\) dimensions"
  },
  {
    "objectID": "lectures/17-pca.html#principal-component-analysis-pca",
    "href": "lectures/17-pca.html#principal-component-analysis-pca",
    "title": "Principal Component Analysis",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\\[\n\\begin{pmatrix}\n& & \\text{really} & & \\\\\n& & \\text{wide} & & \\\\\n& & \\text{matrix} & &\n\\end{pmatrix}\n\\rightarrow \\text{matrix algebra stuff} \\rightarrow\n\\begin{pmatrix}\n\\text{much}  \\\\\n\\text{thinner}  \\\\\n\\text{matrix}\n\\end{pmatrix}\n\\]\n\nStart with \\(n \\times p\\) matrix of correlated variables \\(\\rightarrow\\) \\(n \\times k\\) matrix of uncorrelated variables\n\n\n\nEach of the \\(k\\) columns in the right-hand matrix are principal components, all uncorrelated with each other\nFirst column accounts for most variation in the data, second column for second-most variation, and so on\n\nIntuition: first few principal components account for most of the variation in the data"
  },
  {
    "objectID": "lectures/17-pca.html#what-are-principal-components",
    "href": "lectures/17-pca.html#what-are-principal-components",
    "title": "Principal Component Analysis",
    "section": "What are principal components?",
    "text": "What are principal components?\n\nAssume \\(\\boldsymbol{X}\\) is a \\(n \\times p\\) matrix that is centered and stardardized\nTotal variation \\(= p\\), since Var( \\(\\boldsymbol{x}_j\\) ) = 1 for all \\(j = 1, \\dots, p\\)\nPCA will give us \\(p\\) principal components that are \\(n\\)-length columns - call these \\(Z_1, \\dots, Z_p\\)\n\n\nFirst principal component (aka PC1):\n\\[Z_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + \\dots + \\phi_{p1} X_p\\]\n\n\n\n\\(\\phi_{j1}\\) are the weights indicating contributions of each variable \\(j \\in 1, \\dots, p\\)\nWeights are normalized \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\)\n\\(\\phi_{1} = (\\phi_{11}, \\phi_{21}, \\dots, \\phi_{p1})\\) is the loading vector for PC1\n\n\n\n\n\\(Z_1\\) is a linear combination of the \\(p\\) variables that has the largest variance"
  },
  {
    "objectID": "lectures/17-pca.html#what-are-principal-components-1",
    "href": "lectures/17-pca.html#what-are-principal-components-1",
    "title": "Principal Component Analysis",
    "section": "What are principal components?",
    "text": "What are principal components?\nSecond principal component:\n\\[Z_2 = \\phi_{12} X_1 + \\phi_{22} X_2 + \\dots + \\phi_{p2} X_p\\]\n\n\\(\\phi_{j2}\\) are the weights indicating the contributions of each variable \\(j \\in 1, \\dots, p\\)\nWeights are normalized \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\)\n\\(\\phi_{2} = (\\phi_{12}, \\phi_{22}, \\dots, \\phi_{p2})\\) is the loading vector for PC2\n\\(Z_2\\) is a linear combination of the \\(p\\) variables that has the largest variance\n\nSubject to constraint it is uncorrelated with \\(Z_1\\)"
  },
  {
    "objectID": "lectures/17-pca.html#what-are-principal-components-2",
    "href": "lectures/17-pca.html#what-are-principal-components-2",
    "title": "Principal Component Analysis",
    "section": "What are principal components?",
    "text": "What are principal components?\nWe repeat this process to create \\(p\\) principal components\n\nUncorrelated: Each (\\(Z_j, Z_{j'}\\)) is uncorrelated with each other\nOrdered Variance: Var( \\(Z_1\\) ) \\(&gt;\\) Var( \\(Z_2\\) ) \\(&gt; \\dots &gt;\\) Var( \\(Z_p\\) )\nTotal Variance: \\(\\sum_{j=1}^p \\text{Var}(Z_j) = p\\)\n\nIntuition: pick some \\(k &lt;&lt; p\\) such that if \\(\\sum_{j=1}^k \\text{Var}(Z_j) \\approx p\\), then just using \\(Z_1, \\dots, Z_k\\)"
  },
  {
    "objectID": "lectures/17-pca.html#visualizing-pca-in-two-dimensions",
    "href": "lectures/17-pca.html#visualizing-pca-in-two-dimensions",
    "title": "Principal Component Analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/17-pca.html#visualizing-pca-in-two-dimensions-1",
    "href": "lectures/17-pca.html#visualizing-pca-in-two-dimensions-1",
    "title": "Principal Component Analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/17-pca.html#visualizing-pca-in-two-dimensions-2",
    "href": "lectures/17-pca.html#visualizing-pca-in-two-dimensions-2",
    "title": "Principal Component Analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/17-pca.html#visualizing-pca-in-two-dimensions-3",
    "href": "lectures/17-pca.html#visualizing-pca-in-two-dimensions-3",
    "title": "Principal Component Analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/17-pca.html#visualizing-pca-in-two-dimensions-4",
    "href": "lectures/17-pca.html#visualizing-pca-in-two-dimensions-4",
    "title": "Principal Component Analysis",
    "section": "Visualizing PCA in two dimensions",
    "text": "Visualizing PCA in two dimensions"
  },
  {
    "objectID": "lectures/17-pca.html#so-what-do-we-do-with-the-principal-components",
    "href": "lectures/17-pca.html#so-what-do-we-do-with-the-principal-components",
    "title": "Principal Component Analysis",
    "section": "So what do we do with the principal components?",
    "text": "So what do we do with the principal components?\nThe point: given a dataset with \\(p\\) variables, we can find \\(k\\) variables \\((k &lt;&lt; p)\\) that account for most of the variation in the data\n\nNote that the principal components are NOT easy to interpret - these are combinations of all variables\nPCA is similar to MDS with these main differences:\n\nMDS reduces a distance matrix while PCA reduces a data matrix\nPCA has a principled way to choose \\(k\\)\nCan visualize how the principal components are related to variables in data"
  },
  {
    "objectID": "lectures/17-pca.html#working-with-pca-on-starbucks-drinks",
    "href": "lectures/17-pca.html#working-with-pca-on-starbucks-drinks",
    "title": "Principal Component Analysis",
    "section": "Working with PCA on Starbucks drinks",
    "text": "Working with PCA on Starbucks drinks\nUse the prcomp() function (based on SVD) for PCA on centered and scaled data\n\nstarbucks_pca &lt;- prcomp(dplyr::select(starbucks, serv_size_m_l:caffeine_mg),\n                        center = TRUE, scale. = TRUE)\nsummary(starbucks_pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000"
  },
  {
    "objectID": "lectures/17-pca.html#computing-principal-components",
    "href": "lectures/17-pca.html#computing-principal-components",
    "title": "Principal Component Analysis",
    "section": "Computing Principal Components",
    "text": "Computing Principal Components\nExtract the matrix of principal components \\(\\boldsymbol{Z} = XV\\) (dimension of \\(\\boldsymbol{Z}\\) will match original data)\n\nstarbucks_pc_matrix &lt;- starbucks_pca$x\nhead(starbucks_pc_matrix)\n\n           PC1        PC2        PC3           PC4         PC5         PC6\n[1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847\n[2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410\n[3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237\n[4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485\n[5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809\n[6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543\n             PC7         PC8        PC9        PC10         PC11\n[1,] -0.02812472 0.006489978 0.05145094 -0.06678083 -0.019741873\n[2,]  0.05460952 0.021148978 0.07094211 -0.08080545 -0.023480029\n[3,]  0.09050806 0.031575955 0.08901403 -0.09389227 -0.028669251\n[4,]  0.11585507 0.037521689 0.11287190 -0.11582260 -0.034691142\n[5,]  0.07009414 0.037736197 0.02892317 -0.03631676 -0.005775410\n[6,]  0.20236484 0.068154160 0.03705252 -0.03497690 -0.002469611\n\n\nColumns are uncorrelated, such that Var( \\(Z_1\\) ) \\(&gt;\\) Var( \\(Z_2\\) ) \\(&gt; \\dots &gt;\\) Var( \\(Z_p\\) ) - can start with a scatterplot of \\(Z_1, Z_2\\)"
  },
  {
    "objectID": "lectures/17-pca.html#starbucks-drinks-pc1-and-pc2",
    "href": "lectures/17-pca.html#starbucks-drinks-pc1-and-pc2",
    "title": "Principal Component Analysis",
    "section": "Starbucks drinks: PC1 and PC2",
    "text": "Starbucks drinks: PC1 and PC2\n\nstarbucks &lt;- starbucks |&gt;\n  mutate(pc1 = starbucks_pc_matrix[,1], pc2 = starbucks_pc_matrix[,2])\nstarbucks |&gt;\n  ggplot(aes(x = pc1, y = pc2)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"PC 1\", y = \"PC 2\")"
  },
  {
    "objectID": "lectures/17-pca.html#making-pcs-interpretable-with-biplots-factoextra",
    "href": "lectures/17-pca.html#making-pcs-interpretable-with-biplots-factoextra",
    "title": "Principal Component Analysis",
    "section": "Making PCs interpretable with biplots (factoextra)",
    "text": "Making PCs interpretable with biplots (factoextra)\n\nlibrary(factoextra)\n# Designate to only label the variables:\nfviz_pca_biplot(starbucks_pca, \n                label = \"var\",\n                # Change the alpha for observations which is represented by ind\n                alpha.ind = .25,\n                # Modify the alpha for variables (var):\n                alpha.var = .75,\n                col.var = \"darkblue\")"
  },
  {
    "objectID": "lectures/17-pca.html#making-pcs-interpretable-with-biplots-factoextra-output",
    "href": "lectures/17-pca.html#making-pcs-interpretable-with-biplots-factoextra-output",
    "title": "Principal Component Analysis",
    "section": "Making PCs interpretable with biplots (factoextra)",
    "text": "Making PCs interpretable with biplots (factoextra)"
  },
  {
    "objectID": "lectures/17-pca.html#how-many-principal-components-to-use",
    "href": "lectures/17-pca.html#how-many-principal-components-to-use",
    "title": "Principal Component Analysis",
    "section": "How many principal components to use?",
    "text": "How many principal components to use?\nIntuition: Additional principal components will add smaller and smaller variance\n\nKeep adding components until the added variance drops off\n\n\nsummary(starbucks_pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413\nProportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177\nCumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.28123 0.16874 0.08702 0.04048\nProportion of Variance 0.00719 0.00259 0.00069 0.00015\nCumulative Proportion  0.99657 0.99916 0.99985 1.00000"
  },
  {
    "objectID": "lectures/17-pca.html#create-scree-plot-aka-elbow-plot-to-choose",
    "href": "lectures/17-pca.html#create-scree-plot-aka-elbow-plot-to-choose",
    "title": "Principal Component Analysis",
    "section": "Create scree plot (aka “elbow plot”) to choose",
    "text": "Create scree plot (aka “elbow plot”) to choose\n\nfviz_eig(starbucks_pca, addlabels = TRUE) + \n  geom_hline(yintercept = 100 * (1 / ncol(starbucks_pca$x)), linetype = \"dashed\", color = \"darkred\")"
  },
  {
    "objectID": "lectures/17-pca.html#recap-and-next-steps",
    "href": "lectures/17-pca.html#recap-and-next-steps",
    "title": "Principal Component Analysis",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nWalked through PCA for dimension reduction\nPCA is a very common way to define “most important dimensions”\nEspecially useful to plot principal components with a biplot (e.g., with factoextra)\n\n\n\nHW6 is due Wednesday March 26th and you have lab on Friday!\nI will not have office hours on Wednesday\nWe do NOT have lecture this Wednesday - instead I will post a recording\n\n\n\n\nNext time: Visualizing trends\nRecommended reading: CW Chapter 12 Visualizing associations among two or more quantitative variables"
  },
  {
    "objectID": "lectures/17-pca.html#pca-singular-value-decomposition-svd",
    "href": "lectures/17-pca.html#pca-singular-value-decomposition-svd",
    "title": "Principal Component Analysis",
    "section": "PCA: singular value decomposition (SVD)",
    "text": "PCA: singular value decomposition (SVD)\n\\[\nX = U D V^T\n\\]\n\nMatrices \\(U\\) and \\(V\\) contain the left and right (respectively) singular vectors of scaled matrix \\(X\\)\n\\(D\\) is the diagonal matrix of the singular values\nSVD simplifies matrix-vector multiplication as rotate, scale, and rotate again\n\n\\(V\\) is called the loading matrix for \\(X\\) with \\(\\phi_{j}\\) as columns,\n\n\\(Z = X  V\\) is the PC matrix"
  },
  {
    "objectID": "lectures/17-pca.html#eigenvalue-decomposition-aka-spectral-decomposition",
    "href": "lectures/17-pca.html#eigenvalue-decomposition-aka-spectral-decomposition",
    "title": "Principal Component Analysis",
    "section": "Eigenvalue decomposition (aka spectral decomposition)",
    "text": "Eigenvalue decomposition (aka spectral decomposition)\n\\[\nX = U D V^T\n\\]\n\n\\(V\\) are the eigenvectors of \\(X^TX\\) (covariance matrix, \\(^T\\) means transpose)\n\\(U\\) are the eigenvectors of \\(XX^T\\)\nThe singular values (diagonal of \\(D\\)) are square roots of the eigenvalues of \\(X^TX\\) or \\(XX^T\\)\nMeaning that \\(Z = UD\\)"
  },
  {
    "objectID": "lectures/17-pca.html#eigenvalues-guide-dimension-reduction",
    "href": "lectures/17-pca.html#eigenvalues-guide-dimension-reduction",
    "title": "Principal Component Analysis",
    "section": "Eigenvalues guide dimension reduction",
    "text": "Eigenvalues guide dimension reduction\nWe want to choose \\(p^* &lt; p\\) such that we are explaining variation in the data\nEigenvalues \\(\\lambda_j\\) for \\(j \\in 1, \\dots, p\\) indicate the variance explained by each component\n\n\\(\\sum_j^p \\lambda_j = p\\), meaning \\(\\lambda_j \\geq 1\\) indicates \\(\\text{PC}j\\) contains at least one variable’s worth in variability\n\\(\\lambda_j / p\\) equals proportion of variance explained by \\(\\text{PC}j\\)\nArranged in descending order so that \\(\\lambda_1\\) is largest eigenvalue and corresponds to PC1\nCan compute the cumulative proportion of variance explained (CVE) with \\(p^*\\) components:\n\n\\[\\text{CVE}_{p^*} = \\frac{\\sum_j^{p*} \\lambda_j}{p}\\]"
  },
  {
    "objectID": "lectures/06-1dquant.html#announcements-previously-and-today",
    "href": "lectures/06-1dquant.html#announcements-previously-and-today",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW2 is due Wednesday by 11:59 PM\nYou have Lab 4 again on Friday\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM; and Perry (zoom): Wednesdays @ 11 AM\n\n\n\n\n\nCan create stacked and side-by-side bar charts to visualize 2D categorical data\nPerform 2D Chi-squared test to test if two categorical variables are associated with each other\nCreate mosaic plots to visualize 2D categorical data, shade by Pearson residuals\n\n\n\n\n\nTODAY:\n\nHow do we visualize 1D quantitative data?\nFor this week, we’ll focus on visualization issues and move to inference next week"
  },
  {
    "objectID": "lectures/06-1dquant.html#d-quantitative-data",
    "href": "lectures/06-1dquant.html#d-quantitative-data",
    "title": "Visualizing 1D Quantitative Data",
    "section": "1D Quantitative Data",
    "text": "1D Quantitative Data\nObservations are collected into a vector \\((x_1, \\dots, x_n)\\), \\(x_i \\in \\mathbb{R}\\) (or \\(\\mathbb{R}^+\\), \\(\\mathbb{Z}\\))\nCommon summary statistics for 1D quantitative data:\n\n\nCenter: Mean, median, weighted mean, mode\n\nRelated to the first moment, i.e., \\(\\mathbb{E}[X]\\)\n\n\n\n\n\nSpread: Variance, range, min/max, quantiles, IQR\n\nRelated to the second moment, i.e., \\(\\mathbb{E}[X^2]\\)\n\n\n\n\n\nShape: symmetry, skew, kurtosis (“peakedness”)\n\nRelated to higher order moments, i.e., skewness is \\(\\mathbb{E}[X^3]\\), kurtosis is \\(\\mathbb{E}[X^4]\\)\n\n\nCompute various statistics with summary(), mean(), median(), quantile(), range(), sd(), var(), etc."
  },
  {
    "objectID": "lectures/06-1dquant.html#box-plots-visualize-summary-statistics",
    "href": "lectures/06-1dquant.html#box-plots-visualize-summary-statistics",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Box plots visualize summary statistics",
    "text": "Box plots visualize summary statistics\n\npenguins |&gt;\n  ggplot(aes(y = flipper_length_mm)) +\n  geom_boxplot(aes(x = \"\")) +\n  coord_flip()"
  },
  {
    "objectID": "lectures/06-1dquant.html#histograms-display-1d-continuous-distributions",
    "href": "lectures/06-1dquant.html#histograms-display-1d-continuous-distributions",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Histograms display 1D continuous distributions",
    "text": "Histograms display 1D continuous distributions\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) +\n  geom_histogram()"
  },
  {
    "objectID": "lectures/06-1dquant.html#do-not-rely-on-box-plots",
    "href": "lectures/06-1dquant.html#do-not-rely-on-box-plots",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Do NOT rely on box plots…",
    "text": "Do NOT rely on box plots…"
  },
  {
    "objectID": "lectures/06-1dquant.html#simulate-from-mixture-of-normal-distributions",
    "href": "lectures/06-1dquant.html#simulate-from-mixture-of-normal-distributions",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Simulate from mixture of Normal distributions",
    "text": "Simulate from mixture of Normal distributions\nWill sample 100 draws from \\(N(-1.5, 1)\\) and 100 draws from \\(N(1.5, 1)\\)"
  },
  {
    "objectID": "lectures/06-1dquant.html#can-we-trust-the-default",
    "href": "lectures/06-1dquant.html#can-we-trust-the-default",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Can we trust the default?",
    "text": "Can we trust the default?\n\nset.seed(2025)\nfake_data &lt;- \n  tibble(fake_x = c(rnorm(100, -1.5), rnorm(100, 1.5))) |&gt;\n  mutate(component = c(rep(\"left\", 100), rep(\"right\", 100)))\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram() +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins",
    "href": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What happens as we change the number of bins?",
    "text": "What happens as we change the number of bins?\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(bins = 15) +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-1",
    "href": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-1",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What happens as we change the number of bins?",
    "text": "What happens as we change the number of bins?\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(bins = 60) +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-2",
    "href": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-2",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What happens as we change the number of bins?",
    "text": "What happens as we change the number of bins?\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(bins = 5) +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-3",
    "href": "lectures/06-1dquant.html#what-happens-as-we-change-the-number-of-bins-3",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What happens as we change the number of bins?",
    "text": "What happens as we change the number of bins?\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(bins = 100) +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#variability-of-graphs---30-bins",
    "href": "lectures/06-1dquant.html#variability-of-graphs---30-bins",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Variability of graphs - 30 bins",
    "text": "Variability of graphs - 30 bins\n\nset.seed(2025)\nfake_data &lt;- \n  tibble(fake_x = c(rnorm(100, -1.5), rnorm(100, 1.5))) |&gt;\n  mutate(component = c(rep(\"left\", 100), rep(\"right\", 100)))\n\nfake_data |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram() +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-happens-with-a-different-sample",
    "href": "lectures/06-1dquant.html#what-happens-with-a-different-sample",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What happens with a different sample?",
    "text": "What happens with a different sample?\n\nset.seed(1985)\nfake_data2 &lt;- \n  tibble(fake_x = c(rnorm(100, -1.5), rnorm(100, 1.5))) |&gt;\n  mutate(component = c(rep(\"left\", 100), rep(\"right\", 100)))\n\nfake_data2 |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram() +\n  scale_x_continuous(limits = c(-5, 5))"
  },
  {
    "objectID": "lectures/06-1dquant.html#variability-of-graphs---15-bins",
    "href": "lectures/06-1dquant.html#variability-of-graphs---15-bins",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Variability of graphs - 15 bins",
    "text": "Variability of graphs - 15 bins"
  },
  {
    "objectID": "lectures/06-1dquant.html#variability-of-graphs---a-few-bins",
    "href": "lectures/06-1dquant.html#variability-of-graphs---a-few-bins",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Variability of graphs - a few bins",
    "text": "Variability of graphs - a few bins"
  },
  {
    "objectID": "lectures/06-1dquant.html#variability-of-graphs---too-many-bins",
    "href": "lectures/06-1dquant.html#variability-of-graphs---too-many-bins",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Variability of graphs - too many bins",
    "text": "Variability of graphs - too many bins"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-about-displaying-conditional-distributions",
    "href": "lectures/06-1dquant.html#what-about-displaying-conditional-distributions",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What about displaying conditional distributions?",
    "text": "What about displaying conditional distributions?\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_histogram(aes(fill = species))"
  },
  {
    "objectID": "lectures/06-1dquant.html#what-about-displaying-conditional-distributions-1",
    "href": "lectures/06-1dquant.html#what-about-displaying-conditional-distributions-1",
    "title": "Visualizing 1D Quantitative Data",
    "section": "What about displaying conditional distributions?",
    "text": "What about displaying conditional distributions?\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_histogram(aes(fill = species),\n                 position = \"identity\", alpha = 0.3)"
  },
  {
    "objectID": "lectures/06-1dquant.html#normalize-histogram-frequencies-with-density",
    "href": "lectures/06-1dquant.html#normalize-histogram-frequencies-with-density",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Normalize histogram frequencies with density",
    "text": "Normalize histogram frequencies with density\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_histogram(aes(y = after_stat(density), fill = species),\n                 position = \"identity\", alpha = 0.3)"
  },
  {
    "objectID": "lectures/06-1dquant.html#can-use-density-curves-instead",
    "href": "lectures/06-1dquant.html#can-use-density-curves-instead",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Can use density curves instead",
    "text": "Can use density curves instead\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(aes(color = species))"
  },
  {
    "objectID": "lectures/06-1dquant.html#we-should-not-fill-the-density-curves",
    "href": "lectures/06-1dquant.html#we-should-not-fill-the-density-curves",
    "title": "Visualizing 1D Quantitative Data",
    "section": "We should NOT fill the density curves",
    "text": "We should NOT fill the density curves\n\npenguins |&gt;\n  ggplot(aes(x = flipper_length_mm)) + \n  geom_density(aes(fill = species), alpha = .3)"
  },
  {
    "objectID": "lectures/06-1dquant.html#recap-and-next-steps",
    "href": "lectures/06-1dquant.html#recap-and-next-steps",
    "title": "Visualizing 1D Quantitative Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nVisualize 1D quantitative data to inspect center, spread, and shape\nBoxplots are only a display of summary statistics (i.e., they suck)\nHistograms display shape of the distribution, but comes with tradeoffs\nDensity curves provide an easy way to visualize conditional distributions\n\n\n\n\n\nHW2 is due Wednesday and you have Lab 4 on Friday\nNext time: Density estimation\nRecommended reading: CW Chapter 7 Visualizing distributions: Histograms and density plots"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#announcements-previously-and-today",
    "href": "lectures/03-1dcat-infer.html#announcements-previously-and-today",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\n\nHW1 is posted and due Wednesday Jan 29\nYou have Lab 2 again on Friday\n\nOffice hours schedule:\n\nMy office hours (BH 132D): Wednesdays and Thursdays @ 2 PM\nAnna (zoom): Mondays @ 2 PM\nPerry (zoom): Wednesdays @ 11 AM\n\n\n\n\n\nDiscussed 1D categorical data, basic summaries with counts and proportions\nIntroduced different area plots for visualizing 1D categorical data\nWe make bar charts for categorical data (I told you repeatedly that pie charts suck)\n\n\n\n\n\nTODAY: quantify and display uncertainty for 1D categorical data\n\nAdd confidence intervals to bar charts\nReview the Chi-Squared Test\nDiscuss connections between visualizations and statistical significance"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#crimes-against-bar-charts",
    "href": "lectures/03-1dcat-infer.html#crimes-against-bar-charts",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Crimes against bar charts",
    "text": "Crimes against bar charts"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#crimes-against-bar-charts-1",
    "href": "lectures/03-1dcat-infer.html#crimes-against-bar-charts-1",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Crimes against bar charts",
    "text": "Crimes against bar charts"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#what-does-a-bar-chart-show",
    "href": "lectures/03-1dcat-infer.html#what-does-a-bar-chart-show",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "What does a bar chart show?",
    "text": "What does a bar chart show?\nMarginal Distribution\n\nAssume categorical variable \\(X\\) has \\(K\\) categories: \\(C_1, \\dots, C_K\\)\nTrue marginal distribution of \\(X\\):\n\n\\[\nP(X = C_j) = p_j,\\ j \\in \\{ 1, \\dots, K \\}\n\\]\n\nWe have access to the Empirical Marginal Distribution\n\nObserved distribution of \\(X\\), our best estimate (MLE) of the marginal distribution of \\(X\\): \\(\\hat{p}_1\\), \\(\\hat{p}_2\\), \\(\\dots\\), \\(\\hat{p}_K\\)\n\n\ntable(penguins$species) / nrow(penguins)\n\n\n   Adelie Chinstrap    Gentoo \n0.4418605 0.1976744 0.3604651"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#bar-charts-with-proportions",
    "href": "lectures/03-1dcat-infer.html#bar-charts-with-proportions",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Bar charts with proportions",
    "text": "Bar charts with proportions\n\nafter_stat() indicates the aesthetic mapping is performed after statistical transformation\nUse after_stat(count) to access the stat_count() called by geom_bar()\n\n\npenguins |&gt;\n  ggplot(aes(x = species)) +\n  geom_bar(aes(y = after_stat(count) / sum(after_stat(count)))) + \n  labs(y = \"Proportion\")"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#compute-and-display-the-proportions-directly",
    "href": "lectures/03-1dcat-infer.html#compute-and-display-the-proportions-directly",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Compute and display the proportions directly",
    "text": "Compute and display the proportions directly\n\nUse group_by(), summarize(), and mutate() in a pipeline to compute then display the proportions directly\nNeed to indicate we are displaying the y axis as given, i.e., the identity function\n\n\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  mutate(total = sum(count), \n         prop = count / total) |&gt; \n  ggplot(aes(x = species)) +\n  geom_bar(aes(y = prop), stat = \"identity\")"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#compute-and-display-the-proportions-directly-output",
    "href": "lectures/03-1dcat-infer.html#compute-and-display-the-proportions-directly-output",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Compute and display the proportions directly",
    "text": "Compute and display the proportions directly"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#what-about-uncertainty",
    "href": "lectures/03-1dcat-infer.html#what-about-uncertainty",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "What about uncertainty?",
    "text": "What about uncertainty?\n\nQuantify uncertainty for our estimate \\(\\hat{p}_j = \\frac{n_j}{n}\\) with the standard error:\n\n\\[\nSE(\\hat{p}_j) = \\sqrt{\\frac{\\hat{p}_j(1 - \\hat{p}_j)}{n}}\n\\]\n\n\nCompute \\(\\alpha\\)-level confidence interval (CI) as \\(\\hat{p}_j \\pm z_{1 - \\alpha / 2} \\cdot SE(\\hat{p}_j)\\)\nGood rule-of-thumb: construct 95% CI using \\(\\hat{p}_j \\pm 2 \\cdot SE(\\hat{p}_j)\\)\nApproximation justified by CLT, so CI could include values outside of [0,1]"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#add-standard-errors-to-bars",
    "href": "lectures/03-1dcat-infer.html#add-standard-errors-to-bars",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Add standard errors to bars",
    "text": "Add standard errors to bars\n\nNeed to remember each CI is for each \\(\\hat{p}_j\\) marginally, not jointly\nHave to be careful with multiple testing\n\n\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  mutate(total = sum(count), \n         prop = count / total,\n         se = sqrt(prop * (1 - prop) / total), \n         lower = prop - 2 * se, \n         upper = prop + 2 * se) |&gt; \n  ggplot(aes(x = species)) +\n  geom_bar(aes(y = prop), stat = \"identity\") +\n  geom_errorbar(aes(ymin = lower, ymax = upper), \n                color = \"red\")"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#add-standard-errors-to-bars-output",
    "href": "lectures/03-1dcat-infer.html#add-standard-errors-to-bars-output",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Add standard errors to bars",
    "text": "Add standard errors to bars"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#useful-to-order-categories-by-frequency-with-forcats",
    "href": "lectures/03-1dcat-infer.html#useful-to-order-categories-by-frequency-with-forcats",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Useful to order categories by frequency with forcats",
    "text": "Useful to order categories by frequency with forcats\n\npenguins |&gt;\n  group_by(species) |&gt; \n  summarize(count = n(), .groups = \"drop\") |&gt; \n  mutate(total = sum(count), \n         prop = count / total,\n         se = sqrt(prop * (1 - prop) / total), \n         lower = prop - 2 * se, \n         upper = prop + 2 * se,\n         species = fct_reorder(species, prop)) |&gt;\n  ggplot(aes(x = species)) +\n  geom_bar(aes(y = prop), stat = \"identity\") +\n  geom_errorbar(aes(ymin = lower, ymax = upper), \n                color = \"red\")"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#useful-to-order-categories-by-frequency-with-forcats-output",
    "href": "lectures/03-1dcat-infer.html#useful-to-order-categories-by-frequency-with-forcats-output",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Useful to order categories by frequency with forcats",
    "text": "Useful to order categories by frequency with forcats"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#dont-do-this",
    "href": "lectures/03-1dcat-infer.html#dont-do-this",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Don’t do this…",
    "text": "Don’t do this…"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#chi-squared-test-for-1d-categorical-data",
    "href": "lectures/03-1dcat-infer.html#chi-squared-test-for-1d-categorical-data",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Chi-squared test for 1D categorical data:",
    "text": "Chi-squared test for 1D categorical data:\n\n\nNull hypothesis \\(H_0\\): \\(p_1 = p_2 = \\dots = p_K\\), compute the test statistic:\n\n\\[\n\\chi^2 = \\sum_{j=1}^K \\frac{(O_j - E_j)^2}{E_j}\n\\]\n\n\\(O_j\\): observed counts in category \\(j\\)\n\\(E_j\\): expected counts under \\(H_0\\), i.e., each category is equally to occur \\(n / K = p_1 = p_2 = \\dots = p_K\\)\n\n\n\n\nchisq.test(table(penguins$species))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(penguins$species)\nX-squared = 31.907, df = 2, p-value = 1.179e-07"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#hypothesis-testing-review",
    "href": "lectures/03-1dcat-infer.html#hypothesis-testing-review",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Hypothesis testing review",
    "text": "Hypothesis testing review\n\n\n\nComputing \\(p\\)-values works like this:\n\nChoose a test statistic.\nCompute the test statistic in your dataset.\nIs test statistic “unusual” compared to what I would expect under \\(H_0\\)?\nCompare \\(p\\)-value to target error rate \\(\\alpha\\) (typically referred to as target level \\(\\alpha\\) )\nTypically choose \\(\\alpha = 0.05\\)\n\ni.e., if we reject null hypothesis at \\(\\alpha = 0.05\\) then, assuming \\(H_0\\) is true, there is a 5% chance it is a false positive (aka Type 1 error)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#chi-squared-test-for-1d-categorical-data-1",
    "href": "lectures/03-1dcat-infer.html#chi-squared-test-for-1d-categorical-data-1",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Chi-squared test for 1D categorical data:",
    "text": "Chi-squared test for 1D categorical data:\n\n\nNull hypothesis \\(H_0\\): \\(p_1 = p_2 = \\dots = p_K\\), compute the test statistic:\n\n\\[\n\\chi^2 = \\sum_{j=1}^K \\frac{(O_j - E_j)^2}{E_j}\n\\]\n\n\\(O_j\\): observed counts in category \\(j\\)\n\\(E_j\\): expected counts under \\(H_0\\), i.e., each category is equally to occur \\(n / K = p_1 = p_2 = \\dots = p_K\\)\n\n\n\nchisq.test(table(penguins$species))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(penguins$species)\nX-squared = 31.907, df = 2, p-value = 1.179e-07"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#graphics-versus-statistical-inference",
    "href": "lectures/03-1dcat-infer.html#graphics-versus-statistical-inference",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Graphics versus Statistical Inference",
    "text": "Graphics versus Statistical Inference\n\nReminder Anscombe’s Quartet: where statistical inference was the same but the graphics were very different\n\n\n\nThe opposite can be true! Graphics are the same, but statistical inference is very different…"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14",
    "href": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)",
    "text": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-1",
    "href": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-1",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)",
    "text": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-2",
    "href": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-2",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)",
    "text": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-3",
    "href": "lectures/03-1dcat-infer.html#example-3-categories-p_1-12-p_2-p_3-14-3",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)",
    "text": "Example: 3 categories, \\(p_1 = 1/2,\\ p_2 = p_3 = 1/4\\)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#power-under-this-scenario-2n4-n4-n4",
    "href": "lectures/03-1dcat-infer.html#power-under-this-scenario-2n4-n4-n4",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Power under this scenario: (2n/4, n/4, n/4)",
    "text": "Power under this scenario: (2n/4, n/4, n/4)"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#how-do-we-combine-graphs-with-inference",
    "href": "lectures/03-1dcat-infer.html#how-do-we-combine-graphs-with-inference",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "How do we combine graphs with inference?",
    "text": "How do we combine graphs with inference?\n\nSimply add \\(p\\)-values (or other info) to graph via text\nAdd confidence intervals to the graph\n\n\nNeed to remember what each CI is for!\nOur CIs on previous slides are for each \\(\\hat{p}_j\\) marginally, NOT jointly\nHave to be careful with multiple testing…"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#cis-will-visually-capture-uncertainty-in-estimates",
    "href": "lectures/03-1dcat-infer.html#cis-will-visually-capture-uncertainty-in-estimates",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "CIs will visually capture uncertainty in estimates",
    "text": "CIs will visually capture uncertainty in estimates"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#rough-rules-of-thumb-for-comparing-cis-on-bar-charts",
    "href": "lectures/03-1dcat-infer.html#rough-rules-of-thumb-for-comparing-cis-on-bar-charts",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "(Rough) Rules-of-thumb for comparing CIs on bar charts",
    "text": "(Rough) Rules-of-thumb for comparing CIs on bar charts\n\n\nComparing overlap of two CIs is NOT exactly the same as directly testing for a significant difference…\n\nReally you want CI( \\(\\hat{p}_1 - \\hat{p}_2\\) ), not CI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) )\nCI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) ) not overlapping implies \\(0 \\notin\\) CI( \\(\\hat{p}_1 - \\hat{p}_2\\) )\nHowever CI( \\(\\hat{p_1}\\) ) and CI( \\(\\hat{p_2}\\) ) overlapping DOES NOT imply \\(0 \\in\\) CI( \\(\\hat{p}_1 - \\hat{p}_2\\) )\n\n\nRoughly speaking:\n\nIf CIs don’t overlap \\(\\rightarrow\\) significant difference\nIf CIs overlap a little \\(\\rightarrow\\) ambiguous\nIf CIs overlap a lot \\(\\rightarrow\\) no significant difference\n\n\n\n\nBut if we’re comparing more than two CIs simultaneously, we need to account for multiple testing!\n\nWhen you look for all non-overlapping CIs: implicitly making \\(\\binom{K}{2} = \\frac{K!}{2!(K-2)!}\\) pairwise tests in your head!"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#corrections-for-multiple-testing",
    "href": "lectures/03-1dcat-infer.html#corrections-for-multiple-testing",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Corrections for multiple testing",
    "text": "Corrections for multiple testing\n\n\nIn those bar plots, when we determine whether CIs overlap we make 3 comparisons:\n\nA vs B\nA vs C\nB vs C\n\n\nThis is a multiple testing issue\n\n\n\n\nIn short: we will make Type 1 errors (chance of false rejecting) more than 5% of the time!\nReminder: Type 1 error = Rejecting \\(H_0\\) when \\(H_0\\) is true\ne.g., CIs don’t overlap but actually \\(H_0: p_A = p_B\\) is true\nIf only interested in A vs B and nothing else, then just construct 95% CI for A vs B and control error rate at 5%\nHowever, if we construct several CIs, where A vs B is just one comparison we make, our Type 1 error rate &gt; 5%!"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#corrections-for-multiple-testing-1",
    "href": "lectures/03-1dcat-infer.html#corrections-for-multiple-testing-1",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Corrections for multiple testing",
    "text": "Corrections for multiple testing\n\nVast literature on corrections for multiple testing (beyond the scope of this class… but in my thesis!)\nBut you should understand the following:\n\nCorrections for multiple testing inflate \\(p\\)-values (i.e., make them bigger)\nEquivalently, they inflate CIs (i.e., make them wider)\nPurpose of these corrections is to control Type 1 error rate \\(\\leq 5\\%\\)\n\n\n\n\nWe’ll focus on the Bonferroni correction, which inflates \\(p\\)-values the most but is easy to implement and very popular:\n\nWe usually reject null hypothesis when \\(p\\)-value \\(\\leq .05\\)\nBonferroni: if making \\(K\\) comparisons, reject only if \\(p\\)-value \\(\\leq .05/K\\)\nFor CIs: instead of plotting 95% CIs, we plot (1 - \\(0.05/K\\))% CIs\n\ne.g., for \\(K = 3\\) then plot 98.3% CIs"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#impact-of-bonferroni-correction-on-cis",
    "href": "lectures/03-1dcat-infer.html#impact-of-bonferroni-correction-on-cis",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Impact of Bonferroni correction on CIs…",
    "text": "Impact of Bonferroni correction on CIs…"
  },
  {
    "objectID": "lectures/03-1dcat-infer.html#recap-and-next-steps",
    "href": "lectures/03-1dcat-infer.html#recap-and-next-steps",
    "title": "Statistical Inference for 1D Categorical Data",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nBar charts display the empirical distribution of the categorical variable ( \\(\\hat{p}_1, \\dots, \\hat{p}_K\\) )\nChi-squared test is a global test for 1D categorical data, testing \\(H_0 : p_1 = \\cdot \\cdot \\cdot = p_K\\)\n\nDoes not tell us which probabilities differ!\n\nCan visualize CIs for each \\(\\hat{p}_1\\), \\(\\dots\\), \\(\\hat{p}_K\\), but need to deal with multiple testing\nGraphs with the same trends can display very different statistical significance (largely due to sample size)\n\n\n\n\n\nHW1 is due next week and you have Lab 2 on Friday!\nNext time: 2D categorical data\nRecommended reading:\n\nCW Chapter 16.2 Visualizing the uncertainty of point estimates"
  },
  {
    "objectID": "lectures/01-intro.html#who-am-i",
    "href": "lectures/01-intro.html#who-am-i",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Who am I?",
    "text": "Who am I?\n\n\n\n\nAssistant Teaching Professor\nFinished Phd in Statistics @ CMU in May 2022\nPreviously BS in Statistics @ CMU in 2015\nResearch interests: sports analytics, natural language processing, clustering, selective inference\n\n\n\n\n\nIndustry experience: finance before returning to grad school and also as data scientist in professional sports"
  },
  {
    "objectID": "lectures/01-intro.html#why-do-we-visualize-data",
    "href": "lectures/01-intro.html#why-do-we-visualize-data",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Why do we visualize data?",
    "text": "Why do we visualize data?"
  },
  {
    "objectID": "lectures/01-intro.html#course-structure",
    "href": "lectures/01-intro.html#course-structure",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Course Structure",
    "text": "Course Structure\n\n\n\nLectures on Mondays/Wednesdays\n\nAll slides and demos posted on https://ryurko.github.io/statds-36315-spring25/\nParticipate and ask questions!\n\nWeekly homeworks due Wednesdays by 11:59 PM ET\nWeekly Friday labs due Saturdays by 11:59 AM ET\n\n\n\n\nTwo Graphics Critiques of Data Viz in the Wild (due Feb 28 and Mar 31)\nTake-home exam on Wednesday, Feb 26th\nFinal project with individual and group grade\n\nWork in teams on dataset you choose\nIn-class presentations during final week of class\nPublic facing HTML report due during finals week"
  },
  {
    "objectID": "lectures/01-intro.html#course-logistics",
    "href": "lectures/01-intro.html#course-logistics",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Course logistics",
    "text": "Course logistics\n\n\nAll homework/lab assignments will be in Quarto. You’ll generate a PDF, which you’ll submit on Gradescope\nMake sure R and RStudio are installed on your computer!\nHW0 due Wednesday Jan 15 at 11:59 PM ET: install R/RStudio, install/load tidyverse, render to PDF, and post to Gradescope\n\nHave any installation issues? Post to the course Piazza!\n\nPiazza: all questions about course material, HWs, exam, and projects\n\nDo NOT share code on Piazza\n\nOnly email for address administrative/logistic issues\nLab attendance on Friday is mandatory - submit lab assignment but don’t attend YOU LOSE 20PTS!\n\nQuestions about lab assignments will only be answered during lab\nIf you need to miss a lab due to illness, interviews, emergencies, etc., email me 48 hours in advance"
  },
  {
    "objectID": "lectures/01-intro.html#important-hw0-is-due-wednesday-night",
    "href": "lectures/01-intro.html#important-hw0-is-due-wednesday-night",
    "title": "Introduction and the Grammar of Graphics",
    "section": "IMPORTANT: HW0 is due Wednesday night",
    "text": "IMPORTANT: HW0 is due Wednesday night\nAs seen in today’s Canvas announcement - you must submit HW0 by Wednesday night!\n\nThis is just to make sure you have everything installed correctly and can render .qmd files to PDF\n\nRead through all of the directions in HW0 carefully!\nYou will stop saving your workspace upon exiting RStudio!\nYou will need to be set-up for the first lab on Friday"
  },
  {
    "objectID": "lectures/01-intro.html#course-objectives-read-the-syllabus",
    "href": "lectures/01-intro.html#course-objectives-read-the-syllabus",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Course Objectives (read the syllabus)",
    "text": "Course Objectives (read the syllabus)\nLearn useful principles for making appropriate statistical graphics.\nCritique existing graphs and remake better ones.\nVisualize statistical analyses to facilitate communication.\nPinpoint the statistical claims you can/cannot make from graphics.\nWrite and speak publicly about statistical graphics.\nPractice tidy data manipulation in R using the tidyverse\nPractice reproducible workflows with Quarto"
  },
  {
    "objectID": "lectures/01-intro.html#what-do-i-mean-by-tidy-data",
    "href": "lectures/01-intro.html#what-do-i-mean-by-tidy-data",
    "title": "Introduction and the Grammar of Graphics",
    "section": "What do I mean by tidy data?",
    "text": "What do I mean by tidy data?\nData are often stored in tabular (or matrix) form:\n\nlibrary(palmerpenguins)\npenguins |&gt; slice(1:5)\n\n# A tibble: 5 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;"
  },
  {
    "objectID": "lectures/01-intro.html#the-grammar-of-graphics",
    "href": "lectures/01-intro.html#the-grammar-of-graphics",
    "title": "Introduction and the Grammar of Graphics",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\n\n\nOriginally defined by Leland Wilkinson\n\n\ndata\ngeometries: type of geometric objects to represent data, e.g., points, lines\naesthetics: visual characteristics of geometric objects to represent data, e.g., position, size\nscales: how each aesthetic is converted into values on the graph, e.g., color scales\nstats: statistical transformations to summarize data, e.g., counts, means, regression lines\nfacets: split data and view as multiple graphs\ncoordinate system: 2D space the data are projected onto, e.g., Cartesian coordinates\n\n\n\n\nHadley Wickham created ggplot2\n\n\ndata\ngeom\naes: mappings of columns to geometric objects\nscale: one scale for each aes variable\nstat\nfacet\ncoord\nlabs: labels/guides for each variable and other parts of the plot, e.g., title, subtitle, caption\ntheme: customization of plot layout"
  },
  {
    "objectID": "lectures/01-intro.html#start-with-the-data",
    "href": "lectures/01-intro.html#start-with-the-data",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Start with the data",
    "text": "Start with the data\n\n\nAccess ggplot2 from the tidyverse:\n\nlibrary(tidyverse)\nggplot(data = penguins)\n\n\nOr equivalently using |&gt;:\n\npenguins |&gt;\n  ggplot()"
  },
  {
    "objectID": "lectures/01-intro.html#need-to-add-geometric-objects",
    "href": "lectures/01-intro.html#need-to-add-geometric-objects",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Need to add geometric objects!",
    "text": "Need to add geometric objects!\n\n\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, \n             y = bill_depth_mm)) + \n  geom_point()\n\n\n\npenguins %&gt;%\n  ggplot(mapping = aes(x = bill_length_mm,\n                       y = bill_depth_mm)) + \n  geom_point()"
  },
  {
    "objectID": "lectures/01-intro.html#modify-scale-add-statistical-summary-and-so-on",
    "href": "lectures/01-intro.html#modify-scale-add-statistical-summary-and-so-on",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Modify scale, add statistical summary, and so on…",
    "text": "Modify scale, add statistical summary, and so on…\n\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm,\n             y = bill_depth_mm)) + \n  # Adjust alpha of points\n  geom_point(alpha = 0.5) +\n  # Add smooth regression line\n  stat_smooth(method = \"lm\") + \n  # Flip the x-axis scale\n  scale_x_reverse() + \n  # Change title & axes labels \n  labs(x = \"Bill length (mm)\", \n       y = \"Bill depth (mm)\", \n       title = \"Clustering of penguins bills\") + \n  # Change the theme:\n  theme_bw() +\n  # Update font size of text:\n  theme(axis.title = element_text(size = 12),\n        plot.title = element_text(size = 16))"
  },
  {
    "objectID": "lectures/01-intro.html#modify-scale-add-statistical-summary-and-so-on-output",
    "href": "lectures/01-intro.html#modify-scale-add-statistical-summary-and-so-on-output",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Modify scale, add statistical summary, and so on…",
    "text": "Modify scale, add statistical summary, and so on…"
  },
  {
    "objectID": "lectures/01-intro.html#in-the-beginning",
    "href": "lectures/01-intro.html#in-the-beginning",
    "title": "Introduction and the Grammar of Graphics",
    "section": "In the beginning…",
    "text": "In the beginning…\n\nMichael Florent van Langren published the first (known) statistical graphic in 1644\n\n\n\n\n\n\nPlots different estimates of the longitudinal distance between Toledo, Spain and Rome, Italy\ni.e., visualization of collected data to aid in estimation of parameter"
  },
  {
    "objectID": "lectures/01-intro.html#john-snow-knows-something-about-cholera",
    "href": "lectures/01-intro.html#john-snow-knows-something-about-cholera",
    "title": "Introduction and the Grammar of Graphics",
    "section": "John Snow Knows Something About Cholera",
    "text": "John Snow Knows Something About Cholera"
  },
  {
    "objectID": "lectures/01-intro.html#charles-minards-map-of-napoleons-russian-disaster",
    "href": "lectures/01-intro.html#charles-minards-map-of-napoleons-russian-disaster",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Charles Minard’s Map of Napoleon’s Russian Disaster",
    "text": "Charles Minard’s Map of Napoleon’s Russian Disaster"
  },
  {
    "objectID": "lectures/01-intro.html#florence-nightingales-rose-diagram",
    "href": "lectures/01-intro.html#florence-nightingales-rose-diagram",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Florence Nightingale’s Rose Diagram",
    "text": "Florence Nightingale’s Rose Diagram"
  },
  {
    "objectID": "lectures/01-intro.html#milestones-in-data-visualization-history",
    "href": "lectures/01-intro.html#milestones-in-data-visualization-history",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Milestones in Data Visualization History",
    "text": "Milestones in Data Visualization History"
  },
  {
    "objectID": "lectures/01-intro.html#edward-tuftes-principles-of-data-visualization",
    "href": "lectures/01-intro.html#edward-tuftes-principles-of-data-visualization",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Edward Tufte’s Principles of Data Visualization",
    "text": "Edward Tufte’s Principles of Data Visualization\nGraphics: visually display measured quantities by combining points, lines, coordinate systems, numbers, symbols, words, shading, color\nOften our goal is to show data and/or communicate a story\n\n\nInduce viewer to think about substance, not graphical methodology\nMake large, complex datasets more coherent\nEncourage comparison of different pieces of data\nDescribe, explore, and identify relationships\nAvoid data distortion and data decoration\nUse consistent graph design\n\n\n\nAvoid graphs that lead to misleading conclusions!"
  },
  {
    "objectID": "lectures/01-intro.html#how-to-fail-this-class",
    "href": "lectures/01-intro.html#how-to-fail-this-class",
    "title": "Introduction and the Grammar of Graphics",
    "section": "How to Fail this Class:",
    "text": "How to Fail this Class:"
  },
  {
    "objectID": "lectures/01-intro.html#what-about-this-spiral",
    "href": "lectures/01-intro.html#what-about-this-spiral",
    "title": "Introduction and the Grammar of Graphics",
    "section": "What about this spiral?",
    "text": "What about this spiral?\n\n\nRequires distortion"
  },
  {
    "objectID": "lectures/01-intro.html#recap-and-next-steps",
    "href": "lectures/01-intro.html#recap-and-next-steps",
    "title": "Introduction and the Grammar of Graphics",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\nDiscussed the importance of data visualization in your role as a statistician / data scientist\nWalked through course logistics (READ THE SYLLABUS)\nIntroduced the Grammar of Graphics and ggplot2 basics\nDiscussed data visualization principles and the role of infographics\n\n\n\nComplete HW0 by Wednesday night! Confirms you have everything installed and can render .qmd files to PDF via tinytex\n\n\n\n\nNext time: 1D Categorical Data\nRecommended reading:\n\nCW Chapter 2 Visualizing data: Mapping data onto aesthetics, CW Chapter 17 The principle of proportional ink\nKH Chapter 1 Look at data, KH Chapter 3 Make a plot"
  },
  {
    "objectID": "lectures/16-clustering.html#announcements-previously-and-today",
    "href": "lectures/16-clustering.html#announcements-previously-and-today",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Announcements, previously, and today…",
    "text": "Announcements, previously, and today…\n\nHW5 is due TONIGHT by 11:59 PM ET\nYou have Lab 7 this Friday\nHW6 is due next Wednesday March 26th by 11:59 PM ET\n\n\n\nLast time: General approach for visualizing distance matrices\n\nPuts \\(n\\) observations in a \\(k\\)-dimensional space such that the distances are preserved as much as possible\n\nwhere \\(k &lt;&lt; p\\) typically choose \\(k = 2\\)\n\n\nMDS attempts to create new point \\(\\boldsymbol{y}_i = (y_{i1}, y_{i2})\\) for each unit such that:\n\\[\\sqrt{(y_{i1} - y_{j1})^2 + (y_{i2} - y_{j2})^2} \\approx D_{ij}\\]\n\ni.e., distance in 2D MDS world is approximately equal to the actual distance\n\nThen plot the new \\(\\boldsymbol{y}\\)s on a scatterplot\n\nUse the scale() function to ensure variables are comparable\nMake a distance matrix for this dataset\nVisualize it with MDS"
  },
  {
    "objectID": "lectures/16-clustering.html#demo-mcu-movie-data",
    "href": "lectures/16-clustering.html#demo-mcu-movie-data",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Demo: MCU movie data",
    "text": "Demo: MCU movie data\n\nmcu_movies &lt;- read_csv(\"https://raw.githubusercontent.com/ryurko/DataViz-Class-Data/main/mcu_movies.csv\")\nmcu_movies\n\n# A tibble: 30 × 18\n   film  category worldwide_gross_m percent_budget_recov…¹ critics_percent_score\n   &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;                  &lt;dbl&gt;                 &lt;dbl&gt;\n 1 Ant-… Ant-Man                518                    398                    83\n 2 Ant-… Ant-Man                623                    479                    87\n 3 Aven… Avengers              1395                    382                    76\n 4 Aven… Avengers              2797                    699                    94\n 5 Aven… Avengers              2048                    683                    85\n 6 Blac… Black P…              1336                    668                    96\n 7 Blac… Black P…               855                    342                    84\n 8 Blac… Unique                 379                    190                    79\n 9 Capt… Captain…               370                    264                    79\n10 Capt… Captain…              1151                    460                    90\n# ℹ 20 more rows\n# ℹ abbreviated name: ¹​percent_budget_recovered\n# ℹ 13 more variables: audience_percent_score &lt;dbl&gt;,\n#   audience_vs_critics_percent_deviance &lt;dbl&gt;, budget &lt;dbl&gt;,\n#   domestic_gross_m &lt;dbl&gt;, international_gross_m &lt;dbl&gt;,\n#   opening_weekend_m &lt;dbl&gt;, second_weekend_m &lt;dbl&gt;,\n#   x1st_vs_2nd_weekend_drop_off &lt;dbl&gt;, …"
  },
  {
    "objectID": "lectures/16-clustering.html#mcu-meets-mds",
    "href": "lectures/16-clustering.html#mcu-meets-mds",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "MCU meets MDS",
    "text": "MCU meets MDS\n\nmcu_quant &lt;- mcu_movies |&gt; dplyr::select(-c(film, category, year))\n\nmcu_quant &lt;- apply(mcu_quant, MARGIN = 2, FUN = function(x) x / sd(x))\nrownames(mcu_quant) &lt;- mcu_movies$film\n\nmcu_dist &lt;- dist(mcu_quant)\nmcu_mds &lt;- cmdscale(d = dist(mcu_quant), k = 2)\nmcu_movies &lt;- mcu_movies |&gt; mutate(mds1 = mcu_mds[,1], mds2 = mcu_mds[,2])\n\nmcu_movies |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  # Use text labels instead of points:\n  geom_text(aes(label = film), alpha = .75) +\n  labs(x = \"MDS Coordinate 1\", y = \"MDS Coordinate 2\") +\n  theme_bw()"
  },
  {
    "objectID": "lectures/16-clustering.html#mcu-meets-mds-output",
    "href": "lectures/16-clustering.html#mcu-meets-mds-output",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "MCU meets MDS",
    "text": "MCU meets MDS"
  },
  {
    "objectID": "lectures/16-clustering.html#other-ways-to-visualize-distances-and-clusters",
    "href": "lectures/16-clustering.html#other-ways-to-visualize-distances-and-clusters",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Other ways to visualize distances and clusters",
    "text": "Other ways to visualize distances and clusters\nMDS can be a great way to visualize distances and identify clusters…\n\nHowever, requires picking certain variables that identify clusters well\n\nIs there a way to automatically identify clusters in the dataset?\n\n\nDendrograms are a nice way to visualize distances\nAutomatically clusters different units together based on distance\n\n\\[\\overbrace{\\text{Dendro}}^{\\text{tree}}\\underbrace{\\text{gram}}_{\\text{drawing}}\\]\nFirst, let’s look at dendrograms and learn how to interpret them - then we’ll discuss how they’re made"
  },
  {
    "objectID": "lectures/16-clustering.html#textbook-example",
    "href": "lectures/16-clustering.html#textbook-example",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Textbook example",
    "text": "Textbook example"
  },
  {
    "objectID": "lectures/16-clustering.html#textbook-example-1",
    "href": "lectures/16-clustering.html#textbook-example-1",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Textbook example",
    "text": "Textbook example\n\n\n\nObservations that are closer together are on the same branch\nDoesn’t tell you how many clusters there are, but does tell you which observations are clustered together\nFor now: What is the computer doing to make dendrograms?"
  },
  {
    "objectID": "lectures/16-clustering.html#agglomerative-hierarchical-clustering",
    "href": "lectures/16-clustering.html#agglomerative-hierarchical-clustering",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "(Agglomerative) Hierarchical clustering",
    "text": "(Agglomerative) Hierarchical clustering\nLet’s pretend all \\(N\\) observations are in their own cluster\nStep 1: Compute the pairwise dissimilarities between each cluster\n\ne.g., distance matrix on previous slides\n\n\nStep 2: Identify the pair of clusters that are least dissimilar\n\n\nStep 3: Fuse these two clusters into a new cluster!\n\n\nRepeat Steps 1 to 3 until all observations are in the same cluster\n“Bottom-up”, agglomerative clustering that forms a tree / hierarchy of merging"
  },
  {
    "objectID": "lectures/16-clustering.html#agglomerative-hierarchical-clustering-1",
    "href": "lectures/16-clustering.html#agglomerative-hierarchical-clustering-1",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "(Agglomerative) Hierarchical clustering",
    "text": "(Agglomerative) Hierarchical clustering"
  },
  {
    "objectID": "lectures/16-clustering.html#forms-a-dendrogram",
    "href": "lectures/16-clustering.html#forms-a-dendrogram",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Forms a dendrogram",
    "text": "Forms a dendrogram"
  },
  {
    "objectID": "lectures/16-clustering.html#how-do-we-define-dissimilarity-between-clusters",
    "href": "lectures/16-clustering.html#how-do-we-define-dissimilarity-between-clusters",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "How do we define dissimilarity between clusters?",
    "text": "How do we define dissimilarity between clusters?\n\nWe know how to compute distance / dissimilarity between two observations\nBut how do we handle clusters?\n\nDissimilarity between a cluster and an observation, or between two clusters\n\n\n\n\nWe need to choose a linkage function! Clusters are built up by linking them together\nCompute all pairwise dissimilarities between observations in cluster 1 with observations in cluster 2\ni.e. Compute the distance matrix between observations, \\(d(x_i, x_j)\\) for \\(i \\in C_1\\) and \\(j \\in C_2\\)\n\n\n\n\n\nComplete linkage: Use the maximum value of these dissimilarities: \\(\\underset{i \\in C_1, j \\in C_2}{\\text{max}} d(x_i, x_j)\\)\nSingle linkage: Use the minimum value: \\(\\underset{i \\in C_1, j \\in C_2}{\\text{min}} d(x_i, x_j)\\)\nAverage linkage: Use the average value: \\(\\frac{1}{|C_1| \\cdot |C_2|} \\sum_{i \\in C_1} \\sum_{j \\in C_2} d(x_i, x_j)\\)\n\nDefine dissimilarity between two clusters based on our initial dissimilarity matrix between observations"
  },
  {
    "objectID": "lectures/16-clustering.html#ggdendro-version",
    "href": "lectures/16-clustering.html#ggdendro-version",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "ggdendro version",
    "text": "ggdendro version\n\nlibrary(ggdendro)\nggdendrogram(hc_complete, theme_dendro = FALSE) + #&lt;&lt;\n  labs(y = \"Cluster Dissimilarity (based on complete linkage)\", \n       title = \"Which MCU movies are similar to each other?\") + \n  coord_flip() + theme_bw() + theme(axis.title.y = element_blank())"
  },
  {
    "objectID": "lectures/16-clustering.html#display-mds-plot-with-dendrogram-side-by-side",
    "href": "lectures/16-clustering.html#display-mds-plot-with-dendrogram-side-by-side",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Display MDS plot with dendrogram side-by-side",
    "text": "Display MDS plot with dendrogram side-by-side\n\nlibrary(patchwork)\nhc_complete_ggdendro &lt;- ggdendrogram(hc_complete, theme_dendro = FALSE) +\n  labs(y = \"Cluster Dissimilarity (based on complete linkage)\", \n       title = \"Which MCU movies are similar to each other?\") + \n  coord_flip() +\n  theme_bw() +\n  # Remove the y-axis title (changed from x to y since we flipped it!)\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_text(size = 6),\n        plot.title = element_text(size = 10))\n\nmcu_mds_plot &lt;- mcu_movies |&gt;\n  ggplot(aes(x = mds1, y = mds2)) +\n  geom_text(aes(label = film),\n            alpha = .75, size = 2) +\n  labs(x = \"MDS Coordinate 1\", y = \"MDS Coordinate 2\") +\n  theme_bw()\n\nmcu_mds_plot + hc_complete_ggdendro"
  },
  {
    "objectID": "lectures/16-clustering.html#display-mds-plot-with-dendrogram-side-by-side-output",
    "href": "lectures/16-clustering.html#display-mds-plot-with-dendrogram-side-by-side-output",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Display MDS plot with dendrogram side-by-side",
    "text": "Display MDS plot with dendrogram side-by-side"
  },
  {
    "objectID": "lectures/16-clustering.html#how-do-we-assign-cluster-labels",
    "href": "lectures/16-clustering.html#how-do-we-assign-cluster-labels",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "How do we assign cluster labels?",
    "text": "How do we assign cluster labels?\nWe cut the dendrogram to return cluster labels\nTwo ways to specify how to cut the tree using the cutree function:\n\nvia the height using h, e.g., cut the tree at height = 10\n\n\ncutree(hc_complete, h = 10)\n\n\n\nvia the desired number of clusters k - and let the computer figure out the height for us, e.g., k = 2\n\n\ncutree(hc_complete, k = 2)"
  },
  {
    "objectID": "lectures/16-clustering.html#view-results-with-cut-on-dendrogram",
    "href": "lectures/16-clustering.html#view-results-with-cut-on-dendrogram",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "View results with cut on dendrogram",
    "text": "View results with cut on dendrogram\n\ncut_dendro &lt;- hc_complete_ggdendro +\n  # This is a horizontal line since its considered before the flip:\n  geom_hline(yintercept = 10, linetype = \"dashed\", \n             color = \"darkred\")\n\ncluster_mcu_mds_plot &lt;- mcu_movies |&gt;\n  mutate(cluster = as.factor(mcu_clusters)) |&gt;\n  ggplot(aes(x = mds1, y = mds2,\n             color = cluster)) +\n  geom_text(aes(label = film),\n            alpha = .75, size = 2) +\n  ggthemes::scale_color_colorblind() +\n  labs(x = \"MDS Coordinate 1\", y = \"MDS Coordinate 2\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\ncluster_mcu_mds_plot + cut_dendro"
  },
  {
    "objectID": "lectures/16-clustering.html#view-results-with-cut-on-dendrogram-output",
    "href": "lectures/16-clustering.html#view-results-with-cut-on-dendrogram-output",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "View results with cut on dendrogram",
    "text": "View results with cut on dendrogram"
  },
  {
    "objectID": "lectures/16-clustering.html#factoextra-package-version",
    "href": "lectures/16-clustering.html#factoextra-package-version",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "factoextra package version",
    "text": "factoextra package version\n\nlibrary(factoextra)\nfviz_dend(hc_complete, cex = 0.5, k = 3, color_labels_by_k = TRUE)"
  },
  {
    "objectID": "lectures/16-clustering.html#recap-and-next-steps",
    "href": "lectures/16-clustering.html#recap-and-next-steps",
    "title": "Dendrograms for Visualizing Distances and Clusters",
    "section": "Recap and next steps",
    "text": "Recap and next steps\n\n\nDendrograms are a great way to visualize distances and the clustering structure in the dataset\nHowever there are several decisions to be made!\nWhat type of linkage is appropriate for the problem?\nHow do we choose the number of clusters?\n\nThere is NOT a one size fits all solution to any of this!\n\n\n\n\nHW5 is due TONIGHT and you have lab this Friday!\nHW6 is posted and due next Wednesday March 26th\nNext time: PCA\nReview more code in lecture demos!"
  }
]